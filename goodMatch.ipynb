{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "142 Project",
      "provenance": [],
      "collapsed_sections": [
        "LcOhfFT81Ert",
        "KZSsaHSo3amp",
        "6YOC79JeZDf_",
        "cdPsEBDJZGOK",
        "rk05hTmwj2pM",
        "GlTsbn8hpbKa",
        "I09tBqmOqIAm",
        "NYgswAEdsPpn",
        "Efi0cwuWseoC",
        "6eOvysGJsnX0",
        "dX2WZemVtiyu",
        "bc3A-TTatwpE",
        "o5RE7vU_t6Q3",
        "ZkWf-cIvuMAJ",
        "C-oc3RUIuYPN",
        "oy-VBlVBuc1-",
        "AXzALhJCuxW6",
        "anh839iQu4Vd",
        "cNThc4vowArZ",
        "LlXZCghYvfX0",
        "2R74lnzOvoDW",
        "CZ5SDUa3vuEv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYIT1lqBch_n"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt \n",
        "pd.options.mode.chained_assignment = None "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcOhfFT81Ert"
      },
      "source": [
        "# Data Cleaning, Wrangling, and Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3AtPrJr3o3z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "837059bf-9742-475e-da04-d9f48cf9b737"
      },
      "source": [
        "speed = pd.read_csv(\"https://raw.githubusercontent.com/jlo-berkeley/142project/main/SpeedDating.csv\")\n",
        "speed.head()\n",
        "\n",
        "# General notes on column notation from source:\n",
        "# All partner (denoted by PID) values are suffixed with _o.\n",
        "# pf_o_attribute is the partner's (PID) value for attribute1_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d6629d8c-51b9-45ad-a7c6-829c0d02ae1b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iid</th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>idg</th>\n",
              "      <th>condtn</th>\n",
              "      <th>wave</th>\n",
              "      <th>round</th>\n",
              "      <th>position</th>\n",
              "      <th>positin1</th>\n",
              "      <th>order</th>\n",
              "      <th>partner</th>\n",
              "      <th>pid</th>\n",
              "      <th>match</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>race_o</th>\n",
              "      <th>pf_o_att</th>\n",
              "      <th>pf_o_sin</th>\n",
              "      <th>pf_o_int</th>\n",
              "      <th>pf_o_fun</th>\n",
              "      <th>pf_o_amb</th>\n",
              "      <th>pf_o_sha</th>\n",
              "      <th>dec_o</th>\n",
              "      <th>attr_o</th>\n",
              "      <th>sinc_o</th>\n",
              "      <th>intel_o</th>\n",
              "      <th>fun_o</th>\n",
              "      <th>amb_o</th>\n",
              "      <th>shar_o</th>\n",
              "      <th>like_o</th>\n",
              "      <th>prob_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>age</th>\n",
              "      <th>field</th>\n",
              "      <th>field_cd</th>\n",
              "      <th>undergra</th>\n",
              "      <th>mn_sat</th>\n",
              "      <th>tuition</th>\n",
              "      <th>race</th>\n",
              "      <th>...</th>\n",
              "      <th>amb5_2</th>\n",
              "      <th>you_call</th>\n",
              "      <th>them_cal</th>\n",
              "      <th>date_3</th>\n",
              "      <th>numdat_3</th>\n",
              "      <th>num_in_3</th>\n",
              "      <th>attr1_3</th>\n",
              "      <th>sinc1_3</th>\n",
              "      <th>intel1_3</th>\n",
              "      <th>fun1_3</th>\n",
              "      <th>amb1_3</th>\n",
              "      <th>shar1_3</th>\n",
              "      <th>attr7_3</th>\n",
              "      <th>sinc7_3</th>\n",
              "      <th>intel7_3</th>\n",
              "      <th>fun7_3</th>\n",
              "      <th>amb7_3</th>\n",
              "      <th>shar7_3</th>\n",
              "      <th>attr4_3</th>\n",
              "      <th>sinc4_3</th>\n",
              "      <th>intel4_3</th>\n",
              "      <th>fun4_3</th>\n",
              "      <th>amb4_3</th>\n",
              "      <th>shar4_3</th>\n",
              "      <th>attr2_3</th>\n",
              "      <th>sinc2_3</th>\n",
              "      <th>intel2_3</th>\n",
              "      <th>fun2_3</th>\n",
              "      <th>amb2_3</th>\n",
              "      <th>shar2_3</th>\n",
              "      <th>attr3_3</th>\n",
              "      <th>sinc3_3</th>\n",
              "      <th>intel3_3</th>\n",
              "      <th>fun3_3</th>\n",
              "      <th>amb3_3</th>\n",
              "      <th>attr5_3</th>\n",
              "      <th>sinc5_3</th>\n",
              "      <th>intel5_3</th>\n",
              "      <th>fun5_3</th>\n",
              "      <th>amb5_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1</td>\n",
              "      <td>22.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 195 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6629d8c-51b9-45ad-a7c6-829c0d02ae1b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6629d8c-51b9-45ad-a7c6-829c0d02ae1b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6629d8c-51b9-45ad-a7c6-829c0d02ae1b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   iid   id  gender  idg  condtn  ...  attr5_3  sinc5_3  intel5_3  fun5_3  amb5_3\n",
              "0    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "1    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "2    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "3    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "4    1  1.0       0    1       1  ...      NaN      NaN       NaN     NaN     NaN\n",
              "\n",
              "[5 rows x 195 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5rD90Su53sP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "9e09aaac-b21e-49ef-b4a7-7ab6aba11bcb"
      },
      "source": [
        "# Import college ranking table, then merge onto speed dataset.\n",
        "\n",
        "college_rankings = pd.read_csv('https://raw.githubusercontent.com/arnaudbenard/university-ranking/master/cwurData.csv')\n",
        "college_rankings = college_rankings.head(1200)\n",
        "college_rankings = college_rankings.filter(['world_rank', 'institution'])\n",
        "college_rankings['institution'] = college_rankings['institution'].str.lower()\n",
        "speed['undergra'] = speed['undergra'].str.lower()\n",
        "NaN_ranking = {'world_rank': 500, 'institution': np.nan}\n",
        "college_rankings = college_rankings.append(NaN_ranking, ignore_index= True)\n",
        "college_rankings = college_rankings.rename({'institution': 'undergra'}, axis=1)\n",
        "speed = pd.merge(speed, college_rankings, how = \"left\", on='undergra')\n",
        "speed['world_rank'] = speed['world_rank'].fillna(1001)\n",
        "speed.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4ce41982-0887-40df-af21-470e8f9109c2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iid</th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>idg</th>\n",
              "      <th>condtn</th>\n",
              "      <th>wave</th>\n",
              "      <th>round</th>\n",
              "      <th>position</th>\n",
              "      <th>positin1</th>\n",
              "      <th>order</th>\n",
              "      <th>partner</th>\n",
              "      <th>pid</th>\n",
              "      <th>match</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>race_o</th>\n",
              "      <th>pf_o_att</th>\n",
              "      <th>pf_o_sin</th>\n",
              "      <th>pf_o_int</th>\n",
              "      <th>pf_o_fun</th>\n",
              "      <th>pf_o_amb</th>\n",
              "      <th>pf_o_sha</th>\n",
              "      <th>dec_o</th>\n",
              "      <th>attr_o</th>\n",
              "      <th>sinc_o</th>\n",
              "      <th>intel_o</th>\n",
              "      <th>fun_o</th>\n",
              "      <th>amb_o</th>\n",
              "      <th>shar_o</th>\n",
              "      <th>like_o</th>\n",
              "      <th>prob_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>age</th>\n",
              "      <th>field</th>\n",
              "      <th>field_cd</th>\n",
              "      <th>undergra</th>\n",
              "      <th>mn_sat</th>\n",
              "      <th>tuition</th>\n",
              "      <th>race</th>\n",
              "      <th>...</th>\n",
              "      <th>you_call</th>\n",
              "      <th>them_cal</th>\n",
              "      <th>date_3</th>\n",
              "      <th>numdat_3</th>\n",
              "      <th>num_in_3</th>\n",
              "      <th>attr1_3</th>\n",
              "      <th>sinc1_3</th>\n",
              "      <th>intel1_3</th>\n",
              "      <th>fun1_3</th>\n",
              "      <th>amb1_3</th>\n",
              "      <th>shar1_3</th>\n",
              "      <th>attr7_3</th>\n",
              "      <th>sinc7_3</th>\n",
              "      <th>intel7_3</th>\n",
              "      <th>fun7_3</th>\n",
              "      <th>amb7_3</th>\n",
              "      <th>shar7_3</th>\n",
              "      <th>attr4_3</th>\n",
              "      <th>sinc4_3</th>\n",
              "      <th>intel4_3</th>\n",
              "      <th>fun4_3</th>\n",
              "      <th>amb4_3</th>\n",
              "      <th>shar4_3</th>\n",
              "      <th>attr2_3</th>\n",
              "      <th>sinc2_3</th>\n",
              "      <th>intel2_3</th>\n",
              "      <th>fun2_3</th>\n",
              "      <th>amb2_3</th>\n",
              "      <th>shar2_3</th>\n",
              "      <th>attr3_3</th>\n",
              "      <th>sinc3_3</th>\n",
              "      <th>intel3_3</th>\n",
              "      <th>fun3_3</th>\n",
              "      <th>amb3_3</th>\n",
              "      <th>attr5_3</th>\n",
              "      <th>sinc5_3</th>\n",
              "      <th>intel5_3</th>\n",
              "      <th>fun5_3</th>\n",
              "      <th>amb5_3</th>\n",
              "      <th>world_rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1</td>\n",
              "      <td>22.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>500.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 196 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ce41982-0887-40df-af21-470e8f9109c2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4ce41982-0887-40df-af21-470e8f9109c2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4ce41982-0887-40df-af21-470e8f9109c2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   iid   id  gender  idg  condtn  ...  sinc5_3  intel5_3  fun5_3  amb5_3  world_rank\n",
              "0    1  1.0       0    1       1  ...      NaN       NaN     NaN     NaN       500.0\n",
              "1    1  1.0       0    1       1  ...      NaN       NaN     NaN     NaN       500.0\n",
              "2    1  1.0       0    1       1  ...      NaN       NaN     NaN     NaN       500.0\n",
              "3    1  1.0       0    1       1  ...      NaN       NaN     NaN     NaN       500.0\n",
              "4    1  1.0       0    1       1  ...      NaN       NaN     NaN     NaN       500.0\n",
              "\n",
              "[5 rows x 196 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhUvpISz2fI2"
      },
      "source": [
        "# Replace nan values with string 'n/a' for regex processing.\n",
        "\n",
        "field_col = speed['field'].fillna('n/a')\n",
        "\n",
        "# Find all master student entries with regex.\n",
        "# Replace values with 1 if master's student or above, else 0.\n",
        "\n",
        "find_masters = [re.findall(r'(M[BIF\\.]?A\\.?)|((Ed)|(Ph)|J)\\.?D\\.?|([Mm]asters?)|GSAS', i) for i in field_col]\n",
        "replace_masters = [0 if i == [] else 1 for i in find_masters]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "1CfMWkHw2sJo",
        "outputId": "be8a3390-1a93-4260-e26c-d4bffa1e1515"
      },
      "source": [
        "# Add masters column to original table\n",
        "\n",
        "speed['masters'] = replace_masters\n",
        "speed[['field', 'masters']].groupby('field').first().query('masters == 1').head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-71a95d0b-383a-46c9-a4d1-35d7bc2da84b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>masters</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>field</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>American Studies (Masters)</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Biology PhD</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Business (MBA)</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Business and International Affairs (MBA/MIA Dual Degree)</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Business- MBA</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71a95d0b-383a-46c9-a4d1-35d7bc2da84b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-71a95d0b-383a-46c9-a4d1-35d7bc2da84b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-71a95d0b-383a-46c9-a4d1-35d7bc2da84b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    masters\n",
              "field                                                      \n",
              "American Studies (Masters)                                1\n",
              "Biology PhD                                               1\n",
              "Business (MBA)                                            1\n",
              "Business and International Affairs (MBA/MIA Dua...        1\n",
              "Business- MBA                                             1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8AD3nvR3q8O"
      },
      "source": [
        "# Convert strings to floats where needed.\n",
        "\n",
        "speed['income'] = speed['income'].str.replace(\",\", \"\")\n",
        "speed['income'] = pd.to_numeric(speed[['income']].income, errors='coerce')\n",
        "speed['mn_sat'] = speed['mn_sat'].str.replace(\",\", \"\")\n",
        "speed['mn_sat'] = pd.to_numeric(speed[['mn_sat']].mn_sat, errors='coerce')\n",
        "speed['tuition'] = speed['tuition'].str.replace(\",\", \"\")\n",
        "speed['tuition'] = pd.to_numeric(speed[['tuition']].tuition, errors='coerce')\n",
        "\n",
        "# Compute mean/median values, then use them to fill NaNs.\n",
        "\n",
        "mean_sat = np.mean(speed['mn_sat'].dropna().values).round(2)\n",
        "mean_tuition = np.mean(speed['tuition'].dropna().values).round(2)\n",
        "med_income = np.median(speed['income'].dropna().values).round(2)\n",
        "med_match_es = np.median(speed['match_es'].dropna().values).round(2)\n",
        "speed = speed.fillna(value={'mn_sat': mean_sat, 'tuition': mean_tuition, 'income': med_income, 'match_es': med_match_es})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5howAWBckyp"
      },
      "source": [
        "# These values are surveyed /after/ the speed dating occurs. These are not of interest\n",
        "# to our particular points of exploration outlined in the report, so we will drop them.\n",
        "speed = speed.drop(columns=[\"attr1_3\", \"sinc1_3\", \"intel1_3\", \"fun1_3\", \"amb1_3\", \"shar1_3\"])\n",
        "speed = speed.drop(columns=[\"attr7_3\", \"sinc7_3\", \"intel7_3\", \"fun7_3\", \"amb7_3\", \"shar7_3\"])\n",
        "speed = speed.drop(columns=[\"attr4_3\", \"sinc4_3\", \"intel4_3\", \"fun4_3\", \"amb4_3\", \"shar4_3\"])\n",
        "speed = speed.drop(columns=[\"attr2_3\", \"sinc2_3\", \"intel2_3\", \"fun2_3\", \"amb2_3\", \"shar2_3\"])\n",
        "speed = speed.drop(columns=[\"attr3_3\", \"sinc3_3\", \"intel3_3\", \"fun3_3\", \"amb3_3\"])\n",
        "speed = speed.drop(columns=[\"attr5_3\", \"sinc5_3\", \"intel5_3\", \"fun5_3\", \"amb5_3\"])\n",
        "speed = speed.drop(columns=[\"attr1_2\", \"sinc1_2\", \"intel1_2\", \"fun1_2\", \"amb1_2\", \"shar1_2\"])\n",
        "speed = speed.drop(columns=[\"attr7_2\", \"sinc7_2\", \"intel7_2\", \"fun7_2\", \"amb7_2\", \"shar7_2\"])\n",
        "speed = speed.drop(columns=[\"attr4_2\", \"sinc4_2\", \"intel4_2\", \"fun4_2\", \"amb4_2\", \"shar4_2\"])\n",
        "speed = speed.drop(columns=[\"attr2_2\", \"sinc2_2\", \"intel2_2\", \"fun2_2\", \"amb2_2\", \"shar2_2\"])\n",
        "speed = speed.drop(columns=[\"attr3_2\", \"sinc3_2\", \"intel3_2\", \"fun3_2\", \"amb3_2\"])\n",
        "speed = speed.drop(columns=[\"attr5_2\", \"sinc5_2\", \"intel5_2\", \"fun5_2\", \"amb5_2\"])\n",
        "speed = speed.drop(columns=[\"attr1_s\", \"sinc1_s\", \"intel1_s\", \"fun1_s\", \"amb1_s\", \"shar1_s\"])\n",
        "speed = speed.drop(columns=[\"attr3_s\", \"sinc3_s\", \"intel3_s\", \"fun3_s\", \"amb3_s\"])\n",
        "speed = speed.drop(columns=[\"num_in_3\", \"numdat_3\", \"expnum\", \"date_3\", \"them_cal\", \"you_call\", \"numdat_2\"])\n",
        "\n",
        "# There are two rows for every paid, such that each individual from each pair appears as both\n",
        "# the PID and IID once for each match up. As such, we can safely remove these attr, sinc... etc.,\n",
        "# as they will appear as attr_o, sinc_o... etc. in another row.\n",
        "speed = speed.drop(columns=[\"attr\", \"sinc\", \"intel\", \"fun\", \"amb\", \"shar\", \"like\", \"prob\", \"like_o\", \"prob_o\", \"satis_2\"])\n",
        "\n",
        "# Drop values not used in analysis due to encoding from other variables (e.g. mn_sat).\n",
        "speed = speed.drop(columns=[\"positin1\", \"zipcode\", \"length\", \"undergra\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUc2rTje5CtI",
        "outputId": "c15b6e05-ba9f-4448-d9ef-3dc2ceb83d4d"
      },
      "source": [
        "# Check to see if there are any remaining columns with too many null values to fill.\n",
        "\n",
        "k = speed.isnull().sum(axis = 0).sort_values(ascending=False)\n",
        "print(\"Number of Null Values per Column\")\n",
        "for i in range(len(k)):\n",
        "  if k.values[i] > 200:\n",
        "    print(k.index[i], k.values[i])\n",
        "speed[['attr5_1']].value_counts()\n",
        "speed.shape\n",
        "# Drop corresponding columns.\n",
        "\n",
        "speed = speed.drop(columns=[\"attr5_1\", \"sinc5_1\", \"intel5_1\", \"fun5_1\", \"amb5_1\"])\n",
        "speed = speed.drop(columns=[\"attr4_1\", \"sinc4_1\", \"intel4_1\", \"fun4_1\", \"amb4_1\", \"shar4_1\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Null Values per Column\n",
            "amb5_1 3472\n",
            "fun5_1 3472\n",
            "intel5_1 3472\n",
            "sinc5_1 3472\n",
            "attr5_1 3472\n",
            "shar4_1 1911\n",
            "sinc4_1 1889\n",
            "attr4_1 1889\n",
            "intel4_1 1889\n",
            "fun4_1 1889\n",
            "amb4_1 1889\n",
            "shar_o 1189\n",
            "amb_o 821\n",
            "met_o 434\n",
            "met 409\n",
            "fun_o 402\n",
            "intel_o 344\n",
            "sinc_o 331\n",
            "attr_o 244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHsQbfYhquRJ",
        "outputId": "3fc9f80a-dc60-4e13-c0bf-6e87285bfa7a"
      },
      "source": [
        "# Check final NaN counts. \n",
        "# Note: We can run 6 different regressions for attribute_o, and remove NaN as needed.\n",
        "# The reason for NaNs is as given:\n",
        "# \"If you haven‟t formed an opinion based on your conversation, fill in N/A, but please fill in all boxes.\"\n",
        "\n",
        "k = speed.isnull().sum(axis = 0).sort_values(ascending=False)\n",
        "print(\"Number of Null Values per Column\")\n",
        "for i in range(len(k)):\n",
        "  if k.values[i] > 100:\n",
        "    print(k.index[i], k.values[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Null Values per Column\n",
            "shar_o 1189\n",
            "amb_o 821\n",
            "met_o 434\n",
            "met 409\n",
            "fun_o 402\n",
            "intel_o 344\n",
            "sinc_o 331\n",
            "attr_o 244\n",
            "int_corr 170\n",
            "pf_o_sha 147\n",
            "career_c 138\n",
            "age_o 122\n",
            "shar1_1 121\n",
            "pf_o_amb 119\n",
            "pf_o_fun 110\n",
            "fun3_1 105\n",
            "amb3_1 105\n",
            "intel3_1 105\n",
            "sinc3_1 105\n",
            "attr3_1 105\n",
            "exphappy 101\n",
            "pf_o_sin 101\n",
            "pf_o_int 101\n",
            "pf_o_att 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "C6kzROVE5Naj",
        "outputId": "7e129360-79a1-4d42-8fb8-b67bac6ad70e"
      },
      "source": [
        "speed.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cf8a3728-46eb-414b-98da-0b54651b245f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iid</th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>idg</th>\n",
              "      <th>condtn</th>\n",
              "      <th>wave</th>\n",
              "      <th>round</th>\n",
              "      <th>position</th>\n",
              "      <th>order</th>\n",
              "      <th>partner</th>\n",
              "      <th>pid</th>\n",
              "      <th>match</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>race_o</th>\n",
              "      <th>pf_o_att</th>\n",
              "      <th>pf_o_sin</th>\n",
              "      <th>pf_o_int</th>\n",
              "      <th>pf_o_fun</th>\n",
              "      <th>pf_o_amb</th>\n",
              "      <th>pf_o_sha</th>\n",
              "      <th>dec_o</th>\n",
              "      <th>attr_o</th>\n",
              "      <th>sinc_o</th>\n",
              "      <th>intel_o</th>\n",
              "      <th>fun_o</th>\n",
              "      <th>amb_o</th>\n",
              "      <th>shar_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>age</th>\n",
              "      <th>field</th>\n",
              "      <th>field_cd</th>\n",
              "      <th>mn_sat</th>\n",
              "      <th>tuition</th>\n",
              "      <th>race</th>\n",
              "      <th>imprace</th>\n",
              "      <th>imprelig</th>\n",
              "      <th>from</th>\n",
              "      <th>income</th>\n",
              "      <th>...</th>\n",
              "      <th>sports</th>\n",
              "      <th>tvsports</th>\n",
              "      <th>exercise</th>\n",
              "      <th>dining</th>\n",
              "      <th>museums</th>\n",
              "      <th>art</th>\n",
              "      <th>hiking</th>\n",
              "      <th>gaming</th>\n",
              "      <th>clubbing</th>\n",
              "      <th>reading</th>\n",
              "      <th>tv</th>\n",
              "      <th>theater</th>\n",
              "      <th>movies</th>\n",
              "      <th>concerts</th>\n",
              "      <th>music</th>\n",
              "      <th>shopping</th>\n",
              "      <th>yoga</th>\n",
              "      <th>exphappy</th>\n",
              "      <th>attr1_1</th>\n",
              "      <th>sinc1_1</th>\n",
              "      <th>intel1_1</th>\n",
              "      <th>fun1_1</th>\n",
              "      <th>amb1_1</th>\n",
              "      <th>shar1_1</th>\n",
              "      <th>attr2_1</th>\n",
              "      <th>sinc2_1</th>\n",
              "      <th>intel2_1</th>\n",
              "      <th>fun2_1</th>\n",
              "      <th>amb2_1</th>\n",
              "      <th>shar2_1</th>\n",
              "      <th>attr3_1</th>\n",
              "      <th>sinc3_1</th>\n",
              "      <th>fun3_1</th>\n",
              "      <th>intel3_1</th>\n",
              "      <th>amb3_1</th>\n",
              "      <th>dec</th>\n",
              "      <th>met</th>\n",
              "      <th>match_es</th>\n",
              "      <th>world_rank</th>\n",
              "      <th>masters</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>69487.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>69487.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.16</td>\n",
              "      <td>1</td>\n",
              "      <td>22.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>1</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>69487.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>14.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>69487.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>69487.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 85 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf8a3728-46eb-414b-98da-0b54651b245f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cf8a3728-46eb-414b-98da-0b54651b245f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cf8a3728-46eb-414b-98da-0b54651b245f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   iid   id  gender  idg  condtn  ...  dec  met  match_es  world_rank  masters\n",
              "0    1  1.0       0    1       1  ...    1  2.0       4.0       500.0        0\n",
              "1    1  1.0       0    1       1  ...    1  1.0       4.0       500.0        0\n",
              "2    1  1.0       0    1       1  ...    1  1.0       4.0       500.0        0\n",
              "3    1  1.0       0    1       1  ...    1  2.0       4.0       500.0        0\n",
              "4    1  1.0       0    1       1  ...    1  2.0       4.0       500.0        0\n",
              "\n",
              "[5 rows x 85 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VoNJXCdcxls",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "1c015faf-42c5-432d-ffe1-d3c640b1e6c5"
      },
      "source": [
        "# Sanity check on columns in speed.\n",
        "\n",
        "display(np.array(speed.columns))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array(['iid', 'id', 'gender', 'idg', 'condtn', 'wave', 'round',\n",
              "       'position', 'order', 'partner', 'pid', 'match', 'int_corr',\n",
              "       'samerace', 'age_o', 'race_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int',\n",
              "       'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'dec_o', 'attr_o', 'sinc_o',\n",
              "       'intel_o', 'fun_o', 'amb_o', 'shar_o', 'met_o', 'age', 'field',\n",
              "       'field_cd', 'mn_sat', 'tuition', 'race', 'imprace', 'imprelig',\n",
              "       'from', 'income', 'goal', 'date', 'go_out', 'career', 'career_c',\n",
              "       'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art',\n",
              "       'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n",
              "       'movies', 'concerts', 'music', 'shopping', 'yoga', 'exphappy',\n",
              "       'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1',\n",
              "       'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1',\n",
              "       'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', 'dec', 'met',\n",
              "       'match_es', 'world_rank', 'masters'], dtype=object)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "OIsK3nr5wu7k",
        "outputId": "c41f8c81-c3b2-4c94-c1e7-b55667ab815a"
      },
      "source": [
        "# Drop rows where we cannot observe the partner.\n",
        "\n",
        "speed = speed[speed['pid'].notna()]\n",
        "\n",
        "# Create IID lookup dataframe.\n",
        "\n",
        "iid = speed.groupby('iid').first()[[\"gender\", \"age\", \"field\", \"field_cd\", \"mn_sat\", \n",
        "       \"tuition\", 'race', 'imprace', 'imprelig',\n",
        "       'from', 'income', 'goal', 'date', 'go_out', 'career', 'career_c',\n",
        "       'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art',\n",
        "       'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n",
        "       'movies', 'concerts', 'music', 'shopping', 'yoga', 'exphappy',\n",
        "       'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1',\n",
        "       'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1',\n",
        "       'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', 'world_rank', 'masters']]\n",
        "iid.head(10)\n",
        "\n",
        "# Rename columns for joining speed with partner info.\n",
        "\n",
        "pid = iid.copy().drop(columns=[\"gender\", \"age\", \"field\", \"race\", \"career\", 'attr1_1', 'sinc1_1', \n",
        "  'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']).rename(columns={'field_cd':'field_cd_o', \n",
        "  'mn_sat':'mn_sat_o', 'tuition':'tuition_o', 'world_rank': 'world_rank_o', 'masters': 'masters_o',\n",
        "  'imprace':'imprace_o', 'imprelig':'imprelig_o', 'from':'from_o', 'income':'income_o', \n",
        "  'goal':'goal_o', 'date':'date_o', 'go_out':'go_out_o', 'exphappy':'exphappy_o', \n",
        "  'career_c':'career_c_o', 'sports':'sports_o', 'tvsports':'tvsport_o', 'exercise':'exercise_o', \n",
        "  'dining':'dining_o', 'museums':'museums_o', 'art':'art_o', 'hiking':'hiking_o', 'gaming':'gaming_o', \n",
        "  'clubbing':'clubbing_o', 'reading':'reading_o', 'tv':'tv_o', 'theater':'theater_o',\n",
        "  'movies':'movies_o', 'concerts':'concerts_o', 'music':'music_o', 'shopping':'shopping_o', 'yoga':'yoga_o',\n",
        "  'attr3_1':'attr3_1_o', 'sinc3_1':'sinc3_1_o', 'fun3_1':'fun3_1_o', 'intel3_1':'intel3_1_o', 'amb3_1':'amb3_1_o',\n",
        "  'attr2_1':'attr2_1_o', 'sinc2_1':'sinc2_1_o', 'intel2_1':'intel2_1_o', 'fun2_1':'fun2_1_o', 'amb2_1':'amb2_1_o', 'shar2_1':'shar2_1_o'})\n",
        "\n",
        "# Merge partner info with speed info into DataFrame merged.\n",
        "\n",
        "merged = pd.merge(left=speed, right=pid, how=\"inner\", left_on=\"pid\", right_index=True)\n",
        "merged.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-29afcf53-5b2c-4bbc-a8af-4dfb7633f237\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iid</th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>idg</th>\n",
              "      <th>condtn</th>\n",
              "      <th>wave</th>\n",
              "      <th>round</th>\n",
              "      <th>position</th>\n",
              "      <th>order</th>\n",
              "      <th>partner</th>\n",
              "      <th>pid</th>\n",
              "      <th>match</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>race_o</th>\n",
              "      <th>pf_o_att</th>\n",
              "      <th>pf_o_sin</th>\n",
              "      <th>pf_o_int</th>\n",
              "      <th>pf_o_fun</th>\n",
              "      <th>pf_o_amb</th>\n",
              "      <th>pf_o_sha</th>\n",
              "      <th>dec_o</th>\n",
              "      <th>attr_o</th>\n",
              "      <th>sinc_o</th>\n",
              "      <th>intel_o</th>\n",
              "      <th>fun_o</th>\n",
              "      <th>amb_o</th>\n",
              "      <th>shar_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>age</th>\n",
              "      <th>field</th>\n",
              "      <th>field_cd</th>\n",
              "      <th>mn_sat</th>\n",
              "      <th>tuition</th>\n",
              "      <th>race</th>\n",
              "      <th>imprace</th>\n",
              "      <th>imprelig</th>\n",
              "      <th>from</th>\n",
              "      <th>income</th>\n",
              "      <th>...</th>\n",
              "      <th>tuition_o</th>\n",
              "      <th>imprace_o</th>\n",
              "      <th>imprelig_o</th>\n",
              "      <th>from_o</th>\n",
              "      <th>income_o</th>\n",
              "      <th>goal_o</th>\n",
              "      <th>date_o</th>\n",
              "      <th>go_out_o</th>\n",
              "      <th>career_c_o</th>\n",
              "      <th>sports_o</th>\n",
              "      <th>tvsport_o</th>\n",
              "      <th>exercise_o</th>\n",
              "      <th>dining_o</th>\n",
              "      <th>museums_o</th>\n",
              "      <th>art_o</th>\n",
              "      <th>hiking_o</th>\n",
              "      <th>gaming_o</th>\n",
              "      <th>clubbing_o</th>\n",
              "      <th>reading_o</th>\n",
              "      <th>tv_o</th>\n",
              "      <th>theater_o</th>\n",
              "      <th>movies_o</th>\n",
              "      <th>concerts_o</th>\n",
              "      <th>music_o</th>\n",
              "      <th>shopping_o</th>\n",
              "      <th>yoga_o</th>\n",
              "      <th>exphappy_o</th>\n",
              "      <th>attr2_1_o</th>\n",
              "      <th>sinc2_1_o</th>\n",
              "      <th>intel2_1_o</th>\n",
              "      <th>fun2_1_o</th>\n",
              "      <th>amb2_1_o</th>\n",
              "      <th>shar2_1_o</th>\n",
              "      <th>attr3_1_o</th>\n",
              "      <th>sinc3_1_o</th>\n",
              "      <th>fun3_1_o</th>\n",
              "      <th>intel3_1_o</th>\n",
              "      <th>amb3_1_o</th>\n",
              "      <th>world_rank_o</th>\n",
              "      <th>masters_o</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>69487.0</td>\n",
              "      <td>...</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>43367.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>65929.0</td>\n",
              "      <td>...</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>43367.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>Economics</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Connecticut</td>\n",
              "      <td>43367.0</td>\n",
              "      <td>...</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>43367.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Texas</td>\n",
              "      <td>37754.0</td>\n",
              "      <td>...</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>43367.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Bowdoin College</td>\n",
              "      <td>86340.0</td>\n",
              "      <td>...</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Argentina</td>\n",
              "      <td>43367.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>500.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 127 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29afcf53-5b2c-4bbc-a8af-4dfb7633f237')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-29afcf53-5b2c-4bbc-a8af-4dfb7633f237 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-29afcf53-5b2c-4bbc-a8af-4dfb7633f237');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    iid   id  gender  idg  ...  intel3_1_o  amb3_1_o  world_rank_o  masters_o\n",
              "0     1  1.0       0    1  ...         8.0       5.0         500.0          0\n",
              "10    2  2.0       0    3  ...         8.0       5.0         500.0          0\n",
              "20    3  3.0       0    5  ...         8.0       5.0         500.0          0\n",
              "30    4  4.0       0    7  ...         8.0       5.0         500.0          0\n",
              "40    5  5.0       0    9  ...         8.0       5.0         500.0          0\n",
              "\n",
              "[5 rows x 127 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "wJjusLITrogt",
        "outputId": "821b5c1d-7392-4c1f-c49a-751047cdd613"
      },
      "source": [
        "# Sanity check on columns in merged.\n",
        "\n",
        "display(np.array(merged.columns))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array(['iid', 'id', 'gender', 'idg', 'condtn', 'wave', 'round',\n",
              "       'position', 'order', 'partner', 'pid', 'match', 'int_corr',\n",
              "       'samerace', 'age_o', 'race_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int',\n",
              "       'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'dec_o', 'attr_o', 'sinc_o',\n",
              "       'intel_o', 'fun_o', 'amb_o', 'shar_o', 'met_o', 'age', 'field',\n",
              "       'field_cd', 'mn_sat', 'tuition', 'race', 'imprace', 'imprelig',\n",
              "       'from', 'income', 'goal', 'date', 'go_out', 'career', 'career_c',\n",
              "       'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art',\n",
              "       'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n",
              "       'movies', 'concerts', 'music', 'shopping', 'yoga', 'exphappy',\n",
              "       'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1',\n",
              "       'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1',\n",
              "       'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', 'dec', 'met',\n",
              "       'match_es', 'world_rank', 'masters', 'field_cd_o', 'mn_sat_o',\n",
              "       'tuition_o', 'imprace_o', 'imprelig_o', 'from_o', 'income_o',\n",
              "       'goal_o', 'date_o', 'go_out_o', 'career_c_o', 'sports_o',\n",
              "       'tvsport_o', 'exercise_o', 'dining_o', 'museums_o', 'art_o',\n",
              "       'hiking_o', 'gaming_o', 'clubbing_o', 'reading_o', 'tv_o',\n",
              "       'theater_o', 'movies_o', 'concerts_o', 'music_o', 'shopping_o',\n",
              "       'yoga_o', 'exphappy_o', 'attr2_1_o', 'sinc2_1_o', 'intel2_1_o',\n",
              "       'fun2_1_o', 'amb2_1_o', 'shar2_1_o', 'attr3_1_o', 'sinc3_1_o',\n",
              "       'fun3_1_o', 'intel3_1_o', 'amb3_1_o', 'world_rank_o', 'masters_o'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrXueLo_vtDq"
      },
      "source": [
        "# Compute the absolute difference between responses for age/age_o, income/income_o, date/date_o, \n",
        "# go_out/go_out_o, sports/sports_o...yoga/yoga_o, and pf_o_attribute/attribute3_1 (not including shar), and \n",
        "# attribute1_1/attribute3_1_o (not including shar), and attribute2_1_o/attribute1_1 and\n",
        "# attribute2_1/pf_o_attribute.\n",
        "\n",
        "# These values will be used in place of absolute values, because we want to\n",
        "# look at pairwise information for matching.\n",
        "\n",
        "# Save all of these as new columns in merged.\n",
        "\n",
        "merged['age_abs(diff)'] = abs(merged['age'] - merged['age_o'])\n",
        "merged['income_abs(diff)'] = abs(merged['income'] - merged['income_o'])\n",
        "merged['date_abs(diff)'] = abs(merged['date'] - merged['date_o'])\n",
        "merged['go_out_abs(diff)'] = abs(merged['go_out'] - merged['go_out_o'])\n",
        "merged['sports_abs(diff)'] = abs(merged['sports'] - merged['sports_o'])\n",
        "merged['tvsport_abs(diff)'] = abs(merged['tvsports'] - merged['tvsport_o'])\n",
        "merged['exercise_abs(diff)'] = abs(merged['exercise'] - merged['exercise_o'])\n",
        "merged['dining_abs(diff)'] = abs(merged['dining'] - merged['dining_o'])\n",
        "merged['museums_abs(diff)'] = abs(merged['museums'] - merged['museums_o'])\n",
        "merged['art_abs(diff)'] = abs(merged['art'] - merged['art_o'])\n",
        "merged['hiking_abs(diff)'] = abs(merged['hiking'] - merged['hiking_o'])\n",
        "merged['gaming_abs(diff)'] = abs(merged['gaming'] - merged['gaming_o'])\n",
        "merged['clubbing_abs(diff)'] = abs(merged['clubbing'] - merged['clubbing_o'])\n",
        "merged['reading_abs(diff)'] = abs(merged['reading'] - merged['reading_o'])\n",
        "merged['tv_abs(diff)'] = abs(merged['tv'] - merged['tv_o'])\n",
        "merged['theater_abs(diff)'] = abs(merged['theater'] - merged['theater_o'])\n",
        "merged['movies_abs(diff)'] = abs(merged['movies'] - merged['movies_o'])\n",
        "merged['concerts_abs(diff)'] = abs(merged['concerts'] - merged['concerts_o'])\n",
        "merged['music_abs(diff)'] = abs(merged['music'] - merged['music_o'])\n",
        "merged['shopping_abs(diff)'] = abs(merged['shopping'] - merged['shopping_o'])\n",
        "merged['yoga_abs(diff)'] = abs(merged['yoga'] - merged['yoga_o'])\n",
        "merged['worldrank_abs(diff)'] = abs(merged['world_rank'] - merged['world_rank_o'])\n",
        "\n",
        "merged['pf_o/3_1_att_abs(diff)'] = abs(merged['pf_o_att'] - merged['attr3_1'])\n",
        "merged['pf_o/3_1_sinc_abs(diff)'] = abs(merged['pf_o_sin'] - merged['sinc3_1'])\n",
        "merged['pf_o/3_1_fun_abs(diff)'] = abs(merged['pf_o_fun'] - merged['fun3_1'])\n",
        "merged['pf_o/3_1_intel_abs(diff)'] = abs(merged['pf_o_int'] - merged['intel3_1'])\n",
        "merged['pf_o/3_1_amb_abs(diff)'] = abs(merged['pf_o_amb'] - merged['amb3_1'])\n",
        "\n",
        "merged['1_1/3_1_o_att_abs(diff)'] = abs(merged['attr1_1'] - merged['attr3_1_o'])\n",
        "merged['1_1/3_1_o_sinc_abs(diff)'] = abs(merged['sinc1_1'] - merged['sinc3_1_o'])\n",
        "merged['1_1/3_1_o_fun_abs(diff)'] = abs(merged['fun1_1'] - merged['fun3_1_o'])\n",
        "merged['1_1/3_1_o_intel_abs(diff)'] = abs(merged['intel1_1'] - merged['intel3_1_o'])\n",
        "merged['1_1/3_1_o_amb_abs(diff)'] = abs(merged['amb1_1'] - merged['amb3_1_o'])\n",
        "\n",
        "merged['2_1_o/1_1_att_abs(diff)'] = abs(merged['attr2_1_o'] - merged['attr1_1'])\n",
        "merged['2_1_o/1_1_sinc_abs(diff)'] = abs(merged['sinc2_1_o'] - merged['sinc1_1'])\n",
        "merged['2_1_o/1_1_fun_abs(diff)'] = abs(merged['fun2_1_o'] - merged['fun1_1'])\n",
        "merged['2_1_o/1_1_intel_abs(diff)'] = abs(merged['intel2_1_o'] - merged['intel1_1'])\n",
        "merged['2_1_o/1_1_amb_abs(diff)'] = abs(merged['amb2_1_o'] - merged['amb1_1'])\n",
        "merged['2_1_o/1_1_shar_abs(diff)'] = abs(merged['shar2_1_o'] - merged['shar1_1'])\n",
        "\n",
        "merged['2_1/pf_o_att_abs(diff)'] = abs(merged['attr2_1'] - merged['pf_o_att'])\n",
        "merged['2_1/pf_o_sinc_abs(diff)'] = abs(merged['sinc2_1'] - merged['pf_o_sin'])\n",
        "merged['2_1/pf_o_fun_abs(diff)'] = abs(merged['fun2_1'] - merged['pf_o_fun'])\n",
        "merged['2_1/pf_o_intel_abs(diff)'] = abs(merged['intel2_1'] - merged['pf_o_int'])\n",
        "merged['2_1/pf_o_amb_abs(diff)'] = abs(merged['amb2_1'] - merged['pf_o_amb'])\n",
        "merged['2_1/pf_o_shar_abs(diff)'] = abs(merged['shar2_1'] - merged['pf_o_sha'])\n",
        "\n",
        "# Compute difference between responses for income/income_o, ... etc.\n",
        "# The partner (represented by PID) is the value that is being subtracted from\n",
        "# (e.g. sports_o - sports). \n",
        "\n",
        "merged['age_diff'] = merged['age_o'] - merged['age']\n",
        "merged['income_diff'] = merged['income_o'] - merged['income']\n",
        "merged['date_diff'] = merged['date_o'] - merged['date']\n",
        "merged['go_out_diff'] = merged['go_out_o'] - merged['go_out']\n",
        "merged['sports_diff'] = merged['sports_o'] - merged['sports']\n",
        "merged['tvsport_diff'] = merged['tvsport_o'] - merged['tvsports']\n",
        "merged['exercise_diff'] = merged['exercise_o'] - merged['exercise']\n",
        "merged['dining_diff'] = merged['dining_o'] - merged['dining']\n",
        "merged['museums_diff'] = merged['museums_o'] - merged['museums']\n",
        "merged['art_diff'] = merged['art_o'] - merged['art']\n",
        "merged['hiking_diff'] = merged['hiking_o'] - merged['hiking']\n",
        "merged['gaming_diff'] = merged['gaming_o'] - merged['gaming']\n",
        "merged['clubbing_diff'] = merged['clubbing_o'] - merged['clubbing']\n",
        "merged['reading_diff'] = merged['reading_o'] - merged['reading']\n",
        "merged['tv_diff'] = merged['tv_o'] - merged['tv']\n",
        "merged['theater_diff'] = merged['theater_o'] - merged['theater']\n",
        "merged['movies_diff'] = merged['movies_o'] - merged['movies']\n",
        "merged['concerts_diff'] = merged['concerts_o'] - merged['concerts']\n",
        "merged['music_diff'] = merged['music_o'] - merged['music']\n",
        "merged['shopping_diff'] = merged['shopping_o'] - merged['shopping']\n",
        "merged['yoga_diff'] = merged['yoga_o'] - merged['yoga']\n",
        "merged['worldrank_diff'] = merged['world_rank_o'] - merged['world_rank']\n",
        "\n",
        "merged['(3_1-pf_o)_att'] =  merged['attr3_1'] - merged['pf_o_att']\n",
        "merged['(3_1-pf_o)_sinc'] = merged['sinc3_1'] - merged['pf_o_sin']\n",
        "merged['(3_1-pf_o)_fun'] =  merged['fun3_1'] - merged['pf_o_fun']\n",
        "merged['(3_1-pf_o)_intel'] = merged['intel3_1'] - merged['pf_o_int']\n",
        "merged['(3_1-pf_o)_amb'] = merged['amb3_1'] - merged['pf_o_amb']\n",
        "\n",
        "merged['(1_1-3_1_o)_att'] = merged['attr1_1'] - merged['attr3_1_o']\n",
        "merged['(1_1-3_1_o)_sinc'] = merged['sinc1_1'] - merged['sinc3_1_o']\n",
        "merged['(1_1-3_1_o)_fun'] = merged['fun1_1'] - merged['fun3_1_o']\n",
        "merged['(1_1-3_1_o)_intel'] = merged['intel1_1'] - merged['intel3_1_o']\n",
        "merged['(1_1-3_1_o)_amb'] = merged['amb1_1'] - merged['amb3_1_o']\n",
        "\n",
        "merged['(1_1-2_1_o)_att'] = merged['attr1_1'] - merged['attr2_1_o']\n",
        "merged['(1_1-2_1_o)_sinc'] = merged['sinc1_1'] - merged['sinc2_1_o']\n",
        "merged['(1_1-2_1_o)_fun'] = merged['fun1_1'] - merged['fun2_1_o']\n",
        "merged['(1_1-2_1_o)_intel'] = merged['intel1_1'] - merged['intel2_1_o']\n",
        "merged['(1_1-2_1_o)_amb'] = merged['amb1_1'] - merged['amb2_1_o']\n",
        "merged['(1_1-2_1_o)_shar'] = merged['shar1_1'] - merged['shar2_1_o']\n",
        "\n",
        "merged['(2_1-pf_o)_att'] = merged['attr2_1'] - merged['pf_o_att']\n",
        "merged['(2_1-pf_o)_sinc'] = merged['sinc2_1'] - merged['pf_o_sin']\n",
        "merged['(2_1-pf_o)_fun'] = merged['fun2_1'] - merged['pf_o_fun']\n",
        "merged['(2_1-pf_o)_intel'] = merged['intel2_1'] - merged['pf_o_int']\n",
        "merged['(2_1-pf_o)_amb'] = merged['amb2_1'] - merged['pf_o_amb']\n",
        "merged['(2_1-pf_o)_shar'] = merged['shar2_1'] - merged['pf_o_sha']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "h43N8V0V5MvQ",
        "outputId": "02311025-3226-454e-a360-7c0b3612207d"
      },
      "source": [
        "# Compute whether from, goal, imprace, impreligion, career_c, masters are the same.\n",
        "\n",
        "merged[\"from_m\"] = (merged[\"from\"] == merged[\"from_o\"])*1\n",
        "display(merged[\"from_m\"].value_counts())\n",
        "\n",
        "merged[\"goal_m\"] = (merged[\"goal\"] == merged[\"goal_o\"])*1\n",
        "display(merged[\"goal_m\"].value_counts())\n",
        "\n",
        "merged[\"imprace_m\"] = (merged[\"imprace\"] == merged[\"imprace_o\"])*1\n",
        "display(merged[\"imprace_m\"].value_counts())\n",
        "\n",
        "merged[\"imprelig_m\"] = (merged[\"imprelig\"] == merged[\"imprelig_o\"])*1\n",
        "display(merged[\"imprelig_m\"].value_counts())\n",
        "\n",
        "merged[\"career_c_m\"] = (merged[\"career_c\"] == merged[\"career_c_o\"])*1\n",
        "display(merged[\"career_c_m\"].value_counts())\n",
        "\n",
        "merged[\"masters_m\"] = (merged[\"masters\"] == merged[\"masters_o\"])*1\n",
        "display(merged[\"masters_m\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    9337\n",
              "1      89\n",
              "Name: from_m, dtype: int64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    6571\n",
              "1    2855\n",
              "Name: goal_m, dtype: int64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    7872\n",
              "1    1554\n",
              "Name: imprace_m, dtype: int64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    7847\n",
              "1    1579\n",
              "Name: imprelig_m, dtype: int64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0    7720\n",
              "1    1706\n",
              "Name: career_c_m, dtype: int64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1    7254\n",
              "0    2172\n",
              "Name: masters_m, dtype: int64"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NNG2kWVx8O_T",
        "outputId": "513f8904-40ea-4f0e-fa14-7bd5c87e7736"
      },
      "source": [
        "display(np.array(merged.columns))\n",
        "merged.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array(['iid', 'id', 'gender', 'idg', 'condtn', 'wave', 'round',\n",
              "       'position', 'order', 'partner', 'pid', 'match', 'int_corr',\n",
              "       'samerace', 'age_o', 'race_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int',\n",
              "       'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'dec_o', 'attr_o', 'sinc_o',\n",
              "       'intel_o', 'fun_o', 'amb_o', 'shar_o', 'met_o', 'age', 'field',\n",
              "       'field_cd', 'mn_sat', 'tuition', 'race', 'imprace', 'imprelig',\n",
              "       'from', 'income', 'goal', 'date', 'go_out', 'career', 'career_c',\n",
              "       'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art',\n",
              "       'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n",
              "       'movies', 'concerts', 'music', 'shopping', 'yoga', 'exphappy',\n",
              "       'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1',\n",
              "       'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1',\n",
              "       'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', 'dec', 'met',\n",
              "       'match_es', 'world_rank', 'masters', 'field_cd_o', 'mn_sat_o',\n",
              "       'tuition_o', 'imprace_o', 'imprelig_o', 'from_o', 'income_o',\n",
              "       'goal_o', 'date_o', 'go_out_o', 'career_c_o', 'sports_o',\n",
              "       'tvsport_o', 'exercise_o', 'dining_o', 'museums_o', 'art_o',\n",
              "       'hiking_o', 'gaming_o', 'clubbing_o', 'reading_o', 'tv_o',\n",
              "       'theater_o', 'movies_o', 'concerts_o', 'music_o', 'shopping_o',\n",
              "       'yoga_o', 'exphappy_o', 'attr2_1_o', 'sinc2_1_o', 'intel2_1_o',\n",
              "       'fun2_1_o', 'amb2_1_o', 'shar2_1_o', 'attr3_1_o', 'sinc3_1_o',\n",
              "       'fun3_1_o', 'intel3_1_o', 'amb3_1_o', 'world_rank_o', 'masters_o',\n",
              "       'age_abs(diff)', 'income_abs(diff)', 'date_abs(diff)',\n",
              "       'go_out_abs(diff)', 'sports_abs(diff)', 'tvsport_abs(diff)',\n",
              "       'exercise_abs(diff)', 'dining_abs(diff)', 'museums_abs(diff)',\n",
              "       'art_abs(diff)', 'hiking_abs(diff)', 'gaming_abs(diff)',\n",
              "       'clubbing_abs(diff)', 'reading_abs(diff)', 'tv_abs(diff)',\n",
              "       'theater_abs(diff)', 'movies_abs(diff)', 'concerts_abs(diff)',\n",
              "       'music_abs(diff)', 'shopping_abs(diff)', 'yoga_abs(diff)',\n",
              "       'worldrank_abs(diff)', 'pf_o/3_1_att_abs(diff)',\n",
              "       'pf_o/3_1_sinc_abs(diff)', 'pf_o/3_1_fun_abs(diff)',\n",
              "       'pf_o/3_1_intel_abs(diff)', 'pf_o/3_1_amb_abs(diff)',\n",
              "       '1_1/3_1_o_att_abs(diff)', '1_1/3_1_o_sinc_abs(diff)',\n",
              "       '1_1/3_1_o_fun_abs(diff)', '1_1/3_1_o_intel_abs(diff)',\n",
              "       '1_1/3_1_o_amb_abs(diff)', '2_1_o/1_1_att_abs(diff)',\n",
              "       '2_1_o/1_1_sinc_abs(diff)', '2_1_o/1_1_fun_abs(diff)',\n",
              "       '2_1_o/1_1_intel_abs(diff)', '2_1_o/1_1_amb_abs(diff)',\n",
              "       '2_1_o/1_1_shar_abs(diff)', '2_1/pf_o_att_abs(diff)',\n",
              "       '2_1/pf_o_sinc_abs(diff)', '2_1/pf_o_fun_abs(diff)',\n",
              "       '2_1/pf_o_intel_abs(diff)', '2_1/pf_o_amb_abs(diff)',\n",
              "       '2_1/pf_o_shar_abs(diff)', 'age_diff', 'income_diff', 'date_diff',\n",
              "       'go_out_diff', 'sports_diff', 'tvsport_diff', 'exercise_diff',\n",
              "       'dining_diff', 'museums_diff', 'art_diff', 'hiking_diff',\n",
              "       'gaming_diff', 'clubbing_diff', 'reading_diff', 'tv_diff',\n",
              "       'theater_diff', 'movies_diff', 'concerts_diff', 'music_diff',\n",
              "       'shopping_diff', 'yoga_diff', 'worldrank_diff', '(3_1-pf_o)_att',\n",
              "       '(3_1-pf_o)_sinc', '(3_1-pf_o)_fun', '(3_1-pf_o)_intel',\n",
              "       '(3_1-pf_o)_amb', '(1_1-3_1_o)_att', '(1_1-3_1_o)_sinc',\n",
              "       '(1_1-3_1_o)_fun', '(1_1-3_1_o)_intel', '(1_1-3_1_o)_amb',\n",
              "       '(1_1-2_1_o)_att', '(1_1-2_1_o)_sinc', '(1_1-2_1_o)_fun',\n",
              "       '(1_1-2_1_o)_intel', '(1_1-2_1_o)_amb', '(1_1-2_1_o)_shar',\n",
              "       '(2_1-pf_o)_att', '(2_1-pf_o)_sinc', '(2_1-pf_o)_fun',\n",
              "       '(2_1-pf_o)_intel', '(2_1-pf_o)_amb', '(2_1-pf_o)_shar', 'from_m',\n",
              "       'goal_m', 'imprace_m', 'imprelig_m', 'career_c_m', 'masters_m'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-461532f7-5ba9-41eb-92d9-c9463f3c639b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iid</th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>idg</th>\n",
              "      <th>condtn</th>\n",
              "      <th>wave</th>\n",
              "      <th>round</th>\n",
              "      <th>position</th>\n",
              "      <th>order</th>\n",
              "      <th>partner</th>\n",
              "      <th>pid</th>\n",
              "      <th>match</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>race_o</th>\n",
              "      <th>pf_o_att</th>\n",
              "      <th>pf_o_sin</th>\n",
              "      <th>pf_o_int</th>\n",
              "      <th>pf_o_fun</th>\n",
              "      <th>pf_o_amb</th>\n",
              "      <th>pf_o_sha</th>\n",
              "      <th>dec_o</th>\n",
              "      <th>attr_o</th>\n",
              "      <th>sinc_o</th>\n",
              "      <th>intel_o</th>\n",
              "      <th>fun_o</th>\n",
              "      <th>amb_o</th>\n",
              "      <th>shar_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>age</th>\n",
              "      <th>field</th>\n",
              "      <th>field_cd</th>\n",
              "      <th>mn_sat</th>\n",
              "      <th>tuition</th>\n",
              "      <th>race</th>\n",
              "      <th>imprace</th>\n",
              "      <th>imprelig</th>\n",
              "      <th>from</th>\n",
              "      <th>income</th>\n",
              "      <th>...</th>\n",
              "      <th>hiking_diff</th>\n",
              "      <th>gaming_diff</th>\n",
              "      <th>clubbing_diff</th>\n",
              "      <th>reading_diff</th>\n",
              "      <th>tv_diff</th>\n",
              "      <th>theater_diff</th>\n",
              "      <th>movies_diff</th>\n",
              "      <th>concerts_diff</th>\n",
              "      <th>music_diff</th>\n",
              "      <th>shopping_diff</th>\n",
              "      <th>yoga_diff</th>\n",
              "      <th>worldrank_diff</th>\n",
              "      <th>(3_1-pf_o)_att</th>\n",
              "      <th>(3_1-pf_o)_sinc</th>\n",
              "      <th>(3_1-pf_o)_fun</th>\n",
              "      <th>(3_1-pf_o)_intel</th>\n",
              "      <th>(3_1-pf_o)_amb</th>\n",
              "      <th>(1_1-3_1_o)_att</th>\n",
              "      <th>(1_1-3_1_o)_sinc</th>\n",
              "      <th>(1_1-3_1_o)_fun</th>\n",
              "      <th>(1_1-3_1_o)_intel</th>\n",
              "      <th>(1_1-3_1_o)_amb</th>\n",
              "      <th>(1_1-2_1_o)_att</th>\n",
              "      <th>(1_1-2_1_o)_sinc</th>\n",
              "      <th>(1_1-2_1_o)_fun</th>\n",
              "      <th>(1_1-2_1_o)_intel</th>\n",
              "      <th>(1_1-2_1_o)_amb</th>\n",
              "      <th>(1_1-2_1_o)_shar</th>\n",
              "      <th>(2_1-pf_o)_att</th>\n",
              "      <th>(2_1-pf_o)_sinc</th>\n",
              "      <th>(2_1-pf_o)_fun</th>\n",
              "      <th>(2_1-pf_o)_intel</th>\n",
              "      <th>(2_1-pf_o)_amb</th>\n",
              "      <th>(2_1-pf_o)_shar</th>\n",
              "      <th>from_m</th>\n",
              "      <th>goal_m</th>\n",
              "      <th>imprace_m</th>\n",
              "      <th>imprelig_m</th>\n",
              "      <th>career_c_m</th>\n",
              "      <th>masters_m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Chicago</td>\n",
              "      <td>69487.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-29.0</td>\n",
              "      <td>-12.0</td>\n",
              "      <td>-12.0</td>\n",
              "      <td>-12.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Alabama</td>\n",
              "      <td>65929.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-28.0</td>\n",
              "      <td>-15.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-12.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>-20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>Economics</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Connecticut</td>\n",
              "      <td>43367.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-27.0</td>\n",
              "      <td>-11.0</td>\n",
              "      <td>-12.0</td>\n",
              "      <td>-11.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>-15.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>-20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Texas</td>\n",
              "      <td>37754.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-28.0</td>\n",
              "      <td>-12.0</td>\n",
              "      <td>-11.0</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-15.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>Law</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Bowdoin College</td>\n",
              "      <td>86340.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-29.0</td>\n",
              "      <td>-17.0</td>\n",
              "      <td>-14.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-15.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 221 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-461532f7-5ba9-41eb-92d9-c9463f3c639b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-461532f7-5ba9-41eb-92d9-c9463f3c639b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-461532f7-5ba9-41eb-92d9-c9463f3c639b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    iid   id  gender  idg  ...  imprace_m  imprelig_m  career_c_m  masters_m\n",
              "0     1  1.0       0    1  ...          0           0           0          1\n",
              "10    2  2.0       0    3  ...          0           0           0          1\n",
              "20    3  3.0       0    5  ...          0           0           0          1\n",
              "30    4  4.0       0    7  ...          0           0           0          1\n",
              "40    5  5.0       0    9  ...          0           0           0          1\n",
              "\n",
              "[5 rows x 221 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "AmahFM8g1B2B",
        "outputId": "eed3cf72-0adb-417f-9d74-159bed758fba"
      },
      "source": [
        "# Remove identifier columns not used in analysis.\n",
        "\n",
        "merged = merged.drop(columns=['iid', 'id', 'wave', 'idg', 'round', 'position', 'partner', 'pid'])\n",
        "\n",
        "# Due to prior encoding in match columns, as well as the \n",
        "# large number of categories in field/field_cd, race, from, goal, career/career_c\n",
        "# features, we will drop them.\n",
        "\n",
        "merged = merged.drop(columns=['field', 'field_cd', 'race', 'from', 'goal', 'career', 'career_c'])\n",
        "merged = merged.drop(columns=['field_cd_o', 'race_o', 'from_o', 'goal_o', 'career_c_o'])\n",
        "\n",
        "# Because we will compute models using difference columns, we will not use sports...yoga. \n",
        "\n",
        "merged = merged.drop(columns = ['sports', 'tvsports', 'exercise', 'dining', 'museums', 'art',\n",
        "       'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n",
        "       'movies', 'concerts', 'music', 'shopping', 'yoga'])\n",
        "merged = merged.drop(columns = ['sports_o',\n",
        "       'tvsport_o', 'exercise_o', 'dining_o', 'museums_o', 'art_o',\n",
        "       'hiking_o', 'gaming_o', 'clubbing_o', 'reading_o', 'tv_o',\n",
        "       'theater_o', 'movies_o', 'concerts_o', 'music_o', 'shopping_o',\n",
        "       'yoga_o'])\n",
        "\n",
        "display(merged.shape)\n",
        "display(np.array(merged.columns))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(9426, 167)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array(['gender', 'condtn', 'order', 'match', 'int_corr', 'samerace',\n",
              "       'age_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun',\n",
              "       'pf_o_amb', 'pf_o_sha', 'dec_o', 'attr_o', 'sinc_o', 'intel_o',\n",
              "       'fun_o', 'amb_o', 'shar_o', 'met_o', 'age', 'mn_sat', 'tuition',\n",
              "       'imprace', 'imprelig', 'income', 'date', 'go_out', 'exphappy',\n",
              "       'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1',\n",
              "       'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1',\n",
              "       'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', 'dec', 'met',\n",
              "       'match_es', 'world_rank', 'masters', 'mn_sat_o', 'tuition_o',\n",
              "       'imprace_o', 'imprelig_o', 'income_o', 'date_o', 'go_out_o',\n",
              "       'exphappy_o', 'attr2_1_o', 'sinc2_1_o', 'intel2_1_o', 'fun2_1_o',\n",
              "       'amb2_1_o', 'shar2_1_o', 'attr3_1_o', 'sinc3_1_o', 'fun3_1_o',\n",
              "       'intel3_1_o', 'amb3_1_o', 'world_rank_o', 'masters_o',\n",
              "       'age_abs(diff)', 'income_abs(diff)', 'date_abs(diff)',\n",
              "       'go_out_abs(diff)', 'sports_abs(diff)', 'tvsport_abs(diff)',\n",
              "       'exercise_abs(diff)', 'dining_abs(diff)', 'museums_abs(diff)',\n",
              "       'art_abs(diff)', 'hiking_abs(diff)', 'gaming_abs(diff)',\n",
              "       'clubbing_abs(diff)', 'reading_abs(diff)', 'tv_abs(diff)',\n",
              "       'theater_abs(diff)', 'movies_abs(diff)', 'concerts_abs(diff)',\n",
              "       'music_abs(diff)', 'shopping_abs(diff)', 'yoga_abs(diff)',\n",
              "       'worldrank_abs(diff)', 'pf_o/3_1_att_abs(diff)',\n",
              "       'pf_o/3_1_sinc_abs(diff)', 'pf_o/3_1_fun_abs(diff)',\n",
              "       'pf_o/3_1_intel_abs(diff)', 'pf_o/3_1_amb_abs(diff)',\n",
              "       '1_1/3_1_o_att_abs(diff)', '1_1/3_1_o_sinc_abs(diff)',\n",
              "       '1_1/3_1_o_fun_abs(diff)', '1_1/3_1_o_intel_abs(diff)',\n",
              "       '1_1/3_1_o_amb_abs(diff)', '2_1_o/1_1_att_abs(diff)',\n",
              "       '2_1_o/1_1_sinc_abs(diff)', '2_1_o/1_1_fun_abs(diff)',\n",
              "       '2_1_o/1_1_intel_abs(diff)', '2_1_o/1_1_amb_abs(diff)',\n",
              "       '2_1_o/1_1_shar_abs(diff)', '2_1/pf_o_att_abs(diff)',\n",
              "       '2_1/pf_o_sinc_abs(diff)', '2_1/pf_o_fun_abs(diff)',\n",
              "       '2_1/pf_o_intel_abs(diff)', '2_1/pf_o_amb_abs(diff)',\n",
              "       '2_1/pf_o_shar_abs(diff)', 'age_diff', 'income_diff', 'date_diff',\n",
              "       'go_out_diff', 'sports_diff', 'tvsport_diff', 'exercise_diff',\n",
              "       'dining_diff', 'museums_diff', 'art_diff', 'hiking_diff',\n",
              "       'gaming_diff', 'clubbing_diff', 'reading_diff', 'tv_diff',\n",
              "       'theater_diff', 'movies_diff', 'concerts_diff', 'music_diff',\n",
              "       'shopping_diff', 'yoga_diff', 'worldrank_diff', '(3_1-pf_o)_att',\n",
              "       '(3_1-pf_o)_sinc', '(3_1-pf_o)_fun', '(3_1-pf_o)_intel',\n",
              "       '(3_1-pf_o)_amb', '(1_1-3_1_o)_att', '(1_1-3_1_o)_sinc',\n",
              "       '(1_1-3_1_o)_fun', '(1_1-3_1_o)_intel', '(1_1-3_1_o)_amb',\n",
              "       '(1_1-2_1_o)_att', '(1_1-2_1_o)_sinc', '(1_1-2_1_o)_fun',\n",
              "       '(1_1-2_1_o)_intel', '(1_1-2_1_o)_amb', '(1_1-2_1_o)_shar',\n",
              "       '(2_1-pf_o)_att', '(2_1-pf_o)_sinc', '(2_1-pf_o)_fun',\n",
              "       '(2_1-pf_o)_intel', '(2_1-pf_o)_amb', '(2_1-pf_o)_shar', 'from_m',\n",
              "       'goal_m', 'imprace_m', 'imprelig_m', 'career_c_m', 'masters_m'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWgH5An04Ewy",
        "outputId": "1fdc3fe5-4917-435a-c68e-7573428f5a0d"
      },
      "source": [
        "# Drop rows missing >= 5 features of data.\n",
        "\n",
        "j = merged.isnull().sum(axis = 1).sort_values(ascending=False)\n",
        "drop_indices = []\n",
        "for i in j.index:\n",
        "  if j[i] >= 5:\n",
        "    drop_indices.append(i)\n",
        "merged = merged.drop(labels=drop_indices)\n",
        "merged.isnull().sum(axis = 1).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9007    4\n",
              "8256    4\n",
              "8955    4\n",
              "9005    4\n",
              "9006    4\n",
              "       ..\n",
              "6106    0\n",
              "5767    0\n",
              "5807    0\n",
              "5827    0\n",
              "0       0\n",
              "Length: 8930, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "rGabvW_l6G3E",
        "outputId": "ff423526-8520-41e5-9c3d-16419b9686a2"
      },
      "source": [
        "del speed\n",
        "display(merged.shape)\n",
        "merged.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(8930, 167)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-83a85ed1-0eb1-4bd1-b62f-8d3c45ff8ea1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>condtn</th>\n",
              "      <th>order</th>\n",
              "      <th>match</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>pf_o_att</th>\n",
              "      <th>pf_o_sin</th>\n",
              "      <th>pf_o_int</th>\n",
              "      <th>pf_o_fun</th>\n",
              "      <th>pf_o_amb</th>\n",
              "      <th>pf_o_sha</th>\n",
              "      <th>dec_o</th>\n",
              "      <th>attr_o</th>\n",
              "      <th>sinc_o</th>\n",
              "      <th>intel_o</th>\n",
              "      <th>fun_o</th>\n",
              "      <th>amb_o</th>\n",
              "      <th>shar_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>age</th>\n",
              "      <th>mn_sat</th>\n",
              "      <th>tuition</th>\n",
              "      <th>imprace</th>\n",
              "      <th>imprelig</th>\n",
              "      <th>income</th>\n",
              "      <th>date</th>\n",
              "      <th>go_out</th>\n",
              "      <th>exphappy</th>\n",
              "      <th>attr1_1</th>\n",
              "      <th>sinc1_1</th>\n",
              "      <th>intel1_1</th>\n",
              "      <th>fun1_1</th>\n",
              "      <th>amb1_1</th>\n",
              "      <th>shar1_1</th>\n",
              "      <th>attr2_1</th>\n",
              "      <th>sinc2_1</th>\n",
              "      <th>intel2_1</th>\n",
              "      <th>fun2_1</th>\n",
              "      <th>...</th>\n",
              "      <th>hiking_diff</th>\n",
              "      <th>gaming_diff</th>\n",
              "      <th>clubbing_diff</th>\n",
              "      <th>reading_diff</th>\n",
              "      <th>tv_diff</th>\n",
              "      <th>theater_diff</th>\n",
              "      <th>movies_diff</th>\n",
              "      <th>concerts_diff</th>\n",
              "      <th>music_diff</th>\n",
              "      <th>shopping_diff</th>\n",
              "      <th>yoga_diff</th>\n",
              "      <th>worldrank_diff</th>\n",
              "      <th>(3_1-pf_o)_att</th>\n",
              "      <th>(3_1-pf_o)_sinc</th>\n",
              "      <th>(3_1-pf_o)_fun</th>\n",
              "      <th>(3_1-pf_o)_intel</th>\n",
              "      <th>(3_1-pf_o)_amb</th>\n",
              "      <th>(1_1-3_1_o)_att</th>\n",
              "      <th>(1_1-3_1_o)_sinc</th>\n",
              "      <th>(1_1-3_1_o)_fun</th>\n",
              "      <th>(1_1-3_1_o)_intel</th>\n",
              "      <th>(1_1-3_1_o)_amb</th>\n",
              "      <th>(1_1-2_1_o)_att</th>\n",
              "      <th>(1_1-2_1_o)_sinc</th>\n",
              "      <th>(1_1-2_1_o)_fun</th>\n",
              "      <th>(1_1-2_1_o)_intel</th>\n",
              "      <th>(1_1-2_1_o)_amb</th>\n",
              "      <th>(1_1-2_1_o)_shar</th>\n",
              "      <th>(2_1-pf_o)_att</th>\n",
              "      <th>(2_1-pf_o)_sinc</th>\n",
              "      <th>(2_1-pf_o)_fun</th>\n",
              "      <th>(2_1-pf_o)_intel</th>\n",
              "      <th>(2_1-pf_o)_amb</th>\n",
              "      <th>(2_1-pf_o)_shar</th>\n",
              "      <th>from_m</th>\n",
              "      <th>goal_m</th>\n",
              "      <th>imprace_m</th>\n",
              "      <th>imprelig_m</th>\n",
              "      <th>career_c_m</th>\n",
              "      <th>masters_m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>69487.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-29.0</td>\n",
              "      <td>-12.0</td>\n",
              "      <td>-12.0</td>\n",
              "      <td>-12.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.29</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>65929.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-28.0</td>\n",
              "      <td>-15.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>-12.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>-20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.24</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>43367.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-27.0</td>\n",
              "      <td>-11.0</td>\n",
              "      <td>-12.0</td>\n",
              "      <td>-11.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>-15.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>-20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.18</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>37754.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-28.0</td>\n",
              "      <td>-12.0</td>\n",
              "      <td>-11.0</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-15.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.43</td>\n",
              "      <td>1</td>\n",
              "      <td>27.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1315.39</td>\n",
              "      <td>22096.85</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>86340.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-29.0</td>\n",
              "      <td>-17.0</td>\n",
              "      <td>-14.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>-15.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 167 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83a85ed1-0eb1-4bd1-b62f-8d3c45ff8ea1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-83a85ed1-0eb1-4bd1-b62f-8d3c45ff8ea1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-83a85ed1-0eb1-4bd1-b62f-8d3c45ff8ea1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    gender  condtn  order  match  ...  imprace_m  imprelig_m  career_c_m  masters_m\n",
              "0        0       1      4      0  ...          0           0           0          1\n",
              "10       0       1     10      0  ...          0           0           0          1\n",
              "20       0       1      6      0  ...          0           0           0          1\n",
              "30       0       1      3      0  ...          0           0           0          1\n",
              "40       0       1      1      0  ...          0           0           0          1\n",
              "\n",
              "[5 rows x 167 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZSsaHSo3amp"
      },
      "source": [
        "# Splitting Data into Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoVGuIvW83-K"
      },
      "source": [
        "We want to make 7 copies of our data (one for our matching/classification, and one for each of the traits we are trying to predict ratings for). We will then remove columns that are not relevant to the particular model that is being produced, then split each of the 7 groups into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8Z_YL9z-t7v"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For matching data, we:\n",
        "# Remove absolute columns for features that have absolute difference columns,\n",
        "# since each row we are looking at one pair rather than an individual.\n",
        "# (e.g. removing sports since sports_abs(diff) exists).\n",
        "# Remove difference columns for features that have absolute difference columns,\n",
        "# Remove absolute columns for features with isEqual rows (e.g. imprace)\n",
        "\n",
        "matching = merged.copy()\n",
        "matching = matching[['condtn', 'order', 'match', 'int_corr', 'samerace', 'met',\n",
        "       'age_abs(diff)', 'income_abs(diff)', 'date_abs(diff)',\n",
        "       'go_out_abs(diff)', 'sports_abs(diff)', 'tvsport_abs(diff)',\n",
        "       'exercise_abs(diff)', 'dining_abs(diff)', 'museums_abs(diff)',\n",
        "       'art_abs(diff)', 'hiking_abs(diff)', 'gaming_abs(diff)',\n",
        "       'clubbing_abs(diff)', 'reading_abs(diff)', 'tv_abs(diff)',\n",
        "       'theater_abs(diff)', 'movies_abs(diff)', 'concerts_abs(diff)',\n",
        "       'music_abs(diff)', 'shopping_abs(diff)', 'yoga_abs(diff)',\n",
        "       'worldrank_abs(diff)', 'pf_o/3_1_att_abs(diff)',\n",
        "       'pf_o/3_1_sinc_abs(diff)', 'pf_o/3_1_fun_abs(diff)',\n",
        "       'pf_o/3_1_intel_abs(diff)', 'pf_o/3_1_amb_abs(diff)',\n",
        "       '1_1/3_1_o_att_abs(diff)', '1_1/3_1_o_sinc_abs(diff)',\n",
        "       '1_1/3_1_o_fun_abs(diff)', '1_1/3_1_o_intel_abs(diff)',\n",
        "       '1_1/3_1_o_amb_abs(diff)', '2_1_o/1_1_att_abs(diff)',\n",
        "       '2_1_o/1_1_sinc_abs(diff)', '2_1_o/1_1_fun_abs(diff)',\n",
        "       '2_1_o/1_1_intel_abs(diff)', '2_1_o/1_1_amb_abs(diff)',\n",
        "       '2_1_o/1_1_shar_abs(diff)', '2_1/pf_o_att_abs(diff)',\n",
        "       '2_1/pf_o_sinc_abs(diff)', '2_1/pf_o_fun_abs(diff)',\n",
        "       '2_1/pf_o_intel_abs(diff)', '2_1/pf_o_amb_abs(diff)',\n",
        "       '2_1/pf_o_shar_abs(diff)', '(3_1-pf_o)_att', '(3_1-pf_o)_sinc',\n",
        "       '(3_1-pf_o)_fun', '(3_1-pf_o)_intel', '(3_1-pf_o)_amb',\n",
        "       '(1_1-3_1_o)_att', '(1_1-3_1_o)_sinc', '(1_1-3_1_o)_fun',\n",
        "       '(1_1-3_1_o)_intel', '(1_1-3_1_o)_amb', '(1_1-2_1_o)_att',\n",
        "       '(1_1-2_1_o)_sinc', '(1_1-2_1_o)_fun', '(1_1-2_1_o)_intel',\n",
        "       '(1_1-2_1_o)_amb', '(1_1-2_1_o)_shar', '(2_1-pf_o)_att',\n",
        "       '(2_1-pf_o)_sinc', '(2_1-pf_o)_fun', '(2_1-pf_o)_intel',\n",
        "       '(2_1-pf_o)_amb', '(2_1-pf_o)_shar']]\n",
        "\n",
        "# Drop any rows with NaN values, then split into train/test sets.\n",
        "\n",
        "matching = matching.dropna(axis=0)\n",
        "m_X_train, m_X_test, m_y_train, m_y_test = train_test_split(matching.drop(columns=['match']), matching['match'], test_size=0.20, random_state=42)\n",
        "\n",
        "# Normalize the data.\n",
        "\n",
        "m_X_train = ((m_X_train - m_X_train.mean())/m_X_train.std())\n",
        "m_X_test = ((m_X_test - m_X_test.mean())/m_X_test.std())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-na0eJ4NyY01",
        "outputId": "bd35c94c-0106-4330-c8f0-6237a0131ed3"
      },
      "source": [
        "np.array(merged.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['gender', 'condtn', 'order', 'match', 'int_corr', 'samerace',\n",
              "       'age_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun',\n",
              "       'pf_o_amb', 'pf_o_sha', 'dec_o', 'attr_o', 'sinc_o', 'intel_o',\n",
              "       'fun_o', 'amb_o', 'shar_o', 'met_o', 'age', 'mn_sat', 'tuition',\n",
              "       'imprace', 'imprelig', 'income', 'date', 'go_out', 'exphappy',\n",
              "       'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1',\n",
              "       'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1',\n",
              "       'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', 'dec', 'met',\n",
              "       'match_es', 'world_rank', 'masters', 'mn_sat_o', 'tuition_o',\n",
              "       'imprace_o', 'imprelig_o', 'income_o', 'date_o', 'go_out_o',\n",
              "       'exphappy_o', 'attr2_1_o', 'sinc2_1_o', 'intel2_1_o', 'fun2_1_o',\n",
              "       'amb2_1_o', 'shar2_1_o', 'attr3_1_o', 'sinc3_1_o', 'fun3_1_o',\n",
              "       'intel3_1_o', 'amb3_1_o', 'world_rank_o', 'masters_o',\n",
              "       'age_abs(diff)', 'income_abs(diff)', 'date_abs(diff)',\n",
              "       'go_out_abs(diff)', 'sports_abs(diff)', 'tvsport_abs(diff)',\n",
              "       'exercise_abs(diff)', 'dining_abs(diff)', 'museums_abs(diff)',\n",
              "       'art_abs(diff)', 'hiking_abs(diff)', 'gaming_abs(diff)',\n",
              "       'clubbing_abs(diff)', 'reading_abs(diff)', 'tv_abs(diff)',\n",
              "       'theater_abs(diff)', 'movies_abs(diff)', 'concerts_abs(diff)',\n",
              "       'music_abs(diff)', 'shopping_abs(diff)', 'yoga_abs(diff)',\n",
              "       'worldrank_abs(diff)', 'pf_o/3_1_att_abs(diff)',\n",
              "       'pf_o/3_1_sinc_abs(diff)', 'pf_o/3_1_fun_abs(diff)',\n",
              "       'pf_o/3_1_intel_abs(diff)', 'pf_o/3_1_amb_abs(diff)',\n",
              "       '1_1/3_1_o_att_abs(diff)', '1_1/3_1_o_sinc_abs(diff)',\n",
              "       '1_1/3_1_o_fun_abs(diff)', '1_1/3_1_o_intel_abs(diff)',\n",
              "       '1_1/3_1_o_amb_abs(diff)', '2_1_o/1_1_att_abs(diff)',\n",
              "       '2_1_o/1_1_sinc_abs(diff)', '2_1_o/1_1_fun_abs(diff)',\n",
              "       '2_1_o/1_1_intel_abs(diff)', '2_1_o/1_1_amb_abs(diff)',\n",
              "       '2_1_o/1_1_shar_abs(diff)', '2_1/pf_o_att_abs(diff)',\n",
              "       '2_1/pf_o_sinc_abs(diff)', '2_1/pf_o_fun_abs(diff)',\n",
              "       '2_1/pf_o_intel_abs(diff)', '2_1/pf_o_amb_abs(diff)',\n",
              "       '2_1/pf_o_shar_abs(diff)', 'age_diff', 'income_diff', 'date_diff',\n",
              "       'go_out_diff', 'sports_diff', 'tvsport_diff', 'exercise_diff',\n",
              "       'dining_diff', 'museums_diff', 'art_diff', 'hiking_diff',\n",
              "       'gaming_diff', 'clubbing_diff', 'reading_diff', 'tv_diff',\n",
              "       'theater_diff', 'movies_diff', 'concerts_diff', 'music_diff',\n",
              "       'shopping_diff', 'yoga_diff', 'worldrank_diff', '(3_1-pf_o)_att',\n",
              "       '(3_1-pf_o)_sinc', '(3_1-pf_o)_fun', '(3_1-pf_o)_intel',\n",
              "       '(3_1-pf_o)_amb', '(1_1-3_1_o)_att', '(1_1-3_1_o)_sinc',\n",
              "       '(1_1-3_1_o)_fun', '(1_1-3_1_o)_intel', '(1_1-3_1_o)_amb',\n",
              "       '(1_1-2_1_o)_att', '(1_1-2_1_o)_sinc', '(1_1-2_1_o)_fun',\n",
              "       '(1_1-2_1_o)_intel', '(1_1-2_1_o)_amb', '(1_1-2_1_o)_shar',\n",
              "       '(2_1-pf_o)_att', '(2_1-pf_o)_sinc', '(2_1-pf_o)_fun',\n",
              "       '(2_1-pf_o)_intel', '(2_1-pf_o)_amb', '(2_1-pf_o)_shar', 'from_m',\n",
              "       'goal_m', 'imprace_m', 'imprelig_m', 'career_c_m', 'masters_m'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq4qB0uufOtP"
      },
      "source": [
        "# For regression data, we:\n",
        "# Remove absolute columns for features that have difference columns.\n",
        "# Remove absolute difference for features that have difference columns.\n",
        "\n",
        "att = merged.copy()\n",
        "att = att[['gender', 'condtn', 'order', 'int_corr', 'samerace',\n",
        "       'age_o', 'mn_sat_o', 'tuition_o', 'income', 'exphappy_o', 'attr_o', 'sinc_o', 'intel_o',\n",
        "       'fun_o', 'amb_o', 'shar_o', 'met_o', 'world_rank_o', 'masters_o',\n",
        "       'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1',\n",
        "       'age_diff', 'income_diff', 'date_diff',\n",
        "       'go_out_diff', 'sports_diff', 'tvsport_diff', 'exercise_diff',\n",
        "       'dining_diff', 'museums_diff', 'art_diff', 'hiking_diff',\n",
        "       'gaming_diff', 'clubbing_diff', 'reading_diff', 'tv_diff',\n",
        "       'theater_diff', 'movies_diff', 'concerts_diff', 'music_diff',\n",
        "       'shopping_diff', 'yoga_diff', 'worldrank_diff', '(3_1-pf_o)_att',\n",
        "       '(3_1-pf_o)_sinc', '(3_1-pf_o)_fun', '(3_1-pf_o)_intel',\n",
        "       '(3_1-pf_o)_amb', '(1_1-2_1_o)_att', '(1_1-2_1_o)_sinc', '(1_1-2_1_o)_fun',\n",
        "       '(1_1-2_1_o)_intel', '(1_1-2_1_o)_amb', '(1_1-2_1_o)_shar',\n",
        "       'from_m','goal_m', 'imprace_m', 'imprelig_m', 'career_c_m', 'masters_m']]\n",
        "\n",
        "sin = att.copy()\n",
        "inte = att.copy()\n",
        "fun = att.copy()\n",
        "amb = att.copy()\n",
        "sha = att.copy()\n",
        "\n",
        "# Dropna on rows after removing non-relevant response variables, then\n",
        "# split into X and y.\n",
        "\n",
        "att = att.drop(columns=['sinc_o', 'intel_o',\n",
        "       'fun_o', 'amb_o', 'shar_o']).dropna(axis=0)\n",
        "att_y = att['attr_o']\n",
        "att = att.drop(columns=['attr_o'])\n",
        "\n",
        "sin = sin.drop(columns=['attr_o', 'intel_o',\n",
        "       'fun_o', 'amb_o', 'shar_o']).dropna(axis=0)\n",
        "sin_y = sin['sinc_o']\n",
        "sin = sin.drop(columns=['sinc_o'])\n",
        "\n",
        "inte = inte.drop(columns=['attr_o', 'sinc_o',\n",
        "       'fun_o', 'amb_o', 'shar_o']).dropna(axis=0)\n",
        "inte_y = inte['intel_o']\n",
        "inte = inte.drop(columns=['intel_o'])\n",
        "\n",
        "fun = fun.drop(columns=['attr_o', 'sinc_o', 'intel_o',\n",
        "       'amb_o', 'shar_o']).dropna(axis=0)\n",
        "fun_y = fun['fun_o']\n",
        "fun = fun.drop(columns=['fun_o'])\n",
        "\n",
        "amb = amb.drop(columns=['attr_o', 'sinc_o', 'intel_o',\n",
        "       'fun_o', 'shar_o']).dropna(axis=0)\n",
        "amb_y = amb['amb_o']\n",
        "amb = amb.drop(columns=['amb_o'])\n",
        "\n",
        "sha = sha.drop(columns=['attr_o', 'sinc_o', 'intel_o',\n",
        "       'fun_o', 'amb_o']).dropna(axis=0)\n",
        "sha_y = sha['shar_o']\n",
        "sha = sha.drop(columns=['shar_o'])\n",
        "\n",
        "# Train test split, then standardize data.\n",
        "\n",
        "att_X_train, att_X_test, att_y_train, att_y_test = train_test_split(att, att_y, test_size=0.20, random_state=42)\n",
        "att_X_train = ((att_X_train - att_X_train.mean())/att_X_train.std())\n",
        "att_X_test = ((att_X_test - att_X_test.mean())/att_X_test.std())\n",
        "att_test = att_X_test.copy()\n",
        "att_test['attr_o'] = att_y_test\n",
        "\n",
        "sin_X_train, sin_X_test, sin_y_train, sin_y_test = train_test_split(sin, sin_y, test_size=0.20, random_state=42)\n",
        "sin_X_train = ((sin_X_train - sin_X_train.mean())/sin_X_train.std())\n",
        "sin_X_test = ((sin_X_test - sin_X_test.mean())/sin_X_test.std())\n",
        "sin_test = sin_X_test.copy()\n",
        "sin_test['sinc_o'] = sin_y_test\n",
        "\n",
        "inte_X_train, inte_X_test, inte_y_train, inte_y_test = train_test_split(inte, inte_y, test_size=0.20, random_state=42)\n",
        "inte_X_train = ((inte_X_train - inte_X_train.mean())/inte_X_train.std())\n",
        "inte_X_test = ((inte_X_test - inte_X_test.mean())/inte_X_test.std())\n",
        "inte_test = inte_X_test.copy()\n",
        "inte_test['intel_o'] = inte_y_test\n",
        "inte_test_2 = inte_test.copy()\n",
        "\n",
        "fun_X_train, fun_X_test, fun_y_train, fun_y_test = train_test_split(fun, fun_y, test_size=0.20, random_state=42)\n",
        "fun_X_train = ((fun_X_train - fun_X_train.mean())/fun_X_train.std())\n",
        "fun_X_test = ((fun_X_test - fun_X_test.mean())/fun_X_test.std())\n",
        "fun_test = fun_X_test.copy()\n",
        "fun_test['fun_o'] = fun_y_test\n",
        "fun_test_2 = fun_test.copy()\n",
        "\n",
        "\n",
        "amb_X_train, amb_X_test, amb_y_train, amb_y_test = train_test_split(amb, amb_y, test_size=0.20, random_state=42)\n",
        "amb_X_train = ((amb_X_train - amb_X_train.mean())/amb_X_train.std())\n",
        "amb_X_test = ((amb_X_test - amb_X_test.mean())/amb_X_test.std())\n",
        "amb_test = amb_X_test.copy()\n",
        "amb_test['amb_o'] = amb_y_test\n",
        "\n",
        "sha_X_train, sha_X_test, sha_y_train, sha_y_test = train_test_split(sha, sha_y, test_size=0.20, random_state=42)\n",
        "sha_X_train = ((sha_X_train - sha_X_train.mean())/sha_X_train.std())\n",
        "sha_X_test = ((sha_X_test - sha_X_test.mean())/sha_X_test.std())\n",
        "sha_test = sha_X_test.copy()\n",
        "sha_test['shar_o'] = sha_y_test\n",
        "sha_test_2 = sha_test.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YOC79JeZDf_"
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAvuHqkr2ERq"
      },
      "source": [
        "def calc_acc(pred, actual, y_train=None):\n",
        "    return np.mean(pred == actual)\n",
        "\n",
        "def calc_recall(pred, actual, y_train=None):\n",
        "    cm = confusion_matrix(actual, pred)\n",
        "    return cm.ravel()[3]/(cm.ravel()[3]+cm.ravel()[2])\n",
        "\n",
        "def bootstrap_validation(test_data, test_label, train_label, model, metrics_list, sample=500, random_state=66):\n",
        "    n_sample = sample\n",
        "    n_metrics = len(metrics_list)\n",
        "    output_array=np.zeros([n_sample, n_metrics])\n",
        "    output_array[:]=np.nan\n",
        "    print(output_array.shape)\n",
        "    for bs_iter in range(n_sample):\n",
        "        bs_index = np.random.choice(test_data.index, len(test_data.index), replace=True)\n",
        "        bs_data = test_data.loc[bs_index]\n",
        "        bs_label = test_label.loc[bs_index]\n",
        "        bs_predicted = model.predict(bs_data)\n",
        "        for metrics_iter in range(n_metrics):\n",
        "            metrics = metrics_list[metrics_iter]\n",
        "            output_array[bs_iter, metrics_iter]=metrics(bs_predicted,bs_label,train_label)\n",
        "    output_df = pd.DataFrame(output_array)\n",
        "    return output_df\n",
        "\n",
        "def bootstrap_validation_cl(test_data, test_label, train_label, model, metrics_list, sample=500, random_state=66):\n",
        "    n_sample = sample\n",
        "    n_metrics = len(metrics_list)\n",
        "    output_array=np.zeros([n_sample, n_metrics])\n",
        "    output_array[:]=np.nan\n",
        "    print(output_array.shape)\n",
        "    for bs_iter in range(n_sample):\n",
        "        bs_index = np.random.choice(test_data.index, len(test_data.index), replace=True)\n",
        "        bs_data = test_data.loc[bs_index]\n",
        "        bs_label = test_label.loc[bs_index]\n",
        "        bs_predicted = model.predict_proba(bs_data)\n",
        "        bs_predicted = pd.Series([1 if x > 1/8 else 0 for x in bs_predicted[:,1]], index=test_label.index)\n",
        "        for metrics_iter in range(n_metrics):\n",
        "            metrics = metrics_list[metrics_iter]\n",
        "            output_array[bs_iter, metrics_iter]=metrics(bs_predicted,bs_label,train_label)\n",
        "    output_df = pd.DataFrame(output_array)\n",
        "    return output_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkJNAvp_WPda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd70f3a-0781-4095-b876-aaebd5637de0"
      },
      "source": [
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "def OSR_2(model, train_y, df_test, dependent_var):   \n",
        "    y_test = df_test[dependent_var]\n",
        "    y_pred = model.predict(df_test)\n",
        "    SSE = np.sum((y_test - y_pred)**2)\n",
        "    SST = np.sum((y_test - np.mean(train_y))**2)\n",
        "    return 1 - SSE/SST\n",
        "\n",
        "# Rescale to 0/10 before computing OSR2.\n",
        "\n",
        "def rescaled_OSR2_1(model, train_y, df_test, dependent_var, type):\n",
        "    y_test = df_test[dependent_var]\n",
        "    if type == 'lda':\n",
        "      y_pred = model.predict(sm.add_constant(df_test.drop(columns=dependent_var)))\n",
        "    else:\n",
        "      y_pred = model.predict(df_test)\n",
        "\n",
        "    # Keep domain of predicted values between 0 and 10.\n",
        "\n",
        "    y_pred = [min(max(x, 0), 10) for x in y_pred]\n",
        "    SSE = np.sum((y_test - y_pred)**2)\n",
        "    SST = np.sum((y_test - np.mean(train_y))**2)\n",
        "    return 1 - SSE/SST\n",
        "\n",
        "# Rescale linearly between 0 and 10 before computing OSR2.\n",
        "\n",
        "def rescaled_OSR2_2(model, train_y, df_test, dependent_var, type):\n",
        "    y_test = df_test[dependent_var]\n",
        "    if type == 'lda':\n",
        "      y_pred = model.predict(sm.add_constant(df_test.drop(columns=dependent_var)))\n",
        "    else:\n",
        "      y_pred = model.predict(df_test)\n",
        "\n",
        "    # Keep domain of predicted values between 0 and 10.\n",
        "\n",
        "    y_pred = rescale(y_pred)\n",
        "    SSE = np.sum((y_test - y_pred)**2)\n",
        "    SST = np.sum((y_test - np.mean(train_y))**2)\n",
        "    return 1 - SSE/SST\n",
        "\n",
        "# Rescale values to be between t_min, t_max.\n",
        "\n",
        "def rescale(y_pred, t_min=0, t_max=10):\n",
        "    r_min = np.min(y_pred)\n",
        "    r_max = np.max(y_pred)\n",
        "    return (((y_pred - r_min)/(r_max - r_min)) * (t_max - t_min)) + t_min\n",
        "\n",
        "def VIF(df, columns):\n",
        "    values = sm.add_constant(df[columns]).values\n",
        "    num_columns = len(columns)+1\n",
        "    vif = [variance_inflation_factor(values, i) for i in range(num_columns)]\n",
        "    return pd.Series(vif[1:], index=columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdPsEBDJZGOK"
      },
      "source": [
        "## Matches (Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITw_dRMgZmjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5a2208-1659-48bc-f534-4f5d05f06f16"
      },
      "source": [
        "print('Match Proportion:', m_y_train.mean())\n",
        "print('Baseline Accuracy:', 1 - m_y_test.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Match Proportion: 0.16678805535324107\n",
            "Baseline Accuracy: 0.8398369248689574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyht22uwZfTU"
      },
      "source": [
        "We note that the number of matches are relatively low (only 16.7% of pairs), with a baseline accuracy of 84.0% on the test set if we predict that no matches occur. However, in the context of the data, we are more concerned with correctly identifying true positives than we are with avoiding false positives; we (and the speed daters) don't expect or need every pair up to result in a match, but we definitely want to have pairs who are likely to match meet up.\n",
        "\n",
        "As such, we do not use accuracy as our main metric. We are more concerned with our false negative and recall ($\\frac{TP}{TP + FN}$) rates.\n",
        "\n",
        "For this application, we will weigh the loss of a false positive as 1, and the loss of a false negative as 7. That is, we would rather see an individual meet with 7 individuals who will not result in a match than miss one potential match. We believe this is a reasonable choice, given that a speed dating round lasts for ~5-8 minutes and the event lasts ~1 hour, which makes for about 8 pairs during the event."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIRy3P__S5hT"
      },
      "source": [
        "![fdsfafdsa.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1P4khWDdFwE"
      },
      "source": [
        "Thus our threshold for logistic regression is at $p$ such that:\n",
        "$$(1-p) = 7p$$\n",
        "$$p = \\frac{1}{8}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiuH2XFV8Jmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb00aaf-c500-4260-caf4-5ba99b35b6e9"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(random_state=88)\n",
        "logreg.fit(m_X_train, m_y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=88)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1bru8EnQXa1",
        "outputId": "54708e04-2e06-4408-ebc2-2419d931a226"
      },
      "source": [
        "# TODO: Matching model\n",
        "# Assigned: Jane\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "m_y_prob = logreg.predict_proba(m_X_test)\n",
        "m_y_pred = pd.Series([1 if x > 1/8 else 0 for x in m_y_prob[:,1]], index=m_y_test.index)\n",
        "\n",
        "cm = confusion_matrix(m_y_test, m_y_pred)\n",
        "print (\"Confusion Matrix: \\n\", cm)\n",
        "print (\"\\nAccuracy:\", accuracy_score(m_y_test, m_y_pred))\n",
        "print (\"\\nFalse Negative Rate:\", cm.ravel()[2]/(cm.ravel()[2]+cm.ravel()[3]))\n",
        "print (\"\\nRecall:\", cm.ravel()[3]/(cm.ravel()[3]+cm.ravel()[2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix: \n",
            " [[480 962]\n",
            " [ 46 229]]\n",
            "\n",
            "Accuracy: 0.41292952824694235\n",
            "\n",
            "False Negative Rate: 0.16727272727272727\n",
            "\n",
            "Recall: 0.8327272727272728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWIv9c8IzJIJ",
        "outputId": "82c43f76-57c9-4d94-c56a-57a766a4322a"
      },
      "source": [
        "match_boot = bootstrap_validation_cl(m_X_test, m_y_test, m_y_train, logreg, [calc_recall])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saCFPLNbz51P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "da3c3df0-2c6f-4a5d-c8a3-4e6e19671e4a"
      },
      "source": [
        "test_recall = calc_recall(m_y_pred, m_y_test)\n",
        "\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap Recall Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot Recall - Test Set Recall', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].axvline(np.mean(match_boot)[0], color='black')\n",
        "axs[1].axvline(np.mean(match_boot)[0] - test_recall, color='black')\n",
        "axs[0].hist(match_boot.iloc[:,0], bins=20,edgecolor='blue', linewidth=2,color = \"white\")\n",
        "axs[1].hist(match_boot.iloc[:,0]-test_recall, bins=20,edgecolor='blue', linewidth=2,color = \"white\");\n",
        "print('The mean bootstrap recall estimate for the logistic regression model is:', np.mean(match_boot)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean bootstrap recall estimate for the logistic regression model is: 0.831861017065495\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics\n",
        "match_CI = np.quantile(match_boot.iloc[:,0],np.array([0.025,0.975]))\n",
        "\n",
        "print('Mean: %.4f, Std: %.4f'%(statistics.mean(match_CI), statistics.stdev(match_CI)))\n",
        "print(\"The 95-percent confidence interval of the recall of the logistic regression model is %s\" % match_CI)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvgq2uZC3aSQ",
        "outputId": "081e2c53-e87f-4b86-ceff-2557c1ede429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 0.8351, Std: 0.0652\n",
            "The 95-percent confidence interval of the recall of the logistic regression model is [0.78896086 0.8812139 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFoPQ5_1r95P"
      },
      "source": [
        "## Attractiveness (Linear Regression + LDA)\n",
        "\n",
        "Make linear model for this trait, perform feature selection.\n",
        "Bootstrap your model and report the mean and 95% CI for OSR^2.\n",
        "\n",
        "Run LDA on this trait. Bootstrap this model and report the mean and 95% CI for accuracy. Calculate OSR^2 as well.\n",
        "\n",
        "Calculate rescaled OSR^2 using the two methods above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk05hTmwj2pM"
      },
      "source": [
        "## Attractiveness (Linear Regression - Feature Selection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q_jclczkIqO",
        "outputId": "0cd936f3-6579-4366-ea64-4e9c30984b8c"
      },
      "source": [
        "# Fit linear model for attr_o\n",
        "X_train = att_X_train.copy()\n",
        "y_train = att_y_train.copy()\n",
        "X_test = att_X_test.copy()\n",
        "y_test = att_y_test.copy()\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "X_train = sm.add_constant(X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "att_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(att_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 attr_o   R-squared:                       0.128\n",
            "Model:                            OLS   Adj. R-squared:                  0.121\n",
            "Method:                 Least Squares   F-statistic:                     17.30\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          1.63e-159\n",
            "Time:                        23:09:03   Log-Likelihood:                -13936.\n",
            "No. Observations:                6900   AIC:                         2.799e+04\n",
            "Df Residuals:                    6841   BIC:                         2.839e+04\n",
            "Df Model:                          58                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.1532      0.022    279.108      0.000       6.110       6.196\n",
            "gender               -0.3623      0.038     -9.446      0.000      -0.438      -0.287\n",
            "condtn                0.0422      0.024      1.752      0.080      -0.005       0.089\n",
            "order                -0.0642      0.023     -2.741      0.006      -0.110      -0.018\n",
            "int_corr              0.0441      0.023      1.938      0.053      -0.000       0.089\n",
            "samerace              0.0811      0.022      3.609      0.000       0.037       0.125\n",
            "age_o                 0.0323      0.032      1.017      0.309      -0.030       0.095\n",
            "mn_sat_o              0.0555      0.029      1.884      0.060      -0.002       0.113\n",
            "tuition_o            -0.0757      0.030     -2.565      0.010      -0.134      -0.018\n",
            "income                0.0061      0.032      0.192      0.848      -0.056       0.068\n",
            "exphappy_o            0.1056      0.024      4.413      0.000       0.059       0.153\n",
            "met_o                -0.1867      0.023     -8.296      0.000      -0.231      -0.143\n",
            "world_rank_o         -0.0568      0.030     -1.875      0.061      -0.116       0.003\n",
            "masters_o            -0.0773      0.029     -2.698      0.007      -0.134      -0.021\n",
            "attr1_1              -0.2166      0.198     -1.096      0.273      -0.604       0.171\n",
            "sinc1_1              -0.3780      0.121     -3.112      0.002      -0.616      -0.140\n",
            "intel1_1             -0.1660      0.120     -1.382      0.167      -0.401       0.069\n",
            "fun1_1               -0.1079      0.102     -1.062      0.288      -0.307       0.091\n",
            "amb1_1               -0.0760      0.101     -0.750      0.453      -0.275       0.123\n",
            "shar1_1              -0.3411      0.109     -3.136      0.002      -0.554      -0.128\n",
            "age_diff              0.0913      0.032      2.843      0.004       0.028       0.154\n",
            "income_diff          -0.0174      0.032     -0.538      0.591      -0.081       0.046\n",
            "date_diff             0.0271      0.025      1.102      0.271      -0.021       0.075\n",
            "go_out_diff           0.1409      0.025      5.688      0.000       0.092       0.189\n",
            "sports_diff          -0.1135      0.032     -3.554      0.000      -0.176      -0.051\n",
            "tvsport_diff          0.0894      0.029      3.071      0.002       0.032       0.146\n",
            "exercise_diff        -0.1502      0.026     -5.813      0.000      -0.201      -0.100\n",
            "dining_diff          -0.0760      0.027     -2.792      0.005      -0.129      -0.023\n",
            "museums_diff         -0.0125      0.047     -0.266      0.790      -0.105       0.080\n",
            "art_diff              0.0905      0.045      2.011      0.044       0.002       0.179\n",
            "hiking_diff          -0.0085      0.026     -0.324      0.746      -0.060       0.043\n",
            "gaming_diff           0.1394      0.027      5.174      0.000       0.087       0.192\n",
            "clubbing_diff        -0.0944      0.024     -3.936      0.000      -0.141      -0.047\n",
            "reading_diff          0.0745      0.025      3.009      0.003       0.026       0.123\n",
            "tv_diff               0.1245      0.030      4.191      0.000       0.066       0.183\n",
            "theater_diff          0.1551      0.033      4.660      0.000       0.090       0.220\n",
            "movies_diff          -0.0523      0.029     -1.799      0.072      -0.109       0.005\n",
            "concerts_diff        -0.0263      0.034     -0.785      0.432      -0.092       0.039\n",
            "music_diff           -0.0314      0.030     -1.031      0.303      -0.091       0.028\n",
            "shopping_diff         0.0539      0.032      1.689      0.091      -0.009       0.116\n",
            "yoga_diff            -0.0515      0.026     -2.004      0.045      -0.102      -0.001\n",
            "worldrank_diff       -0.0719      0.030     -2.434      0.015      -0.130      -0.014\n",
            "(3_1-pf_o)_att        0.0400      0.041      0.976      0.329      -0.040       0.120\n",
            "(3_1-pf_o)_sinc      -0.0073      0.030     -0.240      0.810      -0.067       0.052\n",
            "(3_1-pf_o)_fun        0.0110      0.028      0.397      0.691      -0.043       0.065\n",
            "(3_1-pf_o)_intel      0.0500      0.030      1.658      0.097      -0.009       0.109\n",
            "(3_1-pf_o)_amb       -0.0584      0.031     -1.900      0.058      -0.119       0.002\n",
            "(1_1-2_1_o)_att       0.2705      0.216      1.253      0.210      -0.153       0.694\n",
            "(1_1-2_1_o)_sinc      0.1932      0.124      1.554      0.120      -0.051       0.437\n",
            "(1_1-2_1_o)_fun       0.1970      0.112      1.756      0.079      -0.023       0.417\n",
            "(1_1-2_1_o)_intel     0.1163      0.118      0.989      0.323      -0.114       0.347\n",
            "(1_1-2_1_o)_amb       0.1425      0.105      1.363      0.173      -0.062       0.347\n",
            "(1_1-2_1_o)_shar      0.1317      0.110      1.195      0.232      -0.084       0.348\n",
            "from_m               -0.0366      0.022     -1.651      0.099      -0.080       0.007\n",
            "goal_m                0.0079      0.022      0.354      0.723      -0.036       0.051\n",
            "imprace_m            -0.0302      0.023     -1.333      0.182      -0.075       0.014\n",
            "imprelig_m            0.0434      0.023      1.909      0.056      -0.001       0.088\n",
            "career_c_m            0.0284      0.023      1.245      0.213      -0.016       0.073\n",
            "masters_m            -0.0692      0.028     -2.466      0.014      -0.124      -0.014\n",
            "==============================================================================\n",
            "Omnibus:                       99.007   Durbin-Watson:                   2.032\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              103.024\n",
            "Skew:                          -0.299   Prob(JB):                     4.25e-23\n",
            "Kurtosis:                       3.033   Cond. No.                         42.8\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGFRuH3hkYf5",
        "outputId": "ae5b10a6-d27d-4c0c-dc47-dd80f0acc8d1"
      },
      "source": [
        "#Check VIF values\n",
        "# The dataframe passed to VIF must include the intercept term. We add it the same way we did before.\n",
        "def VIF(df, columns):\n",
        "    values = sm.add_constant(df[columns]).values\n",
        "    num_columns = len(columns)+1\n",
        "    vif = [variance_inflation_factor(values, i) for i in range(num_columns)]\n",
        "    return pd.Series(vif[1:], index=columns).sort_values(ascending=False)\n",
        "\n",
        "att_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, att_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_att      95.883120\n",
              "attr1_1              80.324720\n",
              "(1_1-2_1_o)_sinc     31.819265\n",
              "sinc1_1              30.340062\n",
              "intel1_1             29.681515\n",
              "(1_1-2_1_o)_intel    28.427325\n",
              "(1_1-2_1_o)_fun      25.898424\n",
              "(1_1-2_1_o)_shar     24.979953\n",
              "shar1_1              24.340068\n",
              "(1_1-2_1_o)_amb      22.466463\n",
              "fun1_1               21.241661\n",
              "amb1_1               21.136814\n",
              "museums_diff          4.540347\n",
              "art_diff              4.162790\n",
              "(3_1-pf_o)_att        3.460373\n",
              "gender                3.027172\n",
              "concerts_diff         2.311600\n",
              "theater_diff          2.277512\n",
              "income_diff           2.158708\n",
              "age_diff              2.121926\n",
              "sports_diff           2.097854\n",
              "shopping_diff         2.091761\n",
              "income                2.074683\n",
              "age_o                 2.074345\n",
              "(3_1-pf_o)_amb        1.945996\n",
              "music_diff            1.908946\n",
              "(3_1-pf_o)_sinc       1.890507\n",
              "world_rank_o          1.887289\n",
              "(3_1-pf_o)_intel      1.873177\n",
              "tv_diff               1.814942\n",
              "worldrank_diff        1.794804\n",
              "tuition_o             1.791548\n",
              "mn_sat_o              1.782496\n",
              "tvsport_diff          1.742124\n",
              "movies_diff           1.736660\n",
              "masters_o             1.690483\n",
              "masters_m             1.620110\n",
              "(3_1-pf_o)_fun        1.566403\n",
              "dining_diff           1.525613\n",
              "gaming_diff           1.492976\n",
              "hiking_diff           1.418443\n",
              "exercise_diff         1.372547\n",
              "yoga_diff             1.357166\n",
              "go_out_diff           1.262198\n",
              "reading_diff          1.261089\n",
              "date_diff             1.246907\n",
              "condtn                1.194930\n",
              "clubbing_diff         1.182434\n",
              "exphappy_o            1.179161\n",
              "order                 1.127489\n",
              "career_c_m            1.074653\n",
              "int_corr              1.066348\n",
              "imprelig_m            1.065009\n",
              "imprace_m             1.057700\n",
              "met_o                 1.042074\n",
              "samerace              1.039602\n",
              "goal_m                1.017120\n",
              "from_m                1.009392\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCpRf5KYkhv8",
        "outputId": "7ff558bc-3cb0-4fa2-e557-a8f9958a81c7"
      },
      "source": [
        "#drop '(1_1-2_1_o)_att' since it has the highest VIF value\n",
        "X_train = X_train.drop(columns=['(1_1-2_1_o)_att'])\n",
        "\n",
        "att_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(att_model.summary())\n",
        "\n",
        "att_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, att_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 attr_o   R-squared:                       0.128\n",
            "Model:                            OLS   Adj. R-squared:                  0.120\n",
            "Method:                 Least Squares   F-statistic:                     17.58\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          8.10e-160\n",
            "Time:                        23:09:08   Log-Likelihood:                -13936.\n",
            "No. Observations:                6900   AIC:                         2.799e+04\n",
            "Df Residuals:                    6842   BIC:                         2.839e+04\n",
            "Df Model:                          57                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.1532      0.022    279.096      0.000       6.110       6.196\n",
            "gender               -0.3672      0.038     -9.622      0.000      -0.442      -0.292\n",
            "condtn                0.0420      0.024      1.744      0.081      -0.005       0.089\n",
            "order                -0.0645      0.023     -2.756      0.006      -0.110      -0.019\n",
            "int_corr              0.0443      0.023      1.947      0.052      -0.000       0.089\n",
            "samerace              0.0825      0.022      3.676      0.000       0.039       0.127\n",
            "age_o                 0.0345      0.032      1.089      0.276      -0.028       0.097\n",
            "mn_sat_o              0.0568      0.029      1.930      0.054      -0.001       0.114\n",
            "tuition_o            -0.0775      0.029     -2.631      0.009      -0.135      -0.020\n",
            "income                0.0078      0.032      0.247      0.805      -0.054       0.070\n",
            "exphappy_o            0.1052      0.024      4.394      0.000       0.058       0.152\n",
            "met_o                -0.1866      0.023     -8.291      0.000      -0.231      -0.142\n",
            "world_rank_o         -0.0595      0.030     -1.969      0.049      -0.119      -0.000\n",
            "masters_o            -0.0773      0.029     -2.696      0.007      -0.133      -0.021\n",
            "attr1_1              -0.0326      0.132     -0.247      0.805      -0.292       0.227\n",
            "sinc1_1              -0.2693      0.085     -3.169      0.002      -0.436      -0.103\n",
            "intel1_1             -0.0599      0.085     -0.703      0.482      -0.227       0.107\n",
            "fun1_1               -0.0189      0.073     -0.260      0.795      -0.161       0.124\n",
            "amb1_1                0.0153      0.070      0.217      0.828      -0.123       0.153\n",
            "shar1_1              -0.2457      0.078     -3.165      0.002      -0.398      -0.093\n",
            "age_diff              0.0906      0.032      2.822      0.005       0.028       0.154\n",
            "income_diff          -0.0151      0.032     -0.468      0.640      -0.079       0.048\n",
            "date_diff             0.0273      0.025      1.109      0.267      -0.021       0.076\n",
            "go_out_diff           0.1418      0.025      5.726      0.000       0.093       0.190\n",
            "sports_diff          -0.1130      0.032     -3.539      0.000      -0.176      -0.050\n",
            "tvsport_diff          0.0878      0.029      3.021      0.003       0.031       0.145\n",
            "exercise_diff        -0.1501      0.026     -5.811      0.000      -0.201      -0.099\n",
            "dining_diff          -0.0758      0.027     -2.782      0.005      -0.129      -0.022\n",
            "museums_diff         -0.0113      0.047     -0.242      0.809      -0.103       0.081\n",
            "art_diff              0.0878      0.045      1.954      0.051      -0.000       0.176\n",
            "hiking_diff          -0.0082      0.026     -0.314      0.754      -0.060       0.043\n",
            "gaming_diff           0.1408      0.027      5.231      0.000       0.088       0.194\n",
            "clubbing_diff        -0.0955      0.024     -3.986      0.000      -0.142      -0.049\n",
            "reading_diff          0.0755      0.025      3.050      0.002       0.027       0.124\n",
            "tv_diff               0.1234      0.030      4.157      0.000       0.065       0.182\n",
            "theater_diff          0.1547      0.033      4.649      0.000       0.089       0.220\n",
            "movies_diff          -0.0543      0.029     -1.870      0.062      -0.111       0.003\n",
            "concerts_diff        -0.0260      0.034     -0.776      0.438      -0.092       0.040\n",
            "music_diff           -0.0324      0.030     -1.062      0.288      -0.092       0.027\n",
            "shopping_diff         0.0538      0.032      1.688      0.091      -0.009       0.116\n",
            "yoga_diff            -0.0516      0.026     -2.008      0.045      -0.102      -0.001\n",
            "worldrank_diff       -0.0718      0.030     -2.430      0.015      -0.130      -0.014\n",
            "(3_1-pf_o)_att        0.0432      0.041      1.054      0.292      -0.037       0.123\n",
            "(3_1-pf_o)_sinc      -0.0076      0.030     -0.251      0.802      -0.067       0.052\n",
            "(3_1-pf_o)_fun        0.0103      0.028      0.374      0.708      -0.044       0.064\n",
            "(3_1-pf_o)_intel      0.0494      0.030      1.636      0.102      -0.010       0.109\n",
            "(3_1-pf_o)_amb       -0.0607      0.031     -1.978      0.048      -0.121      -0.001\n",
            "(1_1-2_1_o)_sinc      0.0446      0.037      1.195      0.232      -0.029       0.118\n",
            "(1_1-2_1_o)_fun       0.0629      0.034      1.873      0.061      -0.003       0.129\n",
            "(1_1-2_1_o)_intel    -0.0238      0.036     -0.656      0.512      -0.095       0.047\n",
            "(1_1-2_1_o)_amb       0.0175      0.031      0.562      0.574      -0.044       0.079\n",
            "(1_1-2_1_o)_shar      0.0010      0.035      0.027      0.978      -0.068       0.070\n",
            "from_m               -0.0367      0.022     -1.658      0.097      -0.080       0.007\n",
            "goal_m                0.0074      0.022      0.333      0.739      -0.036       0.051\n",
            "imprace_m            -0.0312      0.023     -1.376      0.169      -0.076       0.013\n",
            "imprelig_m            0.0437      0.023      1.919      0.055      -0.001       0.088\n",
            "career_c_m            0.0288      0.023      1.261      0.208      -0.016       0.074\n",
            "masters_m            -0.0683      0.028     -2.435      0.015      -0.123      -0.013\n",
            "==============================================================================\n",
            "Omnibus:                       98.858   Durbin-Watson:                   2.032\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              102.865\n",
            "Skew:                          -0.299   Prob(JB):                     4.60e-23\n",
            "Kurtosis:                       3.032   Cond. No.                         21.1\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "attr1_1              35.968707\n",
              "intel1_1             14.936833\n",
              "sinc1_1              14.856538\n",
              "shar1_1              12.398473\n",
              "fun1_1               10.857129\n",
              "amb1_1               10.198082\n",
              "museums_diff          4.538617\n",
              "art_diff              4.153362\n",
              "(3_1-pf_o)_att        3.447397\n",
              "gender                2.995974\n",
              "(1_1-2_1_o)_sinc      2.870162\n",
              "(1_1-2_1_o)_intel     2.704528\n",
              "(1_1-2_1_o)_shar      2.565785\n",
              "(1_1-2_1_o)_fun       2.323775\n",
              "concerts_diff         2.311463\n",
              "theater_diff          2.277325\n",
              "income_diff           2.151879\n",
              "age_diff              2.121300\n",
              "sports_diff           2.097533\n",
              "shopping_diff         2.091760\n",
              "income                2.070646\n",
              "age_o                 2.067829\n",
              "(1_1-2_1_o)_amb       2.001544\n",
              "(3_1-pf_o)_amb        1.939053\n",
              "music_diff            1.907767\n",
              "(3_1-pf_o)_sinc       1.890376\n",
              "world_rank_o          1.877699\n",
              "(3_1-pf_o)_intel      1.872610\n",
              "tv_diff               1.813526\n",
              "worldrank_diff        1.794777\n",
              "tuition_o             1.787085\n",
              "mn_sat_o              1.780174\n",
              "tvsport_diff          1.739043\n",
              "movies_diff           1.731471\n",
              "masters_o             1.690481\n",
              "masters_m             1.619059\n",
              "(3_1-pf_o)_fun        1.565873\n",
              "dining_diff           1.525516\n",
              "gaming_diff           1.490382\n",
              "hiking_diff           1.418353\n",
              "exercise_diff         1.372544\n",
              "yoga_diff             1.357147\n",
              "go_out_diff           1.261187\n",
              "reading_diff          1.259870\n",
              "date_diff             1.246865\n",
              "condtn                1.194881\n",
              "clubbing_diff         1.180753\n",
              "exphappy_o            1.178879\n",
              "order                 1.127327\n",
              "career_c_m            1.074482\n",
              "int_corr              1.066295\n",
              "imprelig_m            1.064934\n",
              "imprace_m             1.056540\n",
              "met_o                 1.042059\n",
              "samerace              1.037034\n",
              "goal_m                1.016829\n",
              "from_m                1.009357\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc7xzZe2k8kx"
      },
      "source": [
        "approved_removed_cols = []\n",
        "approved_removed_cols.append('(1_1-2_1_o)_att')  # other VIF values decreased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg0b7P8pkh4L",
        "outputId": "aff2f902-1a2f-4bdb-c246-0d021f911444"
      },
      "source": [
        "#drop 'attr1_1' since it has the highest VIF value\n",
        "X_train = X_train.drop(columns=['attr1_1'])\n",
        "\n",
        "att_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(att_model.summary())\n",
        "\n",
        "att_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, att_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 attr_o   R-squared:                       0.128\n",
            "Model:                            OLS   Adj. R-squared:                  0.121\n",
            "Method:                 Least Squares   F-statistic:                     17.89\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          1.96e-160\n",
            "Time:                        23:09:13   Log-Likelihood:                -13937.\n",
            "No. Observations:                6900   AIC:                         2.799e+04\n",
            "Df Residuals:                    6843   BIC:                         2.838e+04\n",
            "Df Model:                          56                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.1532      0.022    279.115      0.000       6.110       6.196\n",
            "gender               -0.3680      0.038     -9.681      0.000      -0.443      -0.294\n",
            "condtn                0.0424      0.024      1.762      0.078      -0.005       0.090\n",
            "order                -0.0646      0.023     -2.759      0.006      -0.110      -0.019\n",
            "int_corr              0.0443      0.023      1.947      0.052      -0.000       0.089\n",
            "samerace              0.0824      0.022      3.670      0.000       0.038       0.126\n",
            "age_o                 0.0347      0.032      1.093      0.274      -0.027       0.097\n",
            "mn_sat_o              0.0567      0.029      1.928      0.054      -0.001       0.114\n",
            "tuition_o            -0.0775      0.029     -2.630      0.009      -0.135      -0.020\n",
            "income                0.0079      0.032      0.249      0.804      -0.054       0.070\n",
            "exphappy_o            0.1052      0.024      4.394      0.000       0.058       0.152\n",
            "met_o                -0.1866      0.023     -8.293      0.000      -0.231      -0.143\n",
            "world_rank_o         -0.0596      0.030     -1.974      0.048      -0.119      -0.000\n",
            "masters_o            -0.0772      0.029     -2.694      0.007      -0.133      -0.021\n",
            "sinc1_1              -0.2503      0.036     -6.965      0.000      -0.321      -0.180\n",
            "intel1_1             -0.0410      0.037     -1.110      0.267      -0.113       0.031\n",
            "fun1_1               -0.0029      0.033     -0.088      0.930      -0.068       0.062\n",
            "amb1_1                0.0306      0.033      0.924      0.355      -0.034       0.096\n",
            "shar1_1              -0.2284      0.034     -6.728      0.000      -0.295      -0.162\n",
            "age_diff              0.0907      0.032      2.825      0.005       0.028       0.154\n",
            "income_diff          -0.0150      0.032     -0.463      0.643      -0.078       0.048\n",
            "date_diff             0.0272      0.025      1.105      0.269      -0.021       0.075\n",
            "go_out_diff           0.1419      0.025      5.733      0.000       0.093       0.190\n",
            "sports_diff          -0.1128      0.032     -3.533      0.000      -0.175      -0.050\n",
            "tvsport_diff          0.0874      0.029      3.011      0.003       0.030       0.144\n",
            "exercise_diff        -0.1501      0.026     -5.811      0.000      -0.201      -0.099\n",
            "dining_diff          -0.0760      0.027     -2.794      0.005      -0.129      -0.023\n",
            "museums_diff         -0.0113      0.047     -0.241      0.809      -0.103       0.081\n",
            "art_diff              0.0878      0.045      1.955      0.051      -0.000       0.176\n",
            "hiking_diff          -0.0082      0.026     -0.310      0.756      -0.060       0.043\n",
            "gaming_diff           0.1407      0.027      5.228      0.000       0.088       0.193\n",
            "clubbing_diff        -0.0954      0.024     -3.983      0.000      -0.142      -0.048\n",
            "reading_diff          0.0756      0.025      3.056      0.002       0.027       0.124\n",
            "tv_diff               0.1235      0.030      4.159      0.000       0.065       0.182\n",
            "theater_diff          0.1543      0.033      4.643      0.000       0.089       0.219\n",
            "movies_diff          -0.0545      0.029     -1.879      0.060      -0.111       0.002\n",
            "concerts_diff        -0.0256      0.033     -0.765      0.444      -0.091       0.040\n",
            "music_diff           -0.0327      0.030     -1.074      0.283      -0.092       0.027\n",
            "shopping_diff         0.0542      0.032      1.701      0.089      -0.008       0.117\n",
            "yoga_diff            -0.0517      0.026     -2.012      0.044      -0.102      -0.001\n",
            "worldrank_diff       -0.0717      0.030     -2.427      0.015      -0.130      -0.014\n",
            "(3_1-pf_o)_att        0.0432      0.041      1.056      0.291      -0.037       0.123\n",
            "(3_1-pf_o)_sinc      -0.0076      0.030     -0.252      0.801      -0.067       0.052\n",
            "(3_1-pf_o)_fun        0.0103      0.028      0.374      0.709      -0.044       0.064\n",
            "(3_1-pf_o)_intel      0.0496      0.030      1.644      0.100      -0.010       0.109\n",
            "(3_1-pf_o)_amb       -0.0609      0.031     -1.983      0.047      -0.121      -0.001\n",
            "(1_1-2_1_o)_sinc      0.0445      0.037      1.193      0.233      -0.029       0.118\n",
            "(1_1-2_1_o)_fun       0.0629      0.034      1.870      0.061      -0.003       0.129\n",
            "(1_1-2_1_o)_intel    -0.0240      0.036     -0.661      0.509      -0.095       0.047\n",
            "(1_1-2_1_o)_amb       0.0174      0.031      0.559      0.576      -0.044       0.079\n",
            "(1_1-2_1_o)_shar      0.0006      0.035      0.018      0.985      -0.069       0.070\n",
            "from_m               -0.0368      0.022     -1.662      0.097      -0.080       0.007\n",
            "goal_m                0.0073      0.022      0.327      0.743      -0.036       0.051\n",
            "imprace_m            -0.0313      0.023     -1.382      0.167      -0.076       0.013\n",
            "imprelig_m            0.0436      0.023      1.918      0.055      -0.001       0.088\n",
            "career_c_m            0.0289      0.023      1.267      0.205      -0.016       0.074\n",
            "masters_m            -0.0683      0.028     -2.436      0.015      -0.123      -0.013\n",
            "==============================================================================\n",
            "Omnibus:                       98.885   Durbin-Watson:                   2.032\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              102.894\n",
            "Skew:                          -0.299   Prob(JB):                     4.54e-23\n",
            "Kurtosis:                       3.032   Cond. No.                         6.28\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "museums_diff         4.538596\n",
              "art_diff             4.153276\n",
              "(3_1-pf_o)_att       3.447175\n",
              "gender               2.973512\n",
              "(1_1-2_1_o)_sinc     2.869920\n",
              "intel1_1             2.801325\n",
              "(1_1-2_1_o)_intel    2.703701\n",
              "sinc1_1              2.656031\n",
              "(1_1-2_1_o)_shar     2.562437\n",
              "shar1_1              2.372136\n",
              "(1_1-2_1_o)_fun      2.323497\n",
              "concerts_diff        2.306856\n",
              "theater_diff         2.272559\n",
              "amb1_1               2.259785\n",
              "fun1_1               2.247478\n",
              "income_diff          2.151005\n",
              "age_diff             2.121017\n",
              "sports_diff          2.095541\n",
              "shopping_diff        2.087609\n",
              "income               2.070576\n",
              "age_o                2.067261\n",
              "(1_1-2_1_o)_amb      2.001191\n",
              "(3_1-pf_o)_amb       1.938361\n",
              "music_diff           1.904323\n",
              "(3_1-pf_o)_sinc      1.890307\n",
              "world_rank_o         1.877146\n",
              "(3_1-pf_o)_intel     1.871075\n",
              "tv_diff              1.813454\n",
              "worldrank_diff       1.794581\n",
              "tuition_o            1.787076\n",
              "mn_sat_o             1.780038\n",
              "tvsport_diff         1.732384\n",
              "movies_diff          1.729719\n",
              "masters_o            1.690247\n",
              "masters_m            1.619026\n",
              "(3_1-pf_o)_fun       1.565869\n",
              "dining_diff          1.523266\n",
              "gaming_diff          1.490024\n",
              "hiking_diff          1.418120\n",
              "exercise_diff        1.372541\n",
              "yoga_diff            1.356919\n",
              "go_out_diff          1.260603\n",
              "reading_diff         1.259264\n",
              "date_diff            1.246576\n",
              "condtn               1.190718\n",
              "clubbing_diff        1.180602\n",
              "exphappy_o           1.178873\n",
              "order                1.127196\n",
              "career_c_m           1.073856\n",
              "int_corr             1.066291\n",
              "imprelig_m           1.064869\n",
              "imprace_m            1.055979\n",
              "met_o                1.042036\n",
              "samerace             1.035900\n",
              "goal_m               1.016256\n",
              "from_m               1.009220\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWfbxWM-lFcY"
      },
      "source": [
        "approved_removed_cols.append('attr1_1')  # other VIF values decreased a lot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C50UmAa9lFjj",
        "outputId": "0827549f-c7c1-42a4-ef4a-3e03ea5d438c"
      },
      "source": [
        "#drop '(1_1-2_1_o)_shar' since it has the highest p-value\n",
        "X_train = X_train.drop(columns=['(1_1-2_1_o)_shar'])\n",
        "\n",
        "att_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(att_model.summary())\n",
        "\n",
        "att_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, att_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 attr_o   R-squared:                       0.128\n",
            "Model:                            OLS   Adj. R-squared:                  0.121\n",
            "Method:                 Least Squares   F-statistic:                     18.22\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          4.57e-161\n",
            "Time:                        23:09:17   Log-Likelihood:                -13937.\n",
            "No. Observations:                6900   AIC:                         2.799e+04\n",
            "Df Residuals:                    6844   BIC:                         2.837e+04\n",
            "Df Model:                          55                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.1532      0.022    279.136      0.000       6.110       6.196\n",
            "gender               -0.3681      0.038     -9.706      0.000      -0.442      -0.294\n",
            "condtn                0.0424      0.024      1.762      0.078      -0.005       0.090\n",
            "order                -0.0646      0.023     -2.759      0.006      -0.110      -0.019\n",
            "int_corr              0.0443      0.023      1.948      0.051      -0.000       0.089\n",
            "samerace              0.0824      0.022      3.670      0.000       0.038       0.126\n",
            "age_o                 0.0347      0.032      1.094      0.274      -0.027       0.097\n",
            "mn_sat_o              0.0567      0.029      1.928      0.054      -0.001       0.114\n",
            "tuition_o            -0.0775      0.029     -2.632      0.009      -0.135      -0.020\n",
            "income                0.0079      0.032      0.248      0.804      -0.054       0.070\n",
            "exphappy_o            0.1052      0.024      4.395      0.000       0.058       0.152\n",
            "met_o                -0.1866      0.023     -8.294      0.000      -0.231      -0.143\n",
            "world_rank_o         -0.0596      0.030     -1.975      0.048      -0.119      -0.000\n",
            "masters_o            -0.0772      0.029     -2.697      0.007      -0.133      -0.021\n",
            "sinc1_1              -0.2503      0.036     -7.005      0.000      -0.320      -0.180\n",
            "intel1_1             -0.0410      0.037     -1.111      0.267      -0.113       0.031\n",
            "fun1_1               -0.0029      0.033     -0.087      0.931      -0.068       0.062\n",
            "amb1_1                0.0306      0.033      0.924      0.355      -0.034       0.096\n",
            "shar1_1              -0.2280      0.024     -9.439      0.000      -0.275      -0.181\n",
            "age_diff              0.0907      0.032      2.827      0.005       0.028       0.154\n",
            "income_diff          -0.0150      0.032     -0.464      0.643      -0.078       0.048\n",
            "date_diff             0.0272      0.025      1.106      0.269      -0.021       0.075\n",
            "go_out_diff           0.1419      0.025      5.742      0.000       0.093       0.190\n",
            "sports_diff          -0.1128      0.032     -3.554      0.000      -0.175      -0.051\n",
            "tvsport_diff          0.0874      0.029      3.016      0.003       0.031       0.144\n",
            "exercise_diff        -0.1501      0.026     -5.818      0.000      -0.201      -0.100\n",
            "dining_diff          -0.0760      0.027     -2.798      0.005      -0.129      -0.023\n",
            "museums_diff         -0.0113      0.047     -0.241      0.809      -0.103       0.081\n",
            "art_diff              0.0878      0.045      1.955      0.051      -0.000       0.176\n",
            "hiking_diff          -0.0081      0.026     -0.310      0.756      -0.060       0.043\n",
            "gaming_diff           0.1407      0.027      5.229      0.000       0.088       0.193\n",
            "clubbing_diff        -0.0954      0.024     -3.992      0.000      -0.142      -0.049\n",
            "reading_diff          0.0756      0.025      3.057      0.002       0.027       0.124\n",
            "tv_diff               0.1235      0.030      4.160      0.000       0.065       0.182\n",
            "theater_diff          0.1543      0.033      4.647      0.000       0.089       0.219\n",
            "movies_diff          -0.0545      0.029     -1.880      0.060      -0.111       0.002\n",
            "concerts_diff        -0.0256      0.033     -0.766      0.444      -0.091       0.040\n",
            "music_diff           -0.0327      0.030     -1.077      0.282      -0.092       0.027\n",
            "shopping_diff         0.0542      0.032      1.705      0.088      -0.008       0.117\n",
            "yoga_diff            -0.0517      0.026     -2.014      0.044      -0.102      -0.001\n",
            "worldrank_diff       -0.0717      0.030     -2.431      0.015      -0.130      -0.014\n",
            "(3_1-pf_o)_att        0.0430      0.039      1.111      0.267      -0.033       0.119\n",
            "(3_1-pf_o)_sinc      -0.0077      0.030     -0.258      0.796      -0.066       0.051\n",
            "(3_1-pf_o)_fun        0.0102      0.027      0.377      0.706      -0.043       0.063\n",
            "(3_1-pf_o)_intel      0.0494      0.029      1.696      0.090      -0.008       0.107\n",
            "(3_1-pf_o)_amb       -0.0610      0.030     -2.004      0.045      -0.121      -0.001\n",
            "(1_1-2_1_o)_sinc      0.0446      0.037      1.200      0.230      -0.028       0.117\n",
            "(1_1-2_1_o)_fun       0.0628      0.033      1.877      0.061      -0.003       0.128\n",
            "(1_1-2_1_o)_intel    -0.0239      0.036     -0.661      0.509      -0.095       0.047\n",
            "(1_1-2_1_o)_amb       0.0174      0.031      0.559      0.576      -0.044       0.078\n",
            "from_m               -0.0368      0.022     -1.662      0.097      -0.080       0.007\n",
            "goal_m                0.0073      0.022      0.327      0.743      -0.036       0.051\n",
            "imprace_m            -0.0313      0.023     -1.382      0.167      -0.076       0.013\n",
            "imprelig_m            0.0436      0.023      1.918      0.055      -0.001       0.088\n",
            "career_c_m            0.0290      0.023      1.268      0.205      -0.016       0.074\n",
            "masters_m            -0.0684      0.028     -2.437      0.015      -0.123      -0.013\n",
            "==============================================================================\n",
            "Omnibus:                       98.907   Durbin-Watson:                   2.032\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              102.918\n",
            "Skew:                          -0.299   Prob(JB):                     4.48e-23\n",
            "Kurtosis:                       3.032   Cond. No.                         6.25\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "museums_diff         4.538558\n",
              "art_diff             4.153078\n",
              "(3_1-pf_o)_att       3.081753\n",
              "gender               2.959438\n",
              "(1_1-2_1_o)_sinc     2.842994\n",
              "intel1_1             2.799884\n",
              "(1_1-2_1_o)_intel    2.702940\n",
              "sinc1_1              2.628082\n",
              "(1_1-2_1_o)_fun      2.302810\n",
              "concerts_diff        2.299249\n",
              "theater_diff         2.267606\n",
              "amb1_1               2.259780\n",
              "fun1_1               2.243832\n",
              "income_diff          2.147704\n",
              "age_diff             2.118805\n",
              "shopping_diff        2.080462\n",
              "sports_diff          2.073331\n",
              "income               2.068325\n",
              "age_o                2.067115\n",
              "(1_1-2_1_o)_amb      1.996194\n",
              "(3_1-pf_o)_amb       1.903387\n",
              "music_diff           1.898172\n",
              "world_rank_o         1.874780\n",
              "(3_1-pf_o)_sinc      1.841958\n",
              "tv_diff              1.813328\n",
              "worldrank_diff       1.790670\n",
              "tuition_o            1.784377\n",
              "mn_sat_o             1.779908\n",
              "(3_1-pf_o)_intel     1.748012\n",
              "tvsport_diff         1.728273\n",
              "movies_diff          1.728025\n",
              "masters_o            1.685027\n",
              "masters_m            1.618857\n",
              "dining_diff          1.520038\n",
              "(3_1-pf_o)_fun       1.511725\n",
              "gaming_diff          1.489778\n",
              "hiking_diff          1.418042\n",
              "exercise_diff        1.369267\n",
              "yoga_diff            1.355206\n",
              "reading_diff         1.258257\n",
              "go_out_diff          1.257402\n",
              "date_diff            1.245462\n",
              "shar1_1              1.200607\n",
              "condtn               1.190717\n",
              "exphappy_o           1.177779\n",
              "clubbing_diff        1.175070\n",
              "order                1.127181\n",
              "career_c_m           1.073495\n",
              "imprelig_m           1.064747\n",
              "int_corr             1.063794\n",
              "imprace_m            1.055978\n",
              "met_o                1.041718\n",
              "samerace             1.035899\n",
              "goal_m               1.016256\n",
              "from_m               1.009212\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXueaLztlFt3"
      },
      "source": [
        "approved_removed_cols.append('(1_1-2_1_o)_shar')  # other p-values decreased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKFyE84Vl8y6",
        "outputId": "f5077f2f-7a54-4fa3-d2f5-48d707822359"
      },
      "source": [
        "#drop 'museums_diff' since it has the high p-value\n",
        "X_train = X_train.drop(columns=['fun1_1'])\n",
        "\n",
        "att_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(att_model.summary())\n",
        "\n",
        "att_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, att_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 attr_o   R-squared:                       0.128\n",
            "Model:                            OLS   Adj. R-squared:                  0.121\n",
            "Method:                 Least Squares   F-statistic:                     18.56\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          1.06e-161\n",
            "Time:                        23:09:21   Log-Likelihood:                -13937.\n",
            "No. Observations:                6900   AIC:                         2.798e+04\n",
            "Df Residuals:                    6845   BIC:                         2.836e+04\n",
            "Df Model:                          54                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.1532      0.022    279.156      0.000       6.110       6.196\n",
            "gender               -0.3677      0.038     -9.781      0.000      -0.441      -0.294\n",
            "condtn                0.0424      0.024      1.762      0.078      -0.005       0.090\n",
            "order                -0.0646      0.023     -2.759      0.006      -0.110      -0.019\n",
            "int_corr              0.0442      0.023      1.947      0.052      -0.000       0.089\n",
            "samerace              0.0823      0.022      3.670      0.000       0.038       0.126\n",
            "age_o                 0.0348      0.032      1.100      0.271      -0.027       0.097\n",
            "mn_sat_o              0.0567      0.029      1.929      0.054      -0.001       0.114\n",
            "tuition_o            -0.0774      0.029     -2.631      0.009      -0.135      -0.020\n",
            "income                0.0077      0.032      0.245      0.807      -0.054       0.070\n",
            "exphappy_o            0.1052      0.024      4.396      0.000       0.058       0.152\n",
            "met_o                -0.1866      0.022     -8.294      0.000      -0.231      -0.143\n",
            "world_rank_o         -0.0595      0.030     -1.973      0.049      -0.119      -0.000\n",
            "masters_o            -0.0771      0.029     -2.697      0.007      -0.133      -0.021\n",
            "sinc1_1              -0.2499      0.035     -7.071      0.000      -0.319      -0.181\n",
            "intel1_1             -0.0404      0.036     -1.113      0.266      -0.111       0.031\n",
            "amb1_1                0.0311      0.033      0.952      0.341      -0.033       0.095\n",
            "shar1_1              -0.2276      0.024     -9.626      0.000      -0.274      -0.181\n",
            "age_diff              0.0905      0.032      2.828      0.005       0.028       0.153\n",
            "income_diff          -0.0150      0.032     -0.465      0.642      -0.078       0.048\n",
            "date_diff             0.0273      0.025      1.112      0.266      -0.021       0.075\n",
            "go_out_diff           0.1418      0.025      5.748      0.000       0.093       0.190\n",
            "sports_diff          -0.1129      0.032     -3.558      0.000      -0.175      -0.051\n",
            "tvsport_diff          0.0876      0.029      3.028      0.002       0.031       0.144\n",
            "exercise_diff        -0.1502      0.026     -5.827      0.000      -0.201      -0.100\n",
            "dining_diff          -0.0762      0.027     -2.808      0.005      -0.129      -0.023\n",
            "museums_diff         -0.0115      0.047     -0.244      0.807      -0.103       0.081\n",
            "art_diff              0.0880      0.045      1.962      0.050    6.41e-05       0.176\n",
            "hiking_diff          -0.0081      0.026     -0.307      0.759      -0.059       0.043\n",
            "gaming_diff           0.1406      0.027      5.228      0.000       0.088       0.193\n",
            "clubbing_diff        -0.0954      0.024     -3.991      0.000      -0.142      -0.049\n",
            "reading_diff          0.0756      0.025      3.058      0.002       0.027       0.124\n",
            "tv_diff               0.1236      0.030      4.164      0.000       0.065       0.182\n",
            "theater_diff          0.1543      0.033      4.648      0.000       0.089       0.219\n",
            "movies_diff          -0.0545      0.029     -1.881      0.060      -0.111       0.002\n",
            "concerts_diff        -0.0257      0.033     -0.768      0.443      -0.091       0.040\n",
            "music_diff           -0.0326      0.030     -1.075      0.282      -0.092       0.027\n",
            "shopping_diff         0.0541      0.032      1.703      0.089      -0.008       0.116\n",
            "yoga_diff            -0.0516      0.026     -2.012      0.044      -0.102      -0.001\n",
            "worldrank_diff       -0.0718      0.029     -2.437      0.015      -0.130      -0.014\n",
            "(3_1-pf_o)_att        0.0429      0.039      1.109      0.268      -0.033       0.119\n",
            "(3_1-pf_o)_sinc      -0.0076      0.030     -0.254      0.799      -0.066       0.051\n",
            "(3_1-pf_o)_fun        0.0106      0.027      0.396      0.692      -0.042       0.063\n",
            "(3_1-pf_o)_intel      0.0494      0.029      1.696      0.090      -0.008       0.107\n",
            "(3_1-pf_o)_amb       -0.0606      0.030     -2.008      0.045      -0.120      -0.001\n",
            "(1_1-2_1_o)_sinc      0.0444      0.037      1.197      0.231      -0.028       0.117\n",
            "(1_1-2_1_o)_fun       0.0608      0.025      2.465      0.014       0.012       0.109\n",
            "(1_1-2_1_o)_intel    -0.0242      0.036     -0.670      0.503      -0.095       0.047\n",
            "(1_1-2_1_o)_amb       0.0170      0.031      0.552      0.581      -0.043       0.077\n",
            "from_m               -0.0368      0.022     -1.660      0.097      -0.080       0.007\n",
            "goal_m                0.0072      0.022      0.326      0.744      -0.036       0.051\n",
            "imprace_m            -0.0313      0.023     -1.381      0.167      -0.076       0.013\n",
            "imprelig_m            0.0436      0.023      1.916      0.055      -0.001       0.088\n",
            "career_c_m            0.0289      0.023      1.265      0.206      -0.016       0.074\n",
            "masters_m            -0.0682      0.028     -2.436      0.015      -0.123      -0.013\n",
            "==============================================================================\n",
            "Omnibus:                       98.838   Durbin-Watson:                   2.032\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              102.844\n",
            "Skew:                          -0.299   Prob(JB):                     4.65e-23\n",
            "Kurtosis:                       3.032   Cond. No.                         6.24\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "museums_diff         4.532906\n",
              "art_diff             4.143681\n",
              "(3_1-pf_o)_att       3.078259\n",
              "gender               2.907303\n",
              "(1_1-2_1_o)_sinc     2.835078\n",
              "intel1_1             2.707532\n",
              "(1_1-2_1_o)_intel    2.686753\n",
              "sinc1_1              2.569689\n",
              "concerts_diff        2.298104\n",
              "theater_diff         2.267606\n",
              "amb1_1               2.196664\n",
              "income_diff          2.147667\n",
              "age_diff             2.108847\n",
              "shopping_diff        2.077613\n",
              "sports_diff          2.072164\n",
              "income               2.064298\n",
              "age_o                2.061541\n",
              "(1_1-2_1_o)_amb      1.938591\n",
              "music_diff           1.897096\n",
              "(3_1-pf_o)_amb       1.876792\n",
              "world_rank_o         1.872655\n",
              "(3_1-pf_o)_sinc      1.837699\n",
              "tv_diff              1.812107\n",
              "worldrank_diff       1.787809\n",
              "tuition_o            1.783326\n",
              "mn_sat_o             1.779899\n",
              "(3_1-pf_o)_intel     1.747884\n",
              "movies_diff          1.727995\n",
              "tvsport_diff         1.721554\n",
              "masters_o            1.680493\n",
              "masters_m            1.615264\n",
              "dining_diff          1.514940\n",
              "gaming_diff          1.488833\n",
              "(3_1-pf_o)_fun       1.474034\n",
              "hiking_diff          1.415525\n",
              "exercise_diff        1.366902\n",
              "yoga_diff            1.354362\n",
              "reading_diff         1.258122\n",
              "(1_1-2_1_o)_fun      1.252992\n",
              "go_out_diff          1.252347\n",
              "date_diff            1.241792\n",
              "condtn               1.190677\n",
              "exphappy_o           1.177772\n",
              "clubbing_diff        1.174744\n",
              "shar1_1              1.150244\n",
              "order                1.127169\n",
              "career_c_m           1.071712\n",
              "imprelig_m           1.064264\n",
              "int_corr             1.062574\n",
              "imprace_m            1.055866\n",
              "met_o                1.041619\n",
              "samerace             1.033735\n",
              "goal_m               1.016088\n",
              "from_m               1.008622\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P7FRNuXmAa0"
      },
      "source": [
        "approved_removed_cols.append('fun1_1')  # other p-values decreased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysFlnvvjmJpL",
        "outputId": "4f219d4a-72e8-43fd-a7b3-3b66e9cf6a04"
      },
      "source": [
        "#drop 'income' since it has the high p-value\n",
        "X_train = X_train.drop(columns=['income'])\n",
        "\n",
        "att_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(att_model.summary())\n",
        "\n",
        "att_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, att_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 attr_o   R-squared:                       0.128\n",
            "Model:                            OLS   Adj. R-squared:                  0.121\n",
            "Method:                 Least Squares   F-statistic:                     18.91\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          2.49e-162\n",
            "Time:                        23:09:25   Log-Likelihood:                -13937.\n",
            "No. Observations:                6900   AIC:                         2.798e+04\n",
            "Df Residuals:                    6846   BIC:                         2.835e+04\n",
            "Df Model:                          53                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.1532      0.022    279.175      0.000       6.110       6.196\n",
            "gender               -0.3678      0.038     -9.787      0.000      -0.441      -0.294\n",
            "condtn                0.0422      0.024      1.756      0.079      -0.005       0.089\n",
            "order                -0.0645      0.023     -2.757      0.006      -0.110      -0.019\n",
            "int_corr              0.0444      0.023      1.954      0.051      -0.000       0.089\n",
            "samerace              0.0825      0.022      3.688      0.000       0.039       0.126\n",
            "age_o                 0.0344      0.032      1.088      0.277      -0.028       0.096\n",
            "mn_sat_o              0.0566      0.029      1.925      0.054      -0.001       0.114\n",
            "tuition_o            -0.0772      0.029     -2.623      0.009      -0.135      -0.019\n",
            "exphappy_o            0.1051      0.024      4.394      0.000       0.058       0.152\n",
            "met_o                -0.1868      0.022     -8.314      0.000      -0.231      -0.143\n",
            "world_rank_o         -0.0596      0.030     -1.975      0.048      -0.119      -0.000\n",
            "masters_o            -0.0770      0.029     -2.694      0.007      -0.133      -0.021\n",
            "sinc1_1              -0.2500      0.035     -7.078      0.000      -0.319      -0.181\n",
            "intel1_1             -0.0403      0.036     -1.111      0.267      -0.111       0.031\n",
            "amb1_1                0.0310      0.033      0.950      0.342      -0.033       0.095\n",
            "shar1_1              -0.2274      0.024     -9.624      0.000      -0.274      -0.181\n",
            "age_diff              0.0908      0.032      2.838      0.005       0.028       0.154\n",
            "income_diff          -0.0206      0.023     -0.894      0.371      -0.066       0.025\n",
            "date_diff             0.0273      0.025      1.111      0.267      -0.021       0.075\n",
            "go_out_diff           0.1418      0.025      5.748      0.000       0.093       0.190\n",
            "sports_diff          -0.1127      0.032     -3.553      0.000      -0.175      -0.051\n",
            "tvsport_diff          0.0874      0.029      3.023      0.003       0.031       0.144\n",
            "exercise_diff        -0.1501      0.026     -5.826      0.000      -0.201      -0.100\n",
            "dining_diff          -0.0762      0.027     -2.808      0.005      -0.129      -0.023\n",
            "museums_diff         -0.0116      0.047     -0.248      0.804      -0.104       0.080\n",
            "art_diff              0.0883      0.045      1.968      0.049       0.000       0.176\n",
            "hiking_diff          -0.0081      0.026     -0.310      0.757      -0.060       0.043\n",
            "gaming_diff           0.1406      0.027      5.227      0.000       0.088       0.193\n",
            "clubbing_diff        -0.0954      0.024     -3.991      0.000      -0.142      -0.049\n",
            "reading_diff          0.0757      0.025      3.060      0.002       0.027       0.124\n",
            "tv_diff               0.1237      0.030      4.170      0.000       0.066       0.182\n",
            "theater_diff          0.1542      0.033      4.646      0.000       0.089       0.219\n",
            "movies_diff          -0.0545      0.029     -1.883      0.060      -0.111       0.002\n",
            "concerts_diff        -0.0257      0.033     -0.770      0.441      -0.091       0.040\n",
            "music_diff           -0.0324      0.030     -1.069      0.285      -0.092       0.027\n",
            "shopping_diff         0.0543      0.032      1.711      0.087      -0.008       0.117\n",
            "yoga_diff            -0.0517      0.026     -2.014      0.044      -0.102      -0.001\n",
            "worldrank_diff       -0.0717      0.029     -2.432      0.015      -0.129      -0.014\n",
            "(3_1-pf_o)_att        0.0435      0.039      1.129      0.259      -0.032       0.119\n",
            "(3_1-pf_o)_sinc      -0.0074      0.030     -0.247      0.805      -0.066       0.051\n",
            "(3_1-pf_o)_fun        0.0105      0.027      0.393      0.694      -0.042       0.063\n",
            "(3_1-pf_o)_intel      0.0498      0.029      1.709      0.087      -0.007       0.107\n",
            "(3_1-pf_o)_amb       -0.0602      0.030     -1.997      0.046      -0.119      -0.001\n",
            "(1_1-2_1_o)_sinc      0.0447      0.037      1.204      0.229      -0.028       0.117\n",
            "(1_1-2_1_o)_fun       0.0609      0.025      2.470      0.014       0.013       0.109\n",
            "(1_1-2_1_o)_intel    -0.0244      0.036     -0.675      0.500      -0.095       0.046\n",
            "(1_1-2_1_o)_amb       0.0168      0.031      0.548      0.584      -0.043       0.077\n",
            "from_m               -0.0367      0.022     -1.659      0.097      -0.080       0.007\n",
            "goal_m                0.0072      0.022      0.325      0.745      -0.036       0.051\n",
            "imprace_m            -0.0314      0.023     -1.385      0.166      -0.076       0.013\n",
            "imprelig_m            0.0436      0.023      1.917      0.055      -0.001       0.088\n",
            "career_c_m            0.0288      0.023      1.263      0.207      -0.016       0.074\n",
            "masters_m            -0.0683      0.028     -2.436      0.015      -0.123      -0.013\n",
            "==============================================================================\n",
            "Omnibus:                       99.114   Durbin-Watson:                   2.032\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              103.141\n",
            "Skew:                          -0.299   Prob(JB):                     4.01e-23\n",
            "Kurtosis:                       3.032   Cond. No.                         6.23\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "museums_diff         4.532026\n",
              "art_diff             4.141707\n",
              "(3_1-pf_o)_att       3.063232\n",
              "gender               2.906723\n",
              "(1_1-2_1_o)_sinc     2.833106\n",
              "intel1_1             2.707264\n",
              "(1_1-2_1_o)_intel    2.685474\n",
              "sinc1_1              2.568632\n",
              "concerts_diff        2.297932\n",
              "theater_diff         2.267440\n",
              "amb1_1               2.196469\n",
              "age_diff             2.106473\n",
              "shopping_diff        2.076122\n",
              "sports_diff          2.071191\n",
              "age_o                2.054998\n",
              "(1_1-2_1_o)_amb      1.937974\n",
              "music_diff           1.895512\n",
              "world_rank_o         1.872589\n",
              "(3_1-pf_o)_amb       1.869063\n",
              "(3_1-pf_o)_sinc      1.835867\n",
              "tv_diff              1.811233\n",
              "worldrank_diff       1.786953\n",
              "tuition_o            1.780399\n",
              "mn_sat_o             1.779303\n",
              "(3_1-pf_o)_intel     1.743903\n",
              "movies_diff          1.727935\n",
              "tvsport_diff         1.720440\n",
              "masters_o            1.680329\n",
              "masters_m            1.615252\n",
              "dining_diff          1.514938\n",
              "gaming_diff          1.488774\n",
              "(3_1-pf_o)_fun       1.473840\n",
              "hiking_diff          1.415321\n",
              "exercise_diff        1.366842\n",
              "yoga_diff            1.354304\n",
              "reading_diff         1.258073\n",
              "(1_1-2_1_o)_fun      1.252620\n",
              "go_out_diff          1.252341\n",
              "date_diff            1.241754\n",
              "condtn               1.189512\n",
              "exphappy_o           1.177621\n",
              "clubbing_diff        1.174744\n",
              "shar1_1              1.149294\n",
              "order                1.127041\n",
              "income_diff          1.087707\n",
              "career_c_m           1.071631\n",
              "imprelig_m           1.064260\n",
              "int_corr             1.061840\n",
              "imprace_m            1.055683\n",
              "met_o                1.039572\n",
              "samerace             1.031103\n",
              "goal_m               1.016078\n",
              "from_m               1.008574\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLKxiXfimM-d"
      },
      "source": [
        "approved_removed_cols.append('income')  # other p-values decreased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SVWDmpxlF0F",
        "outputId": "73f8f000-cfc2-44a8-9b2c-93c09decea60"
      },
      "source": [
        "#drop 'museums_diff' since it has the high p-value\n",
        "X_train = X_train.drop(columns=['museums_diff'])\n",
        "\n",
        "att_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(att_model.summary())\n",
        "\n",
        "att_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, att_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 attr_o   R-squared:                       0.128\n",
            "Model:                            OLS   Adj. R-squared:                  0.121\n",
            "Method:                 Least Squares   F-statistic:                     19.28\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          5.80e-163\n",
            "Time:                        23:09:29   Log-Likelihood:                -13937.\n",
            "No. Observations:                6900   AIC:                         2.798e+04\n",
            "Df Residuals:                    6847   BIC:                         2.834e+04\n",
            "Df Model:                          52                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.1532      0.022    279.194      0.000       6.110       6.196\n",
            "gender               -0.3671      0.037     -9.796      0.000      -0.441      -0.294\n",
            "condtn                0.0422      0.024      1.757      0.079      -0.005       0.089\n",
            "order                -0.0645      0.023     -2.757      0.006      -0.110      -0.019\n",
            "int_corr              0.0444      0.023      1.953      0.051      -0.000       0.089\n",
            "samerace              0.0825      0.022      3.688      0.000       0.039       0.126\n",
            "age_o                 0.0343      0.032      1.084      0.278      -0.028       0.096\n",
            "mn_sat_o              0.0569      0.029      1.935      0.053      -0.001       0.114\n",
            "tuition_o            -0.0773      0.029     -2.631      0.009      -0.135      -0.020\n",
            "exphappy_o            0.1053      0.024      4.403      0.000       0.058       0.152\n",
            "met_o                -0.1869      0.022     -8.320      0.000      -0.231      -0.143\n",
            "world_rank_o         -0.0595      0.030     -1.974      0.048      -0.119      -0.000\n",
            "masters_o            -0.0770      0.029     -2.696      0.007      -0.133      -0.021\n",
            "sinc1_1              -0.2497      0.035     -7.074      0.000      -0.319      -0.181\n",
            "intel1_1             -0.0394      0.036     -1.092      0.275      -0.110       0.031\n",
            "amb1_1                0.0309      0.033      0.945      0.345      -0.033       0.095\n",
            "shar1_1              -0.2274      0.024     -9.624      0.000      -0.274      -0.181\n",
            "age_diff              0.0908      0.032      2.837      0.005       0.028       0.153\n",
            "income_diff          -0.0201      0.023     -0.877      0.380      -0.065       0.025\n",
            "date_diff             0.0272      0.025      1.109      0.268      -0.021       0.075\n",
            "go_out_diff           0.1416      0.025      5.744      0.000       0.093       0.190\n",
            "sports_diff          -0.1128      0.032     -3.557      0.000      -0.175      -0.051\n",
            "tvsport_diff          0.0868      0.029      3.013      0.003       0.030       0.143\n",
            "exercise_diff        -0.1500      0.026     -5.822      0.000      -0.201      -0.100\n",
            "dining_diff          -0.0774      0.027     -2.895      0.004      -0.130      -0.025\n",
            "art_diff              0.0798      0.029      2.750      0.006       0.023       0.137\n",
            "hiking_diff          -0.0085      0.026     -0.324      0.746      -0.060       0.043\n",
            "gaming_diff           0.1411      0.027      5.263      0.000       0.089       0.194\n",
            "clubbing_diff        -0.0955      0.024     -3.998      0.000      -0.142      -0.049\n",
            "reading_diff          0.0744      0.024      3.075      0.002       0.027       0.122\n",
            "tv_diff               0.1242      0.030      4.194      0.000       0.066       0.182\n",
            "theater_diff          0.1528      0.033      4.673      0.000       0.089       0.217\n",
            "movies_diff          -0.0543      0.029     -1.876      0.061      -0.111       0.002\n",
            "concerts_diff        -0.0258      0.033     -0.773      0.439      -0.091       0.040\n",
            "music_diff           -0.0325      0.030     -1.071      0.284      -0.092       0.027\n",
            "shopping_diff         0.0535      0.032      1.694      0.090      -0.008       0.115\n",
            "yoga_diff            -0.0514      0.026     -2.005      0.045      -0.102      -0.001\n",
            "worldrank_diff       -0.0720      0.029     -2.447      0.014      -0.130      -0.014\n",
            "(3_1-pf_o)_att        0.0432      0.039      1.120      0.263      -0.032       0.119\n",
            "(3_1-pf_o)_sinc      -0.0073      0.030     -0.245      0.806      -0.066       0.051\n",
            "(3_1-pf_o)_fun        0.0104      0.027      0.390      0.697      -0.042       0.063\n",
            "(3_1-pf_o)_intel      0.0502      0.029      1.729      0.084      -0.007       0.107\n",
            "(3_1-pf_o)_amb       -0.0604      0.030     -2.007      0.045      -0.119      -0.001\n",
            "(1_1-2_1_o)_sinc      0.0445      0.037      1.201      0.230      -0.028       0.117\n",
            "(1_1-2_1_o)_fun       0.0610      0.025      2.472      0.013       0.013       0.109\n",
            "(1_1-2_1_o)_intel    -0.0250      0.036     -0.694      0.487      -0.096       0.046\n",
            "(1_1-2_1_o)_amb       0.0173      0.031      0.564      0.573      -0.043       0.077\n",
            "from_m               -0.0367      0.022     -1.660      0.097      -0.080       0.007\n",
            "goal_m                0.0073      0.022      0.329      0.742      -0.036       0.051\n",
            "imprace_m            -0.0312      0.023     -1.380      0.168      -0.076       0.013\n",
            "imprelig_m            0.0436      0.023      1.918      0.055      -0.001       0.088\n",
            "career_c_m            0.0288      0.023      1.261      0.207      -0.016       0.074\n",
            "masters_m            -0.0683      0.028     -2.439      0.015      -0.123      -0.013\n",
            "==============================================================================\n",
            "Omnibus:                       99.009   Durbin-Watson:                   2.032\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              103.028\n",
            "Skew:                          -0.299   Prob(JB):                     4.24e-23\n",
            "Kurtosis:                       3.032   Cond. No.                         5.45\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3_1-pf_o)_att       3.058757\n",
              "gender               2.890145\n",
              "(1_1-2_1_o)_sinc     2.832389\n",
              "intel1_1             2.680680\n",
              "(1_1-2_1_o)_intel    2.672138\n",
              "sinc1_1              2.564879\n",
              "concerts_diff        2.297583\n",
              "theater_diff         2.200760\n",
              "amb1_1               2.195358\n",
              "age_diff             2.106443\n",
              "sports_diff          2.070885\n",
              "shopping_diff        2.054540\n",
              "age_o                2.054500\n",
              "(1_1-2_1_o)_amb      1.930681\n",
              "music_diff           1.895314\n",
              "world_rank_o         1.872580\n",
              "(3_1-pf_o)_amb       1.866416\n",
              "(3_1-pf_o)_sinc      1.835765\n",
              "tv_diff              1.804278\n",
              "worldrank_diff       1.782965\n",
              "tuition_o            1.779179\n",
              "mn_sat_o             1.776835\n",
              "(3_1-pf_o)_intel     1.736801\n",
              "art_diff             1.733083\n",
              "movies_diff          1.726070\n",
              "tvsport_diff         1.709410\n",
              "masters_o            1.680254\n",
              "masters_m            1.615055\n",
              "gaming_diff          1.479677\n",
              "(3_1-pf_o)_fun       1.473590\n",
              "dining_diff          1.470016\n",
              "hiking_diff          1.411198\n",
              "exercise_diff        1.366302\n",
              "yoga_diff            1.351130\n",
              "(1_1-2_1_o)_fun      1.252535\n",
              "go_out_diff          1.251704\n",
              "date_diff            1.241666\n",
              "reading_diff         1.205278\n",
              "condtn               1.189479\n",
              "exphappy_o           1.176550\n",
              "clubbing_diff        1.174205\n",
              "shar1_1              1.149287\n",
              "order                1.127039\n",
              "income_diff          1.080909\n",
              "career_c_m           1.071547\n",
              "imprelig_m           1.064256\n",
              "int_corr             1.061831\n",
              "imprace_m            1.055282\n",
              "met_o                1.039269\n",
              "samerace             1.031100\n",
              "goal_m               1.015890\n",
              "from_m               1.008552\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nGw7f3ElF-D"
      },
      "source": [
        "approved_removed_cols.append('museums_diff')  # other p-values decreased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCROeChflGEf",
        "outputId": "37565140-d1c9-4fc3-d8a1-301dc84223b8"
      },
      "source": [
        "#drop 'museums_diff' since it has the high p-value\n",
        "X_train = X_train.drop(columns=['hiking_diff'])\n",
        "\n",
        "att_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(att_model.summary())\n",
        "\n",
        "att_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, att_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 attr_o   R-squared:                       0.128\n",
            "Model:                            OLS   Adj. R-squared:                  0.121\n",
            "Method:                 Least Squares   F-statistic:                     19.65\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          1.37e-163\n",
            "Time:                        23:09:32   Log-Likelihood:                -13937.\n",
            "No. Observations:                6900   AIC:                         2.798e+04\n",
            "Df Residuals:                    6848   BIC:                         2.833e+04\n",
            "Df Model:                          51                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.1532      0.022    279.212      0.000       6.110       6.196\n",
            "gender               -0.3690      0.037     -9.981      0.000      -0.442      -0.297\n",
            "condtn                0.0422      0.024      1.754      0.079      -0.005       0.089\n",
            "order                -0.0645      0.023     -2.755      0.006      -0.110      -0.019\n",
            "int_corr              0.0444      0.023      1.955      0.051      -0.000       0.089\n",
            "samerace              0.0825      0.022      3.687      0.000       0.039       0.126\n",
            "age_o                 0.0342      0.032      1.083      0.279      -0.028       0.096\n",
            "mn_sat_o              0.0566      0.029      1.927      0.054      -0.001       0.114\n",
            "tuition_o            -0.0773      0.029     -2.631      0.009      -0.135      -0.020\n",
            "exphappy_o            0.1049      0.024      4.394      0.000       0.058       0.152\n",
            "met_o                -0.1868      0.022     -8.316      0.000      -0.231      -0.143\n",
            "world_rank_o         -0.0595      0.030     -1.971      0.049      -0.119      -0.000\n",
            "masters_o            -0.0766      0.029     -2.683      0.007      -0.132      -0.021\n",
            "sinc1_1              -0.2495      0.035     -7.069      0.000      -0.319      -0.180\n",
            "intel1_1             -0.0397      0.036     -1.100      0.271      -0.110       0.031\n",
            "amb1_1                0.0309      0.033      0.946      0.344      -0.033       0.095\n",
            "shar1_1              -0.2277      0.024     -9.643      0.000      -0.274      -0.181\n",
            "age_diff              0.0908      0.032      2.837      0.005       0.028       0.153\n",
            "income_diff          -0.0187      0.023     -0.831      0.406      -0.063       0.025\n",
            "date_diff             0.0271      0.025      1.103      0.270      -0.021       0.075\n",
            "go_out_diff           0.1412      0.025      5.736      0.000       0.093       0.189\n",
            "sports_diff          -0.1149      0.031     -3.696      0.000      -0.176      -0.054\n",
            "tvsport_diff          0.0877      0.029      3.057      0.002       0.031       0.144\n",
            "exercise_diff        -0.1500      0.026     -5.823      0.000      -0.201      -0.100\n",
            "dining_diff          -0.0781      0.027     -2.932      0.003      -0.130      -0.026\n",
            "art_diff              0.0787      0.029      2.731      0.006       0.022       0.135\n",
            "gaming_diff           0.1391      0.026      5.338      0.000       0.088       0.190\n",
            "clubbing_diff        -0.0951      0.024     -3.987      0.000      -0.142      -0.048\n",
            "reading_diff          0.0744      0.024      3.075      0.002       0.027       0.122\n",
            "tv_diff               0.1249      0.030      4.230      0.000       0.067       0.183\n",
            "theater_diff          0.1529      0.033      4.677      0.000       0.089       0.217\n",
            "movies_diff          -0.0542      0.029     -1.872      0.061      -0.111       0.003\n",
            "concerts_diff        -0.0274      0.033     -0.830      0.407      -0.092       0.037\n",
            "music_diff           -0.0313      0.030     -1.039      0.299      -0.090       0.028\n",
            "shopping_diff         0.0553      0.031      1.778      0.075      -0.006       0.116\n",
            "yoga_diff            -0.0529      0.025     -2.101      0.036      -0.102      -0.004\n",
            "worldrank_diff       -0.0725      0.029     -2.467      0.014      -0.130      -0.015\n",
            "(3_1-pf_o)_att        0.0435      0.039      1.130      0.259      -0.032       0.119\n",
            "(3_1-pf_o)_sinc      -0.0068      0.030     -0.227      0.820      -0.065       0.052\n",
            "(3_1-pf_o)_fun        0.0109      0.027      0.408      0.683      -0.041       0.063\n",
            "(3_1-pf_o)_intel      0.0500      0.029      1.722      0.085      -0.007       0.107\n",
            "(3_1-pf_o)_amb       -0.0604      0.030     -2.005      0.045      -0.119      -0.001\n",
            "(1_1-2_1_o)_sinc      0.0448      0.037      1.207      0.228      -0.028       0.117\n",
            "(1_1-2_1_o)_fun       0.0614      0.025      2.494      0.013       0.013       0.110\n",
            "(1_1-2_1_o)_intel    -0.0252      0.036     -0.698      0.485      -0.096       0.045\n",
            "(1_1-2_1_o)_amb       0.0174      0.031      0.567      0.570      -0.043       0.077\n",
            "from_m               -0.0367      0.022     -1.658      0.097      -0.080       0.007\n",
            "goal_m                0.0073      0.022      0.328      0.743      -0.036       0.051\n",
            "imprace_m            -0.0313      0.023     -1.385      0.166      -0.076       0.013\n",
            "imprelig_m            0.0437      0.023      1.920      0.055      -0.001       0.088\n",
            "career_c_m            0.0287      0.023      1.259      0.208      -0.016       0.073\n",
            "masters_m            -0.0680      0.028     -2.428      0.015      -0.123      -0.013\n",
            "==============================================================================\n",
            "Omnibus:                       99.007   Durbin-Watson:                   2.032\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              103.026\n",
            "Skew:                          -0.299   Prob(JB):                     4.25e-23\n",
            "Kurtosis:                       3.032   Cond. No.                         5.42\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3_1-pf_o)_att       3.056314\n",
              "(1_1-2_1_o)_sinc     2.831413\n",
              "gender               2.814626\n",
              "intel1_1             2.679126\n",
              "(1_1-2_1_o)_intel    2.671744\n",
              "sinc1_1              2.563722\n",
              "concerts_diff        2.247737\n",
              "theater_diff         2.200580\n",
              "amb1_1               2.195337\n",
              "age_diff             2.106443\n",
              "age_o                2.054463\n",
              "shopping_diff        1.992249\n",
              "sports_diff          1.988343\n",
              "(1_1-2_1_o)_amb      1.930509\n",
              "world_rank_o         1.872412\n",
              "(3_1-pf_o)_amb       1.866327\n",
              "music_diff           1.865697\n",
              "(3_1-pf_o)_sinc      1.830129\n",
              "tv_diff              1.794083\n",
              "tuition_o            1.779179\n",
              "worldrank_diff       1.778283\n",
              "mn_sat_o             1.775262\n",
              "(3_1-pf_o)_intel     1.735800\n",
              "movies_diff          1.725785\n",
              "art_diff             1.708855\n",
              "tvsport_diff         1.694131\n",
              "masters_o            1.675816\n",
              "masters_m            1.612274\n",
              "(3_1-pf_o)_fun       1.469180\n",
              "dining_diff          1.459898\n",
              "gaming_diff          1.396952\n",
              "exercise_diff        1.366298\n",
              "yoga_diff            1.304617\n",
              "(1_1-2_1_o)_fun      1.248845\n",
              "go_out_diff          1.247204\n",
              "date_diff            1.241214\n",
              "reading_diff         1.205274\n",
              "condtn               1.189378\n",
              "exphappy_o           1.174289\n",
              "clubbing_diff        1.171165\n",
              "shar1_1              1.147696\n",
              "order                1.126987\n",
              "career_c_m           1.071510\n",
              "imprelig_m           1.064189\n",
              "int_corr             1.061817\n",
              "imprace_m            1.055077\n",
              "income_diff          1.042229\n",
              "met_o                1.038862\n",
              "samerace             1.031096\n",
              "goal_m               1.015878\n",
              "from_m               1.008523\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj5xea4ZmYAO"
      },
      "source": [
        "approved_removed_cols.append('hiking_diff')  # other p-values decreased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7njMpTGSmYDp",
        "outputId": "7e5fd042-da44-4108-c7e4-4c0b029d3d93"
      },
      "source": [
        "#drop 'museums_diff' since it has the high p-value\n",
        "X_train = X_train.drop(columns=['goal_m'])\n",
        "\n",
        "att_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(att_model.summary())\n",
        "\n",
        "att_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, att_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 attr_o   R-squared:                       0.128\n",
            "Model:                            OLS   Adj. R-squared:                  0.121\n",
            "Method:                 Least Squares   F-statistic:                     20.05\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          3.20e-164\n",
            "Time:                        23:09:34   Log-Likelihood:                -13937.\n",
            "No. Observations:                6900   AIC:                         2.798e+04\n",
            "Df Residuals:                    6849   BIC:                         2.832e+04\n",
            "Df Model:                          50                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.1532      0.022    279.231      0.000       6.110       6.196\n",
            "gender               -0.3691      0.037     -9.984      0.000      -0.442      -0.297\n",
            "condtn                0.0424      0.024      1.767      0.077      -0.005       0.090\n",
            "order                -0.0645      0.023     -2.758      0.006      -0.110      -0.019\n",
            "int_corr              0.0442      0.023      1.947      0.052      -0.000       0.089\n",
            "samerace              0.0823      0.022      3.680      0.000       0.038       0.126\n",
            "age_o                 0.0337      0.032      1.067      0.286      -0.028       0.096\n",
            "mn_sat_o              0.0565      0.029      1.925      0.054      -0.001       0.114\n",
            "tuition_o            -0.0773      0.029     -2.629      0.009      -0.135      -0.020\n",
            "exphappy_o            0.1051      0.024      4.400      0.000       0.058       0.152\n",
            "met_o                -0.1872      0.022     -8.351      0.000      -0.231      -0.143\n",
            "world_rank_o         -0.0594      0.030     -1.971      0.049      -0.119      -0.000\n",
            "masters_o            -0.0763      0.029     -2.674      0.008      -0.132      -0.020\n",
            "sinc1_1              -0.2497      0.035     -7.077      0.000      -0.319      -0.181\n",
            "intel1_1             -0.0397      0.036     -1.101      0.271      -0.110       0.031\n",
            "amb1_1                0.0310      0.033      0.951      0.342      -0.033       0.095\n",
            "shar1_1              -0.2275      0.024     -9.638      0.000      -0.274      -0.181\n",
            "age_diff              0.0911      0.032      2.850      0.004       0.028       0.154\n",
            "income_diff          -0.0188      0.022     -0.835      0.404      -0.063       0.025\n",
            "date_diff             0.0271      0.025      1.104      0.270      -0.021       0.075\n",
            "go_out_diff           0.1411      0.025      5.733      0.000       0.093       0.189\n",
            "sports_diff          -0.1150      0.031     -3.700      0.000      -0.176      -0.054\n",
            "tvsport_diff          0.0878      0.029      3.059      0.002       0.032       0.144\n",
            "exercise_diff        -0.1499      0.026     -5.820      0.000      -0.200      -0.099\n",
            "dining_diff          -0.0781      0.027     -2.933      0.003      -0.130      -0.026\n",
            "art_diff              0.0786      0.029      2.729      0.006       0.022       0.135\n",
            "gaming_diff           0.1391      0.026      5.340      0.000       0.088       0.190\n",
            "clubbing_diff        -0.0950      0.024     -3.985      0.000      -0.142      -0.048\n",
            "reading_diff          0.0744      0.024      3.076      0.002       0.027       0.122\n",
            "tv_diff               0.1248      0.030      4.227      0.000       0.067       0.183\n",
            "theater_diff          0.1530      0.033      4.681      0.000       0.089       0.217\n",
            "movies_diff          -0.0543      0.029     -1.876      0.061      -0.111       0.002\n",
            "concerts_diff        -0.0274      0.033     -0.830      0.407      -0.092       0.037\n",
            "music_diff           -0.0312      0.030     -1.038      0.299      -0.090       0.028\n",
            "shopping_diff         0.0554      0.031      1.782      0.075      -0.006       0.116\n",
            "yoga_diff            -0.0529      0.025     -2.100      0.036      -0.102      -0.004\n",
            "worldrank_diff       -0.0725      0.029     -2.468      0.014      -0.130      -0.015\n",
            "(3_1-pf_o)_att        0.0438      0.039      1.136      0.256      -0.032       0.119\n",
            "(3_1-pf_o)_sinc      -0.0065      0.030     -0.220      0.826      -0.065       0.052\n",
            "(3_1-pf_o)_fun        0.0109      0.027      0.410      0.682      -0.041       0.063\n",
            "(3_1-pf_o)_intel      0.0501      0.029      1.726      0.084      -0.007       0.107\n",
            "(3_1-pf_o)_amb       -0.0604      0.030     -2.005      0.045      -0.119      -0.001\n",
            "(1_1-2_1_o)_sinc      0.0447      0.037      1.207      0.228      -0.028       0.117\n",
            "(1_1-2_1_o)_fun       0.0614      0.025      2.495      0.013       0.013       0.110\n",
            "(1_1-2_1_o)_intel    -0.0250      0.036     -0.694      0.488      -0.096       0.046\n",
            "(1_1-2_1_o)_amb       0.0172      0.031      0.563      0.573      -0.043       0.077\n",
            "from_m               -0.0368      0.022     -1.661      0.097      -0.080       0.007\n",
            "imprace_m            -0.0313      0.023     -1.385      0.166      -0.076       0.013\n",
            "imprelig_m            0.0436      0.023      1.917      0.055      -0.001       0.088\n",
            "career_c_m            0.0287      0.023      1.256      0.209      -0.016       0.073\n",
            "masters_m            -0.0680      0.028     -2.429      0.015      -0.123      -0.013\n",
            "==============================================================================\n",
            "Omnibus:                       99.178   Durbin-Watson:                   2.032\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              103.210\n",
            "Skew:                          -0.299   Prob(JB):                     3.87e-23\n",
            "Kurtosis:                       3.033   Cond. No.                         5.42\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3_1-pf_o)_att       3.055405\n",
              "(1_1-2_1_o)_sinc     2.831411\n",
              "gender               2.814507\n",
              "intel1_1             2.679087\n",
              "(1_1-2_1_o)_intel    2.671154\n",
              "sinc1_1              2.562937\n",
              "concerts_diff        2.247731\n",
              "theater_diff         2.200279\n",
              "amb1_1               2.194957\n",
              "age_diff             2.104126\n",
              "age_o                2.048868\n",
              "shopping_diff        1.992051\n",
              "sports_diff          1.988166\n",
              "(1_1-2_1_o)_amb      1.930196\n",
              "world_rank_o         1.872410\n",
              "(3_1-pf_o)_amb       1.866319\n",
              "music_diff           1.865683\n",
              "(3_1-pf_o)_sinc      1.829029\n",
              "tv_diff              1.793874\n",
              "tuition_o            1.779051\n",
              "worldrank_diff       1.778262\n",
              "mn_sat_o             1.775213\n",
              "(3_1-pf_o)_intel     1.735462\n",
              "movies_diff          1.725465\n",
              "art_diff             1.708783\n",
              "tvsport_diff         1.694073\n",
              "masters_o            1.674040\n",
              "masters_m            1.612270\n",
              "(3_1-pf_o)_fun       1.469153\n",
              "dining_diff          1.459890\n",
              "gaming_diff          1.396942\n",
              "exercise_diff        1.366059\n",
              "yoga_diff            1.304585\n",
              "(1_1-2_1_o)_fun      1.248834\n",
              "go_out_diff          1.247077\n",
              "date_diff            1.241202\n",
              "reading_diff         1.205268\n",
              "condtn               1.187950\n",
              "exphappy_o           1.173993\n",
              "clubbing_diff        1.171092\n",
              "shar1_1              1.146889\n",
              "order                1.126933\n",
              "career_c_m           1.071390\n",
              "imprelig_m           1.064078\n",
              "int_corr             1.061065\n",
              "imprace_m            1.055077\n",
              "income_diff          1.042102\n",
              "met_o                1.035075\n",
              "samerace             1.030418\n",
              "from_m               1.008465\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tq-LD7SmYHF"
      },
      "source": [
        "approved_removed_cols.append('goal_m')  # other p-values decreased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMGJ8P0amYK7",
        "outputId": "e1c2d06f-a994-47e7-e382-30b284fd7074"
      },
      "source": [
        "#drop 'museums_diff' since it has the high p-value\n",
        "X_train = X_train.drop(columns=['(3_1-pf_o)_sinc'])\n",
        "\n",
        "att_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(att_model.summary())\n",
        "\n",
        "att_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, att_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 attr_o   R-squared:                       0.128\n",
            "Model:                            OLS   Adj. R-squared:                  0.121\n",
            "Method:                 Least Squares   F-statistic:                     20.46\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          7.18e-165\n",
            "Time:                        23:09:36   Log-Likelihood:                -13937.\n",
            "No. Observations:                6900   AIC:                         2.797e+04\n",
            "Df Residuals:                    6850   BIC:                         2.832e+04\n",
            "Df Model:                          49                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.1532      0.022    279.250      0.000       6.110       6.196\n",
            "gender               -0.3690      0.037     -9.982      0.000      -0.441      -0.297\n",
            "condtn                0.0426      0.024      1.773      0.076      -0.004       0.090\n",
            "order                -0.0645      0.023     -2.756      0.006      -0.110      -0.019\n",
            "int_corr              0.0442      0.023      1.946      0.052      -0.000       0.089\n",
            "samerace              0.0821      0.022      3.674      0.000       0.038       0.126\n",
            "age_o                 0.0337      0.032      1.068      0.285      -0.028       0.096\n",
            "mn_sat_o              0.0560      0.029      1.914      0.056      -0.001       0.113\n",
            "tuition_o            -0.0767      0.029     -2.620      0.009      -0.134      -0.019\n",
            "exphappy_o            0.1048      0.024      4.395      0.000       0.058       0.152\n",
            "met_o                -0.1872      0.022     -8.350      0.000      -0.231      -0.143\n",
            "world_rank_o         -0.0594      0.030     -1.971      0.049      -0.119      -0.000\n",
            "masters_o            -0.0765      0.028     -2.687      0.007      -0.132      -0.021\n",
            "sinc1_1              -0.2492      0.035     -7.077      0.000      -0.318      -0.180\n",
            "intel1_1             -0.0393      0.036     -1.092      0.275      -0.110       0.031\n",
            "amb1_1                0.0303      0.032      0.933      0.351      -0.033       0.094\n",
            "shar1_1              -0.2274      0.024     -9.637      0.000      -0.274      -0.181\n",
            "age_diff              0.0913      0.032      2.857      0.004       0.029       0.154\n",
            "income_diff          -0.0189      0.022     -0.840      0.401      -0.063       0.025\n",
            "date_diff             0.0270      0.025      1.102      0.271      -0.021       0.075\n",
            "go_out_diff           0.1414      0.025      5.756      0.000       0.093       0.190\n",
            "sports_diff          -0.1145      0.031     -3.694      0.000      -0.175      -0.054\n",
            "tvsport_diff          0.0879      0.029      3.065      0.002       0.032       0.144\n",
            "exercise_diff        -0.1499      0.026     -5.821      0.000      -0.200      -0.099\n",
            "dining_diff          -0.0780      0.027     -2.930      0.003      -0.130      -0.026\n",
            "art_diff              0.0784      0.029      2.723      0.006       0.022       0.135\n",
            "gaming_diff           0.1389      0.026      5.336      0.000       0.088       0.190\n",
            "clubbing_diff        -0.0950      0.024     -3.983      0.000      -0.142      -0.048\n",
            "reading_diff          0.0747      0.024      3.088      0.002       0.027       0.122\n",
            "tv_diff               0.1249      0.030      4.231      0.000       0.067       0.183\n",
            "theater_diff          0.1532      0.033      4.688      0.000       0.089       0.217\n",
            "movies_diff          -0.0540      0.029     -1.868      0.062      -0.111       0.003\n",
            "concerts_diff        -0.0274      0.033     -0.828      0.408      -0.092       0.037\n",
            "music_diff           -0.0311      0.030     -1.034      0.301      -0.090       0.028\n",
            "shopping_diff         0.0555      0.031      1.784      0.075      -0.005       0.116\n",
            "yoga_diff            -0.0529      0.025     -2.102      0.036      -0.102      -0.004\n",
            "worldrank_diff       -0.0725      0.029     -2.467      0.014      -0.130      -0.015\n",
            "(3_1-pf_o)_att        0.0483      0.032      1.495      0.135      -0.015       0.112\n",
            "(3_1-pf_o)_fun        0.0131      0.025      0.531      0.596      -0.035       0.062\n",
            "(3_1-pf_o)_intel      0.0527      0.027      1.989      0.047       0.001       0.105\n",
            "(3_1-pf_o)_amb       -0.0580      0.028     -2.064      0.039      -0.113      -0.003\n",
            "(1_1-2_1_o)_sinc      0.0441      0.037      1.194      0.233      -0.028       0.117\n",
            "(1_1-2_1_o)_fun       0.0611      0.025      2.487      0.013       0.013       0.109\n",
            "(1_1-2_1_o)_intel    -0.0251      0.036     -0.696      0.486      -0.096       0.046\n",
            "(1_1-2_1_o)_amb       0.0177      0.031      0.578      0.563      -0.042       0.078\n",
            "from_m               -0.0368      0.022     -1.661      0.097      -0.080       0.007\n",
            "imprace_m            -0.0313      0.023     -1.381      0.167      -0.076       0.013\n",
            "imprelig_m            0.0437      0.023      1.924      0.054      -0.001       0.088\n",
            "career_c_m            0.0288      0.023      1.264      0.206      -0.016       0.074\n",
            "masters_m            -0.0680      0.028     -2.430      0.015      -0.123      -0.013\n",
            "==============================================================================\n",
            "Omnibus:                       99.226   Durbin-Watson:                   2.032\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              103.261\n",
            "Skew:                          -0.299   Prob(JB):                     3.78e-23\n",
            "Kurtosis:                       3.033   Cond. No.                         5.14\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_sinc     2.815046\n",
              "gender               2.814104\n",
              "intel1_1             2.672348\n",
              "(1_1-2_1_o)_intel    2.670843\n",
              "sinc1_1              2.553804\n",
              "concerts_diff        2.247646\n",
              "theater_diff         2.199132\n",
              "amb1_1               2.170134\n",
              "(3_1-pf_o)_att       2.154418\n",
              "age_diff             2.102464\n",
              "age_o                2.048839\n",
              "shopping_diff        1.991908\n",
              "sports_diff          1.979650\n",
              "(1_1-2_1_o)_amb      1.922838\n",
              "world_rank_o         1.872410\n",
              "music_diff           1.865054\n",
              "tv_diff              1.793532\n",
              "worldrank_diff       1.778245\n",
              "tuition_o            1.767354\n",
              "mn_sat_o             1.762984\n",
              "movies_diff          1.721299\n",
              "art_diff             1.706221\n",
              "tvsport_diff         1.693238\n",
              "masters_o            1.670730\n",
              "(3_1-pf_o)_amb       1.625256\n",
              "masters_m            1.612248\n",
              "dining_diff          1.459565\n",
              "(3_1-pf_o)_intel     1.446333\n",
              "gaming_diff          1.395927\n",
              "exercise_diff        1.366055\n",
              "yoga_diff            1.304502\n",
              "(3_1-pf_o)_fun       1.262703\n",
              "(1_1-2_1_o)_fun      1.245032\n",
              "go_out_diff          1.242802\n",
              "date_diff            1.241033\n",
              "reading_diff         1.203157\n",
              "condtn               1.187148\n",
              "exphappy_o           1.171519\n",
              "clubbing_diff        1.171046\n",
              "shar1_1              1.146689\n",
              "order                1.126771\n",
              "career_c_m           1.070145\n",
              "imprelig_m           1.063301\n",
              "int_corr             1.061057\n",
              "imprace_m            1.054697\n",
              "income_diff          1.041547\n",
              "met_o                1.035022\n",
              "samerace             1.028466\n",
              "from_m               1.008465\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liY87CqamYOt"
      },
      "source": [
        "approved_removed_cols.append('(3_1-pf_o)_sinc')  # other p-values decreased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5_lGLgWmYSg",
        "outputId": "3a78b58d-72f2-42f8-c02c-63cdf5322594"
      },
      "source": [
        "#drop '(3_1-pf_o)_fun' since it has the high p-value\n",
        "X_train = X_train.drop(columns=['(3_1-pf_o)_fun'])\n",
        "\n",
        "att_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(att_model.summary())\n",
        "\n",
        "att_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, att_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 attr_o   R-squared:                       0.128\n",
            "Model:                            OLS   Adj. R-squared:                  0.122\n",
            "Method:                 Least Squares   F-statistic:                     20.88\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          1.79e-165\n",
            "Time:                        23:09:38   Log-Likelihood:                -13937.\n",
            "No. Observations:                6900   AIC:                         2.797e+04\n",
            "Df Residuals:                    6851   BIC:                         2.831e+04\n",
            "Df Model:                          48                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.1532      0.022    279.265      0.000       6.110       6.196\n",
            "gender               -0.3684      0.037     -9.971      0.000      -0.441      -0.296\n",
            "condtn                0.0426      0.024      1.774      0.076      -0.004       0.090\n",
            "order                -0.0645      0.023     -2.756      0.006      -0.110      -0.019\n",
            "int_corr              0.0446      0.023      1.966      0.049       0.000       0.089\n",
            "samerace              0.0815      0.022      3.653      0.000       0.038       0.125\n",
            "age_o                 0.0347      0.031      1.103      0.270      -0.027       0.096\n",
            "mn_sat_o              0.0555      0.029      1.899      0.058      -0.002       0.113\n",
            "tuition_o            -0.0765      0.029     -2.612      0.009      -0.134      -0.019\n",
            "exphappy_o            0.1038      0.024      4.367      0.000       0.057       0.150\n",
            "met_o                -0.1870      0.022     -8.344      0.000      -0.231      -0.143\n",
            "world_rank_o         -0.0595      0.030     -1.972      0.049      -0.119      -0.000\n",
            "masters_o            -0.0775      0.028     -2.726      0.006      -0.133      -0.022\n",
            "sinc1_1              -0.2479      0.035     -7.057      0.000      -0.317      -0.179\n",
            "intel1_1             -0.0405      0.036     -1.127      0.260      -0.111       0.030\n",
            "amb1_1                0.0303      0.032      0.934      0.351      -0.033       0.094\n",
            "shar1_1              -0.2275      0.024     -9.640      0.000      -0.274      -0.181\n",
            "age_diff              0.0911      0.032      2.850      0.004       0.028       0.154\n",
            "income_diff          -0.0193      0.022     -0.860      0.390      -0.063       0.025\n",
            "date_diff             0.0270      0.025      1.102      0.271      -0.021       0.075\n",
            "go_out_diff           0.1431      0.024      5.874      0.000       0.095       0.191\n",
            "sports_diff          -0.1142      0.031     -3.685      0.000      -0.175      -0.053\n",
            "tvsport_diff          0.0872      0.029      3.044      0.002       0.031       0.143\n",
            "exercise_diff        -0.1503      0.026     -5.837      0.000      -0.201      -0.100\n",
            "dining_diff          -0.0774      0.027     -2.911      0.004      -0.130      -0.025\n",
            "art_diff              0.0774      0.029      2.693      0.007       0.021       0.134\n",
            "gaming_diff           0.1394      0.026      5.357      0.000       0.088       0.190\n",
            "clubbing_diff        -0.0959      0.024     -4.034      0.000      -0.143      -0.049\n",
            "reading_diff          0.0744      0.024      3.079      0.002       0.027       0.122\n",
            "tv_diff               0.1253      0.029      4.249      0.000       0.068       0.183\n",
            "theater_diff          0.1542      0.033      4.727      0.000       0.090       0.218\n",
            "movies_diff          -0.0542      0.029     -1.873      0.061      -0.111       0.003\n",
            "concerts_diff        -0.0260      0.033     -0.791      0.429      -0.091       0.039\n",
            "music_diff           -0.0320      0.030     -1.066      0.286      -0.091       0.027\n",
            "shopping_diff         0.0541      0.031      1.746      0.081      -0.007       0.115\n",
            "yoga_diff            -0.0534      0.025     -2.122      0.034      -0.103      -0.004\n",
            "worldrank_diff       -0.0728      0.029     -2.477      0.013      -0.130      -0.015\n",
            "(3_1-pf_o)_att        0.0442      0.031      1.408      0.159      -0.017       0.106\n",
            "(3_1-pf_o)_intel      0.0492      0.026      1.918      0.055      -0.001       0.099\n",
            "(3_1-pf_o)_amb       -0.0608      0.028     -2.205      0.027      -0.115      -0.007\n",
            "(1_1-2_1_o)_sinc      0.0425      0.037      1.153      0.249      -0.030       0.115\n",
            "(1_1-2_1_o)_fun       0.0638      0.024      2.654      0.008       0.017       0.111\n",
            "(1_1-2_1_o)_intel    -0.0236      0.036     -0.656      0.512      -0.094       0.047\n",
            "(1_1-2_1_o)_amb       0.0185      0.031      0.605      0.545      -0.041       0.078\n",
            "from_m               -0.0365      0.022     -1.648      0.099      -0.080       0.007\n",
            "imprace_m            -0.0312      0.023     -1.380      0.168      -0.076       0.013\n",
            "imprelig_m            0.0432      0.023      1.905      0.057      -0.001       0.088\n",
            "career_c_m            0.0284      0.023      1.246      0.213      -0.016       0.073\n",
            "masters_m            -0.0679      0.028     -2.428      0.015      -0.123      -0.013\n",
            "==============================================================================\n",
            "Omnibus:                       99.976   Durbin-Watson:                   2.032\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              104.072\n",
            "Skew:                          -0.300   Prob(JB):                     2.52e-23\n",
            "Kurtosis:                       3.034   Cond. No.                         5.07\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gender               2.811077\n",
              "(1_1-2_1_o)_sinc     2.794560\n",
              "intel1_1             2.661843\n",
              "(1_1-2_1_o)_intel    2.654019\n",
              "sinc1_1              2.542175\n",
              "concerts_diff        2.234862\n",
              "theater_diff         2.191478\n",
              "amb1_1               2.170128\n",
              "age_diff             2.102037\n",
              "age_o                2.040975\n",
              "(3_1-pf_o)_att       2.027823\n",
              "sports_diff          1.979035\n",
              "shopping_diff        1.978833\n",
              "(1_1-2_1_o)_amb      1.918303\n",
              "world_rank_o         1.872408\n",
              "music_diff           1.859115\n",
              "tv_diff              1.791912\n",
              "worldrank_diff       1.777732\n",
              "tuition_o            1.766923\n",
              "mn_sat_o             1.761390\n",
              "movies_diff          1.721145\n",
              "art_diff             1.698587\n",
              "tvsport_diff         1.689744\n",
              "masters_o            1.664123\n",
              "masters_m            1.612232\n",
              "(3_1-pf_o)_amb       1.566666\n",
              "dining_diff          1.457287\n",
              "gaming_diff          1.394292\n",
              "exercise_diff        1.365104\n",
              "(3_1-pf_o)_intel     1.355412\n",
              "yoga_diff            1.302830\n",
              "date_diff            1.241033\n",
              "go_out_diff          1.221980\n",
              "reading_diff         1.202750\n",
              "(1_1-2_1_o)_fun      1.191713\n",
              "condtn               1.187147\n",
              "clubbing_diff        1.164651\n",
              "exphappy_o           1.164444\n",
              "shar1_1              1.146661\n",
              "order                1.126771\n",
              "career_c_m           1.068817\n",
              "imprelig_m           1.061642\n",
              "int_corr             1.059845\n",
              "imprace_m            1.054696\n",
              "income_diff          1.040192\n",
              "met_o                1.034767\n",
              "samerace             1.025960\n",
              "from_m               1.007797\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doJoS9Wxm-ZB"
      },
      "source": [
        "approved_removed_cols.append('(3_1-pf_o)_fun')  # other p-values decreased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTLt_DJsnNAf"
      },
      "source": [
        "Overview: First, after dropping the feature with highest VIF (in the very first OLS model trained on all features), other VIF values improved and lowered. But, the R2 and Adjusted R2 remained the same. Second, after dropping the next feature  with highest VIF, other VIF values improved and lowered again. But, the R2 and Adjusted R2 remained the same. Next, in our third OLS model iteration, since the VIF values are all now low, we drop the feature with the highest p-value. Other p-values and VIF values seemed to improve with each feature with the high p-value dropped. But, the R2 and Adjusted R2 remain the same. Therefore, since the R2 and Adjusted R2 continue to remain the same and since the VIF values are all low and p-values relatively low, the above model is our final linear regression model for attractiveness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlTsbn8hpbKa"
      },
      "source": [
        "## Attractiveness (Linear Regression - Bootstrap OSR2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQfE6qWHnGgx",
        "outputId": "2b122652-a026-4158-b66a-38cd7b400c65"
      },
      "source": [
        "approved_removed_cols"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(1_1-2_1_o)_att',\n",
              " 'attr1_1',\n",
              " '(1_1-2_1_o)_shar',\n",
              " 'fun1_1',\n",
              " 'income',\n",
              " 'museums_diff',\n",
              " 'hiking_diff',\n",
              " 'goal_m',\n",
              " '(3_1-pf_o)_sinc',\n",
              " '(3_1-pf_o)_fun']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-tfgmcOzZM4"
      },
      "source": [
        "#define the current model as the final model \"att_lr\"\n",
        "att_lr = att_model\n",
        "\n",
        "#remove the variables from test set\n",
        "X_test = X_test.drop(columns = approved_removed_cols)\n",
        "\n",
        "y_pred = att_lr.predict(sm.add_constant(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cvx6JpUnzldV",
        "outputId": "acaf8c8b-4346-4833-9533-9faf14b8335e"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3635    5.213655\n",
              "345     7.497802\n",
              "7621    6.030626\n",
              "5572    6.153742\n",
              "2437    6.536251\n",
              "          ...   \n",
              "3627    4.765776\n",
              "1468    5.495794\n",
              "603     5.115442\n",
              "2632    6.404941\n",
              "7342    5.580319\n",
              "Length: 1725, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFriFs8FnGkm",
        "outputId": "a216a77b-e0c7-4da6-c361-c84f5d16d4bb"
      },
      "source": [
        "# compute out-of-sample R-squared using the test set\n",
        "def OSR2(predictions, y_test,y_train):\n",
        "    SSE = np.sum((y_test-predictions)**2)\n",
        "    SST = np.sum((y_test-np.mean(y_train))**2)\n",
        "    r2 = 1-SSE/SST\n",
        "    return r2\n",
        "\n",
        "# Check OSR^2 with test set\n",
        "att_test_OSR2 = OSR2(att_lr.predict(sm.add_constant(X_test)), y_test, y_train)\n",
        "print(\"Test OSR2: \\n\", att_test_OSR2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test OSR2: \n",
            " 0.08263346804105587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBL6XX_so7ID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d3047b-714d-4c56-9073-6cef78510e93"
      },
      "source": [
        "att_bs_output_lr = bootstrap_validation(sm.add_constant(X_test),y_test,y_train,att_lr,\n",
        "                                 metrics_list=[OSR2],\n",
        "                                 sample = 5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQSvtmPRm-kg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "5873b847-ec17-4946-b9e6-4ce72a15285f"
      },
      "source": [
        "#bootstrap plots\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap OSR2 Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot OSR2 - Test Set OSR2', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].hist(att_bs_output_lr.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[1].hist(att_bs_output_lr.iloc[:,0]-att_test_OSR2, bins=20,edgecolor='green', linewidth=2,color = \"grey\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  2.,   6.,   8.,  50.,  72., 137., 279., 435., 586., 703., 674.,\n",
              "        643., 525., 401., 236., 128.,  65.,  34.,  13.,   3.]),\n",
              " array([-0.05592928, -0.05053242, -0.04513555, -0.03973869, -0.03434183,\n",
              "        -0.02894496, -0.0235481 , -0.01815124, -0.01275437, -0.00735751,\n",
              "        -0.00196065,  0.00343622,  0.00883308,  0.01422995,  0.01962681,\n",
              "         0.02502367,  0.03042054,  0.0358174 ,  0.04121426,  0.04661113,\n",
              "         0.05200799]),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ6SAaJuiLYC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8a039a19-4c97-4c48-93aa-a18ab4603395"
      },
      "source": [
        "# The 95% confidence interval\n",
        "CI_0 = np.quantile(att_bs_output_lr.iloc[:,0]-att_test_OSR2,np.array([0.025,0.975]))\n",
        "CI = [0,0]\n",
        "CI[0] = att_test_OSR2 - CI_0[1]\n",
        "CI[1] = att_test_OSR2 - CI_0[0]\n",
        "fig, axs = plt.subplots(ncols=1, figsize=(8,6))\n",
        "axs.set_xlabel('Boot OSR2 - Test Set OSR2', fontsize=16)\n",
        "axs.set_ylabel('Count', fontsize=16)\n",
        "axs.hist(att_bs_output_lr.iloc[:,0]-att_test_OSR2, bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs.vlines(x=CI_0[0], ymin = 0, ymax =800, color = \"black\")\n",
        "axs.vlines(x=CI_0[1], ymin = 0, ymax =800, color = \"black\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.LineCollection at 0x7fd0bc361c90>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6f5c8gdjFEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2880f78-a7c8-4d40-a4c1-6bcfec63ba91"
      },
      "source": [
        "import statistics\n",
        "print('The bootstrapped mean of linear regression is: %s' % np.mean(att_bs_output_lr)[0])\n",
        "\n",
        "print(\"The 95-percent confidence interval of the test set OSR2 for the linear regression attractiveness model is %s\" % CI)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The bootstrapped mean of linear regression is: 0.08264157708774161\n",
            "The 95-percent confidence interval of the test set OSR2 for the linear regression attractiveness model is [0.05296610191541393, 0.11252961420726236]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I09tBqmOqIAm"
      },
      "source": [
        "## Attractiveness (LDA - Train Model + Bootstrap Accuracy)]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddatHDQ2jIC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db42d701-2884-498c-fecf-e63b0206f265"
      },
      "source": [
        "# Run LDA on attractiveness trait.\n",
        "import sklearn\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# redefine y_train/y_test type to int\n",
        "y_train = att_y_train.astype(int)\n",
        "y_test = att_y_test.astype(int)\n",
        "\n",
        "# train\n",
        "att_lda = LinearDiscriminantAnalysis()\n",
        "att_lda.fit(sm.add_constant(att_X_train), y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearDiscriminantAnalysis()"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeNk_3wGqzbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8f5eb5-3c28-4e7e-fe06-46d3f1c029dd"
      },
      "source": [
        "# compute attractivness accuracy of LDA\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = att_lda.predict(sm.add_constant(att_X_test))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix : \\n\", cm)\n",
        "att_test_lda_acc = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nTest Set Accuracy:\", att_test_lda_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix : \n",
            " [[  0   0   0   0   0   0   2   1   0   0   0]\n",
            " [  0   4   0   0   2   4   6   5   1   0   2]\n",
            " [  1   1   1   0   5   8  13  11   3   0   0]\n",
            " [  0   2   0   2  11  21  35  21   2   2   4]\n",
            " [  0   0   2   1  22  30  53  44  11   1   1]\n",
            " [  2   4   4   0  15  56 101  77  14   0   3]\n",
            " [  0   1   4   1  17  37 120 117  25   0  12]\n",
            " [  0   1   0   2  15  27 125 121  36   1  14]\n",
            " [  0   1   3   0   2  23  97  94  45   1  11]\n",
            " [  0   0   0   0   0   8  34  32  12   1   9]\n",
            " [  0   0   0   0   0   4  16  23   8   4  10]]\n",
            "\n",
            "Test Set Accuracy: 0.22144927536231884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xASZvhN9mJsl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0b9be1-b275-4317-9975-842e0a7a4c34"
      },
      "source": [
        "# Bootstrap LDA model and report the mean and 95% CI for accuracy. \n",
        "def acc(predictions, y_test, y_train):\n",
        "  return accuracy_score(y_test, predictions)\n",
        "\n",
        "att_bs_output_lda_acc = bootstrap_validation(sm.add_constant(att_X_test),y_test,y_train,att_lda,\n",
        "                                 metrics_list=[acc],\n",
        "                                 sample = 5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugoFKn_znwNZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "7b785c0a-f1ff-4296-fa36-9299d858b3a1"
      },
      "source": [
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap Accuracy Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot Accuracy - Test Set Accuracy', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].hist(att_bs_output_lda_acc.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[1].hist(att_bs_output_lda_acc.iloc[:,0]-att_test_lda_acc, bins=20,edgecolor='green', linewidth=2,color = \"grey\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 12.,  10.,  36.,  83., 133., 277., 381., 518., 627., 716., 726.,\n",
              "        541., 396., 251., 150.,  71.,  44.,  15.,   9.,   4.]),\n",
              " array([-0.03362319, -0.03008696, -0.02655072, -0.02301449, -0.01947826,\n",
              "        -0.01594203, -0.0124058 , -0.00886957, -0.00533333, -0.0017971 ,\n",
              "         0.00173913,  0.00527536,  0.00881159,  0.01234783,  0.01588406,\n",
              "         0.01942029,  0.02295652,  0.02649275,  0.03002899,  0.03356522,\n",
              "         0.03710145]),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lExPN_7NnwWO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "ff5044c5-2e53-47d8-9037-f4f74fa2948c"
      },
      "source": [
        "# The 95% confidence interval\n",
        "CI_0 = np.quantile(att_bs_output_lda_acc.iloc[:,0]-att_test_lda_acc,np.array([0.025,0.975]))\n",
        "CI = [0,0]\n",
        "CI[0] = att_test_lda_acc - CI_0[1]\n",
        "CI[1] = att_test_lda_acc - CI_0[0]\n",
        "fig, axs = plt.subplots(ncols=1, figsize=(8,6))\n",
        "axs.set_xlabel('Boot Accuracy - Test Set Accuracy', fontsize=16)\n",
        "axs.set_ylabel('Count', fontsize=16)\n",
        "axs.hist(att_bs_output_lda_acc.iloc[:,0]-att_test_lda_acc, bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs.vlines(x=CI_0[0], ymin = 0, ymax =800, color = \"black\")\n",
        "axs.vlines(x=CI_0[1], ymin = 0, ymax =800, color = \"black\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.LineCollection at 0x7fd0ba17b9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxDR64sqp6bL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df0d7fbe-3b6b-4e65-879b-a80a93333473"
      },
      "source": [
        "import statistics\n",
        "print('The bootstrapped mean of LDA Accuracy is:', np.mean(att_bs_output_lda_acc)[0])\n",
        "print(\"The 95-percent confidence interval of the test set accuuracy for the LDA attractiveness model is %s\" % CI)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The bootstrapped mean of LDA Accuracy is: 0.22151744927536432\n",
            "The 95-percent confidence interval of the test set accuuracy for the LDA attractiveness model is [0.2011594202898551, 0.24115942028985507]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attractiveness (Rescaled OSR^2 & Baseline Comparisons)"
      ],
      "metadata": {
        "id": "ZambMKJS-D2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_att = 1 - (np.sum((att_y_test - att_y_train.value_counts().index[0])**2)/np.sum((att_y_test - np.mean(att_y_train))**2))\n",
        "rm1_lr_att = rescaled_OSR2_1(att_lr, att_y_train, att_test.drop(columns = approved_removed_cols), 'attr_o', 'lr')\n",
        "rm2_lr_att = rescaled_OSR2_2(att_lr, att_y_train, att_test.drop(columns = approved_removed_cols), 'attr_o', 'lr')\n",
        "rm1_lda_att = rescaled_OSR2_1(att_lda, att_y_train, att_test, 'attr_o', 'lda')\n",
        "rm2_lda_att = rescaled_OSR2_1(att_lda, att_y_train, att_test, 'attr_o', 'lda')\n",
        "\n",
        "print('NON-RESCALED:')\n",
        "non_rescaled_models_OSR2 = {\"LR OSR2\": np.mean(att_bs_output_lr)[0], \"LDA Accuracy\": np.mean(att_bs_output_lda_acc)[0]}\n",
        "print('Models for Attractiveness:')\n",
        "print('No Rescale Bootstrapped Mean, Linear Regression OSR2 for Attractiveness:', np.mean(att_bs_output_lr)[0])\n",
        "print('No Rescale Bootstrapped Mean, LDA Accuracy for Attractiveness:', np.mean(att_bs_output_lda_acc)[0])\n",
        "print()\n",
        "print('Rescaling Linear Regression Model before OSR2 Calculation:')\n",
        "print('Rescale Method 1, Linear Regression Sincerity OSR2:', rm1_lr_att)\n",
        "print('Rescale Method 2, Linear Regression Sincerity OSR2:', rm2_lr_att)\n",
        "print()\n",
        "print('BASELINE:')\n",
        "print('Baseline OSR2:', base_att)\n",
        "print()\n",
        "print('ATTRACTIVENESS MODEL COMPARISONS:')\n",
        "print('OSR2 difference between LR model and baseline:', np.amax(np.array([rm1_lr_att, rm2_lr_att, rm1_lda_att, rm2_lda_att, np.mean(att_bs_output_lr)[0]])) - base_att)\n",
        "print('Accuracy difference for LDA and baseline:', np.mean(att_bs_output_lda_acc)[0] - att_y_train.value_counts().values[0]/np.sum(att_y_train.value_counts()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqhWMiZa-NpK",
        "outputId": "b7aedd8a-2b61-4fc4-ccbe-f4e2607e5ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NON-RESCALED:\n",
            "Models for Attractiveness:\n",
            "No Rescale Bootstrapped Mean, Linear Regression OSR2 for Attractiveness: 0.08264157708774161\n",
            "No Rescale Bootstrapped Mean, LDA Accuracy for Attractiveness: 0.22151744927536432\n",
            "\n",
            "Rescaling Linear Regression Model before OSR2 Calculation:\n",
            "Rescale Method 1, Linear Regression Sincerity OSR2: -5.267815217298948\n",
            "Rescale Method 2, Linear Regression Sincerity OSR2: -4.672718450019012\n",
            "\n",
            "BASELINE:\n",
            "Baseline OSR2: -0.004502690764422779\n",
            "\n",
            "ATTRACTIVENESS MODEL COMPARISONS:\n",
            "OSR2 difference between LR model and baseline: 0.08714426785216439\n",
            "Accuracy difference for LDA and baseline: 0.015575420289857062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R44h8Fi3r-CB"
      },
      "source": [
        "## Sincerity (Linear Regression + LDA)\n",
        "\n",
        "Make linear model for this trait, perform feature selection.\n",
        "Bootstrap your model and report the mean and 95% CI for OSR^2.\n",
        "\n",
        "Run LDA on this trait. Bootstrap this model and report the mean and 95% CI for accuracy. Calculate OSR^2 as well.\n",
        "\n",
        "Calculate rescaled OSR^2 using the two methods above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYgswAEdsPpn"
      },
      "source": [
        "## Sincerity (Linear Regression - Feature Selection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0TNDtNXAnxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13b6f51-919b-430d-975a-109348b49fec"
      },
      "source": [
        "# Fit linear model for sinc_o\n",
        "X_train = sin_X_train.copy()\n",
        "y_train = sin_y_train.copy()\n",
        "X_test = sin_X_test.copy()\n",
        "y_test = sin_y_test.copy()\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "X_train = sm.add_constant(X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "sin_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sin_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 sinc_o   R-squared:                       0.070\n",
            "Model:                            OLS   Adj. R-squared:                  0.062\n",
            "Method:                 Least Squares   F-statistic:                     8.833\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           8.30e-71\n",
            "Time:                        23:10:32   Log-Likelihood:                -13226.\n",
            "No. Observations:                6849   AIC:                         2.657e+04\n",
            "Df Residuals:                    6790   BIC:                         2.697e+04\n",
            "Df Model:                          58                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.1616      0.020    353.603      0.000       7.122       7.201\n",
            "gender               -0.1616      0.035     -4.611      0.000      -0.230      -0.093\n",
            "condtn               -0.0142      0.022     -0.639      0.523      -0.058       0.029\n",
            "order                -0.2118      0.021     -9.876      0.000      -0.254      -0.170\n",
            "int_corr              0.0264      0.021      1.261      0.207      -0.015       0.067\n",
            "samerace             -0.0121      0.021     -0.586      0.558      -0.053       0.028\n",
            "age_o                 0.0615      0.030      2.069      0.039       0.003       0.120\n",
            "mn_sat_o             -0.0184      0.027     -0.687      0.492      -0.071       0.034\n",
            "tuition_o             0.0081      0.027      0.300      0.764      -0.045       0.061\n",
            "income                0.0333      0.029      1.140      0.254      -0.024       0.091\n",
            "exphappy_o            0.0923      0.022      4.192      0.000       0.049       0.135\n",
            "met_o                -0.0680      0.021     -3.294      0.001      -0.108      -0.028\n",
            "world_rank_o         -0.0747      0.028     -2.694      0.007      -0.129      -0.020\n",
            "masters_o            -0.0540      0.026     -2.039      0.041      -0.106      -0.002\n",
            "attr1_1              -0.2039      0.184     -1.109      0.267      -0.564       0.156\n",
            "sinc1_1              -0.1251      0.113     -1.110      0.267      -0.346       0.096\n",
            "intel1_1             -0.0950      0.110     -0.862      0.388      -0.311       0.121\n",
            "fun1_1               -0.1091      0.094     -1.163      0.245      -0.293       0.075\n",
            "amb1_1               -0.0992      0.094     -1.055      0.292      -0.284       0.085\n",
            "shar1_1              -0.0403      0.102     -0.394      0.693      -0.240       0.160\n",
            "age_diff             -0.0196      0.030     -0.653      0.514      -0.079       0.039\n",
            "income_diff           0.0693      0.030      2.333      0.020       0.011       0.128\n",
            "date_diff             0.0022      0.023      0.099      0.921      -0.042       0.047\n",
            "go_out_diff          -0.0019      0.023     -0.082      0.935      -0.047       0.043\n",
            "sports_diff           0.0531      0.029      1.812      0.070      -0.004       0.111\n",
            "tvsport_diff          0.0031      0.027      0.116      0.907      -0.050       0.056\n",
            "exercise_diff         0.0252      0.024      1.065      0.287      -0.021       0.071\n",
            "dining_diff           0.0148      0.025      0.587      0.557      -0.035       0.064\n",
            "museums_diff         -0.0006      0.043     -0.014      0.989      -0.085       0.083\n",
            "art_diff              0.0767      0.041      1.855      0.064      -0.004       0.158\n",
            "hiking_diff          -0.1088      0.024     -4.526      0.000      -0.156      -0.062\n",
            "gaming_diff          -0.0109      0.025     -0.442      0.658      -0.059       0.037\n",
            "clubbing_diff        -0.0574      0.022     -2.604      0.009      -0.101      -0.014\n",
            "reading_diff          0.1486      0.023      6.540      0.000       0.104       0.193\n",
            "tv_diff               0.1435      0.027      5.256      0.000       0.090       0.197\n",
            "theater_diff          0.0823      0.031      2.669      0.008       0.022       0.143\n",
            "movies_diff          -0.0205      0.027     -0.767      0.443      -0.073       0.032\n",
            "concerts_diff        -0.0717      0.031     -2.303      0.021      -0.133      -0.011\n",
            "music_diff            0.0611      0.028      2.172      0.030       0.006       0.116\n",
            "shopping_diff         0.0372      0.029      1.265      0.206      -0.020       0.095\n",
            "yoga_diff            -0.0525      0.023     -2.233      0.026      -0.099      -0.006\n",
            "worldrank_diff        0.0633      0.027      2.345      0.019       0.010       0.116\n",
            "(3_1-pf_o)_att       -0.1079      0.038     -2.816      0.005      -0.183      -0.033\n",
            "(3_1-pf_o)_sinc      -0.0639      0.028     -2.312      0.021      -0.118      -0.010\n",
            "(3_1-pf_o)_fun       -0.0558      0.025     -2.207      0.027      -0.105      -0.006\n",
            "(3_1-pf_o)_intel      0.0303      0.028      1.092      0.275      -0.024       0.085\n",
            "(3_1-pf_o)_amb       -0.1541      0.028     -5.425      0.000      -0.210      -0.098\n",
            "(1_1-2_1_o)_att       0.2158      0.195      1.107      0.268      -0.166       0.598\n",
            "(1_1-2_1_o)_sinc      0.1525      0.111      1.372      0.170      -0.065       0.371\n",
            "(1_1-2_1_o)_fun       0.1546      0.101      1.533      0.125      -0.043       0.352\n",
            "(1_1-2_1_o)_intel     0.0450      0.105      0.426      0.670      -0.162       0.252\n",
            "(1_1-2_1_o)_amb       0.1162      0.093      1.254      0.210      -0.065       0.298\n",
            "(1_1-2_1_o)_shar     -0.0056      0.100     -0.056      0.955      -0.202       0.191\n",
            "from_m               -0.0386      0.020     -1.898      0.058      -0.078       0.001\n",
            "goal_m               -0.0308      0.020     -1.509      0.131      -0.071       0.009\n",
            "imprace_m            -0.0305      0.021     -1.467      0.142      -0.071       0.010\n",
            "imprelig_m           -0.0098      0.021     -0.469      0.639      -0.051       0.031\n",
            "career_c_m            0.0384      0.021      1.825      0.068      -0.003       0.080\n",
            "masters_m             0.0005      0.026      0.021      0.983      -0.050       0.051\n",
            "==============================================================================\n",
            "Omnibus:                      406.681   Durbin-Watson:                   2.049\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              505.923\n",
            "Skew:                          -0.583   Prob(JB):                    1.38e-110\n",
            "Kurtosis:                       3.642   Cond. No.                         42.4\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy3ztU-kBYCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d20ff1-7bb9-4703-83c4-0cc303ffb252"
      },
      "source": [
        "#Check VIF values\n",
        "sin_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(X_train, sin_cols).sort_values(ascending=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_att      92.662468\n",
              "attr1_1              82.356616\n",
              "sinc1_1              30.973064\n",
              "(1_1-2_1_o)_sinc     30.149949\n",
              "intel1_1             29.586034\n",
              "(1_1-2_1_o)_intel    27.111868\n",
              "shar1_1              25.394311\n",
              "(1_1-2_1_o)_fun      24.779600\n",
              "(1_1-2_1_o)_shar     24.429000\n",
              "amb1_1               21.569322\n",
              "fun1_1               21.433765\n",
              "(1_1-2_1_o)_amb      20.950659\n",
              "museums_diff          4.478008\n",
              "art_diff              4.164241\n",
              "(3_1-pf_o)_att        3.579080\n",
              "gender                2.992396\n",
              "concerts_diff         2.363950\n",
              "theater_diff          2.315230\n",
              "age_diff              2.202690\n",
              "age_o                 2.151545\n",
              "income_diff           2.150737\n",
              "shopping_diff         2.102889\n",
              "sports_diff           2.092564\n",
              "income                2.083709\n",
              "(3_1-pf_o)_amb        1.967482\n",
              "music_diff            1.925903\n",
              "(3_1-pf_o)_intel      1.882815\n",
              "world_rank_o          1.873417\n",
              "(3_1-pf_o)_sinc       1.859124\n",
              "tv_diff               1.818135\n",
              "worldrank_diff        1.778769\n",
              "tuition_o             1.763842\n",
              "tvsport_diff          1.759880\n",
              "mn_sat_o              1.746410\n",
              "movies_diff           1.741595\n",
              "masters_o             1.711724\n",
              "masters_m             1.636483\n",
              "(3_1-pf_o)_fun        1.560988\n",
              "dining_diff           1.542588\n",
              "gaming_diff           1.484395\n",
              "hiking_diff           1.408555\n",
              "exercise_diff         1.360945\n",
              "yoga_diff             1.345848\n",
              "go_out_diff           1.266487\n",
              "reading_diff          1.258429\n",
              "date_diff             1.254493\n",
              "condtn                1.197738\n",
              "clubbing_diff         1.183812\n",
              "exphappy_o            1.181116\n",
              "order                 1.120913\n",
              "career_c_m            1.076646\n",
              "int_corr              1.068282\n",
              "imprelig_m            1.060684\n",
              "imprace_m             1.055664\n",
              "met_o                 1.037941\n",
              "samerace              1.037670\n",
              "goal_m                1.018552\n",
              "from_m                1.008662\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi8QL63jE7dP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "928d50fb-ac42-4449-cf68-476770de170a"
      },
      "source": [
        "#drop '(1_1-2_1_o)_att' since it has the highest VIF value\n",
        "X_train = X_train.drop(columns=['(1_1-2_1_o)_att'])\n",
        "\n",
        "sin_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sin_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 sinc_o   R-squared:                       0.070\n",
            "Model:                            OLS   Adj. R-squared:                  0.062\n",
            "Method:                 Least Squares   F-statistic:                     8.966\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           4.75e-71\n",
            "Time:                        23:10:35   Log-Likelihood:                -13227.\n",
            "No. Observations:                6849   AIC:                         2.657e+04\n",
            "Df Residuals:                    6791   BIC:                         2.697e+04\n",
            "Df Model:                          57                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.1616      0.020    353.597      0.000       7.122       7.201\n",
            "gender               -0.1658      0.035     -4.761      0.000      -0.234      -0.098\n",
            "condtn               -0.0142      0.022     -0.641      0.521      -0.058       0.029\n",
            "order                -0.2122      0.021     -9.895      0.000      -0.254      -0.170\n",
            "int_corr              0.0266      0.021      1.273      0.203      -0.014       0.068\n",
            "samerace             -0.0111      0.021     -0.537      0.591      -0.051       0.029\n",
            "age_o                 0.0630      0.030      2.121      0.034       0.005       0.121\n",
            "mn_sat_o             -0.0171      0.027     -0.639      0.523      -0.070       0.035\n",
            "tuition_o             0.0063      0.027      0.235      0.814      -0.046       0.059\n",
            "income                0.0349      0.029      1.193      0.233      -0.022       0.092\n",
            "exphappy_o            0.0918      0.022      4.172      0.000       0.049       0.135\n",
            "met_o                -0.0680      0.021     -3.294      0.001      -0.108      -0.028\n",
            "world_rank_o         -0.0767      0.028     -2.773      0.006      -0.131      -0.022\n",
            "masters_o            -0.0537      0.026     -2.025      0.043      -0.106      -0.002\n",
            "attr1_1              -0.0578      0.128     -0.452      0.651      -0.309       0.193\n",
            "sinc1_1              -0.0391      0.082     -0.479      0.632      -0.199       0.121\n",
            "intel1_1             -0.0117      0.080     -0.145      0.885      -0.169       0.146\n",
            "fun1_1               -0.0387      0.069     -0.562      0.574      -0.174       0.096\n",
            "amb1_1               -0.0270      0.068     -0.398      0.691      -0.160       0.106\n",
            "shar1_1               0.0364      0.075      0.485      0.628      -0.111       0.183\n",
            "age_diff             -0.0199      0.030     -0.663      0.507      -0.079       0.039\n",
            "income_diff           0.0711      0.030      2.395      0.017       0.013       0.129\n",
            "date_diff             0.0023      0.023      0.102      0.919      -0.042       0.047\n",
            "go_out_diff          -0.0013      0.023     -0.057      0.955      -0.046       0.043\n",
            "sports_diff           0.0532      0.029      1.817      0.069      -0.004       0.111\n",
            "tvsport_diff          0.0019      0.027      0.070      0.944      -0.051       0.054\n",
            "exercise_diff         0.0253      0.024      1.070      0.285      -0.021       0.072\n",
            "dining_diff           0.0148      0.025      0.588      0.557      -0.035       0.064\n",
            "museums_diff       -1.91e-05      0.043     -0.000      1.000      -0.084       0.084\n",
            "art_diff              0.0747      0.041      1.810      0.070      -0.006       0.156\n",
            "hiking_diff          -0.1084      0.024     -4.508      0.000      -0.155      -0.061\n",
            "gaming_diff          -0.0098      0.025     -0.398      0.691      -0.058       0.039\n",
            "clubbing_diff        -0.0580      0.022     -2.634      0.008      -0.101      -0.015\n",
            "reading_diff          0.1495      0.023      6.582      0.000       0.105       0.194\n",
            "tv_diff               0.1426      0.027      5.222      0.000       0.089       0.196\n",
            "theater_diff          0.0819      0.031      2.658      0.008       0.022       0.142\n",
            "movies_diff          -0.0221      0.027     -0.829      0.407      -0.074       0.030\n",
            "concerts_diff        -0.0715      0.031     -2.297      0.022      -0.133      -0.010\n",
            "music_diff            0.0606      0.028      2.156      0.031       0.006       0.116\n",
            "shopping_diff         0.0374      0.029      1.272      0.203      -0.020       0.095\n",
            "yoga_diff            -0.0526      0.023     -2.237      0.025      -0.099      -0.006\n",
            "worldrank_diff        0.0634      0.027      2.347      0.019       0.010       0.116\n",
            "(3_1-pf_o)_att       -0.1050      0.038     -2.747      0.006      -0.180      -0.030\n",
            "(3_1-pf_o)_sinc      -0.0639      0.028     -2.315      0.021      -0.118      -0.010\n",
            "(3_1-pf_o)_fun       -0.0564      0.025     -2.228      0.026      -0.106      -0.007\n",
            "(3_1-pf_o)_intel      0.0298      0.028      1.072      0.284      -0.025       0.084\n",
            "(3_1-pf_o)_amb       -0.1559      0.028     -5.495      0.000      -0.211      -0.100\n",
            "(1_1-2_1_o)_sinc      0.0353      0.034      1.041      0.298      -0.031       0.102\n",
            "(1_1-2_1_o)_fun       0.0482      0.031      1.576      0.115      -0.012       0.108\n",
            "(1_1-2_1_o)_intel    -0.0659      0.033     -1.995      0.046      -0.131      -0.001\n",
            "(1_1-2_1_o)_amb       0.0186      0.029      0.652      0.514      -0.037       0.075\n",
            "(1_1-2_1_o)_shar     -0.1105      0.032     -3.399      0.001      -0.174      -0.047\n",
            "from_m               -0.0387      0.020     -1.903      0.057      -0.079       0.001\n",
            "goal_m               -0.0313      0.020     -1.530      0.126      -0.071       0.009\n",
            "imprace_m            -0.0311      0.021     -1.494      0.135      -0.072       0.010\n",
            "imprelig_m           -0.0093      0.021     -0.446      0.656      -0.050       0.032\n",
            "career_c_m            0.0388      0.021      1.847      0.065      -0.002       0.080\n",
            "masters_m             0.0017      0.026      0.067      0.947      -0.049       0.052\n",
            "==============================================================================\n",
            "Omnibus:                      403.151   Durbin-Watson:                   2.049\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              500.699\n",
            "Skew:                          -0.580   Prob(JB):                    1.88e-109\n",
            "Kurtosis:                       3.638   Cond. No.                         22.3\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn2b4zxpF0rw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c674cf5-77db-44e4-b515-708c1c45d079"
      },
      "source": [
        "#Check VIF values again\n",
        "sin_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(X_train, sin_cols).sort_values(ascending=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "attr1_1              39.878240\n",
              "sinc1_1              16.250240\n",
              "intel1_1             15.764493\n",
              "shar1_1              13.718254\n",
              "fun1_1               11.584725\n",
              "amb1_1               11.189660\n",
              "museums_diff          4.477383\n",
              "art_diff              4.156704\n",
              "(3_1-pf_o)_att        3.562535\n",
              "gender                2.956683\n",
              "(1_1-2_1_o)_sinc      2.798632\n",
              "(1_1-2_1_o)_intel     2.660289\n",
              "(1_1-2_1_o)_shar      2.574410\n",
              "concerts_diff         2.363878\n",
              "theater_diff          2.314979\n",
              "(1_1-2_1_o)_fun       2.281163\n",
              "age_diff              2.202490\n",
              "age_o                 2.147134\n",
              "income_diff           2.144652\n",
              "shopping_diff         2.102793\n",
              "sports_diff           2.092524\n",
              "income                2.079046\n",
              "(1_1-2_1_o)_amb       1.983249\n",
              "(3_1-pf_o)_amb        1.961276\n",
              "music_diff            1.925504\n",
              "(3_1-pf_o)_intel      1.882200\n",
              "world_rank_o          1.865222\n",
              "(3_1-pf_o)_sinc       1.859109\n",
              "tv_diff               1.816202\n",
              "worldrank_diff        1.778762\n",
              "tuition_o             1.757809\n",
              "tvsport_diff          1.756749\n",
              "mn_sat_o              1.743080\n",
              "movies_diff           1.736402\n",
              "masters_o             1.711438\n",
              "masters_m             1.633702\n",
              "(3_1-pf_o)_fun        1.560413\n",
              "dining_diff           1.542588\n",
              "gaming_diff           1.481999\n",
              "hiking_diff           1.408187\n",
              "exercise_diff         1.360924\n",
              "yoga_diff             1.345835\n",
              "go_out_diff           1.265835\n",
              "reading_diff          1.256916\n",
              "date_diff             1.254483\n",
              "condtn                1.197733\n",
              "clubbing_diff         1.182971\n",
              "exphappy_o            1.180675\n",
              "order                 1.120620\n",
              "career_c_m            1.076237\n",
              "int_corr              1.068175\n",
              "imprelig_m            1.060225\n",
              "imprace_m             1.055054\n",
              "met_o                 1.037941\n",
              "samerace              1.035565\n",
              "goal_m                1.018177\n",
              "from_m                1.008645\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95YFFeGZGyYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9cbe717-170c-49a6-f985-160e85ee2b37"
      },
      "source": [
        "#drop 'attr1_1 ' since it has the highest VIF value now\n",
        "X_train = X_train.drop(columns=['attr1_1'])\n",
        "\n",
        "sin_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sin_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 sinc_o   R-squared:                       0.070\n",
            "Model:                            OLS   Adj. R-squared:                  0.062\n",
            "Method:                 Least Squares   F-statistic:                     9.124\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           1.71e-71\n",
            "Time:                        23:10:37   Log-Likelihood:                -13227.\n",
            "No. Observations:                6849   AIC:                         2.657e+04\n",
            "Df Residuals:                    6792   BIC:                         2.696e+04\n",
            "Df Model:                          56                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.1616      0.020    353.618      0.000       7.122       7.201\n",
            "gender               -0.1671      0.035     -4.816      0.000      -0.235      -0.099\n",
            "condtn               -0.0137      0.022     -0.619      0.536      -0.057       0.030\n",
            "order                -0.2123      0.021     -9.902      0.000      -0.254      -0.170\n",
            "int_corr              0.0266      0.021      1.270      0.204      -0.014       0.068\n",
            "samerace             -0.0114      0.021     -0.552      0.581      -0.052       0.029\n",
            "age_o                 0.0633      0.030      2.135      0.033       0.005       0.121\n",
            "mn_sat_o             -0.0172      0.027     -0.644      0.519      -0.070       0.035\n",
            "tuition_o             0.0065      0.027      0.240      0.810      -0.046       0.059\n",
            "income                0.0348      0.029      1.192      0.233      -0.022       0.092\n",
            "exphappy_o            0.0916      0.022      4.162      0.000       0.048       0.135\n",
            "met_o                -0.0681      0.021     -3.298      0.001      -0.109      -0.028\n",
            "world_rank_o         -0.0769      0.028     -2.780      0.005      -0.131      -0.023\n",
            "masters_o            -0.0535      0.026     -2.020      0.043      -0.105      -0.002\n",
            "sinc1_1              -0.0053      0.033     -0.162      0.871      -0.070       0.059\n",
            "intel1_1              0.0214      0.033      0.638      0.523      -0.044       0.087\n",
            "fun1_1               -0.0107      0.030     -0.355      0.722      -0.070       0.048\n",
            "amb1_1                0.0004      0.030      0.014      0.989      -0.059       0.060\n",
            "shar1_1               0.0672      0.031      2.144      0.032       0.006       0.129\n",
            "age_diff             -0.0199      0.030     -0.662      0.508      -0.079       0.039\n",
            "income_diff           0.0711      0.030      2.398      0.017       0.013       0.129\n",
            "date_diff             0.0022      0.023      0.098      0.922      -0.042       0.047\n",
            "go_out_diff          -0.0011      0.023     -0.048      0.961      -0.046       0.044\n",
            "sports_diff           0.0537      0.029      1.835      0.066      -0.004       0.111\n",
            "tvsport_diff          0.0011      0.027      0.040      0.968      -0.051       0.054\n",
            "exercise_diff         0.0253      0.024      1.070      0.285      -0.021       0.072\n",
            "dining_diff           0.0144      0.025      0.573      0.567      -0.035       0.064\n",
            "museums_diff       -1.57e-06      0.043  -3.66e-05      1.000      -0.084       0.084\n",
            "art_diff              0.0750      0.041      1.816      0.069      -0.006       0.156\n",
            "hiking_diff          -0.1082      0.024     -4.501      0.000      -0.155      -0.061\n",
            "gaming_diff          -0.0099      0.025     -0.400      0.689      -0.058       0.038\n",
            "clubbing_diff        -0.0579      0.022     -2.630      0.009      -0.101      -0.015\n",
            "reading_diff          0.1498      0.023      6.598      0.000       0.105       0.194\n",
            "tv_diff               0.1426      0.027      5.223      0.000       0.089       0.196\n",
            "theater_diff          0.0813      0.031      2.640      0.008       0.021       0.142\n",
            "movies_diff          -0.0226      0.027     -0.849      0.396      -0.075       0.030\n",
            "concerts_diff        -0.0708      0.031     -2.276      0.023      -0.132      -0.010\n",
            "music_diff            0.0600      0.028      2.138      0.033       0.005       0.115\n",
            "shopping_diff         0.0381      0.029      1.300      0.194      -0.019       0.096\n",
            "yoga_diff            -0.0528      0.023     -2.245      0.025      -0.099      -0.007\n",
            "worldrank_diff        0.0635      0.027      2.351      0.019       0.011       0.116\n",
            "(3_1-pf_o)_att       -0.1049      0.038     -2.744      0.006      -0.180      -0.030\n",
            "(3_1-pf_o)_sinc      -0.0640      0.028     -2.318      0.021      -0.118      -0.010\n",
            "(3_1-pf_o)_fun       -0.0565      0.025     -2.234      0.026      -0.106      -0.007\n",
            "(3_1-pf_o)_intel      0.0301      0.028      1.083      0.279      -0.024       0.085\n",
            "(3_1-pf_o)_amb       -0.1561      0.028     -5.505      0.000      -0.212      -0.101\n",
            "(1_1-2_1_o)_sinc      0.0351      0.034      1.037      0.300      -0.031       0.102\n",
            "(1_1-2_1_o)_fun       0.0483      0.031      1.579      0.114      -0.012       0.108\n",
            "(1_1-2_1_o)_intel    -0.0660      0.033     -1.997      0.046      -0.131      -0.001\n",
            "(1_1-2_1_o)_amb       0.0186      0.029      0.651      0.515      -0.037       0.074\n",
            "(1_1-2_1_o)_shar     -0.1109      0.032     -3.415      0.001      -0.175      -0.047\n",
            "from_m               -0.0388      0.020     -1.910      0.056      -0.079       0.001\n",
            "goal_m               -0.0315      0.020     -1.541      0.123      -0.072       0.009\n",
            "imprace_m            -0.0311      0.021     -1.497      0.135      -0.072       0.010\n",
            "imprelig_m           -0.0093      0.021     -0.447      0.655      -0.050       0.032\n",
            "career_c_m            0.0390      0.021      1.858      0.063      -0.002       0.080\n",
            "masters_m             0.0017      0.026      0.065      0.948      -0.049       0.052\n",
            "==============================================================================\n",
            "Omnibus:                      403.081   Durbin-Watson:                   2.048\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              500.600\n",
            "Skew:                          -0.580   Prob(JB):                    1.98e-109\n",
            "Kurtosis:                       3.638   Cond. No.                         6.30\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0H7dflctHSrR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb52b92-ef4e-4615-c728-c90ef924f9c3"
      },
      "source": [
        "#Check VIF values again\n",
        "sin_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(X_train, sin_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "museums_diff         4.477379\n",
              "art_diff             4.156036\n",
              "(3_1-pf_o)_att       3.562320\n",
              "gender               2.935958\n",
              "(1_1-2_1_o)_sinc     2.798327\n",
              "intel1_1             2.734650\n",
              "(1_1-2_1_o)_intel    2.660220\n",
              "sinc1_1              2.628547\n",
              "(1_1-2_1_o)_shar     2.571822\n",
              "shar1_1              2.392135\n",
              "concerts_diff        2.357381\n",
              "theater_diff         2.309739\n",
              "(1_1-2_1_o)_fun      2.281086\n",
              "amb1_1               2.242851\n",
              "fun1_1               2.208265\n",
              "age_diff             2.202470\n",
              "age_o                2.145410\n",
              "income_diff          2.144604\n",
              "shopping_diff        2.096102\n",
              "sports_diff          2.089623\n",
              "income               2.079037\n",
              "(1_1-2_1_o)_amb      1.983235\n",
              "(3_1-pf_o)_amb       1.960645\n",
              "music_diff           1.921640\n",
              "(3_1-pf_o)_intel     1.881042\n",
              "world_rank_o         1.864783\n",
              "(3_1-pf_o)_sinc      1.859066\n",
              "tv_diff              1.816200\n",
              "worldrank_diff       1.778659\n",
              "tuition_o            1.757596\n",
              "tvsport_diff         1.749295\n",
              "mn_sat_o             1.742854\n",
              "movies_diff          1.733216\n",
              "masters_o            1.711198\n",
              "masters_m            1.633686\n",
              "(3_1-pf_o)_fun       1.560201\n",
              "dining_diff          1.540828\n",
              "gaming_diff          1.481953\n",
              "hiking_diff          1.407694\n",
              "exercise_diff        1.360924\n",
              "yoga_diff            1.345408\n",
              "go_out_diff          1.265384\n",
              "reading_diff         1.255978\n",
              "date_diff            1.254372\n",
              "condtn               1.194613\n",
              "clubbing_diff        1.182847\n",
              "exphappy_o           1.179976\n",
              "order                1.120432\n",
              "career_c_m           1.075650\n",
              "int_corr             1.068139\n",
              "imprelig_m           1.060217\n",
              "imprace_m            1.055033\n",
              "met_o                1.037882\n",
              "samerace             1.034411\n",
              "goal_m               1.017659\n",
              "from_m               1.008438\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DiZ-7d5Ht2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5935894f-1ef2-4beb-a167-7ed55a626717"
      },
      "source": [
        "#drop 'museums_diff' since it has the highest VIF value now\n",
        "X_train = X_train.drop(columns=['museums_diff'])\n",
        "\n",
        "sin_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sin_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 sinc_o   R-squared:                       0.070\n",
            "Model:                            OLS   Adj. R-squared:                  0.062\n",
            "Method:                 Least Squares   F-statistic:                     9.291\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           5.58e-72\n",
            "Time:                        23:10:40   Log-Likelihood:                -13227.\n",
            "No. Observations:                6849   AIC:                         2.657e+04\n",
            "Df Residuals:                    6793   BIC:                         2.695e+04\n",
            "Df Model:                          55                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.1616      0.020    353.644      0.000       7.122       7.201\n",
            "gender               -0.1671      0.035     -4.826      0.000      -0.235      -0.099\n",
            "condtn               -0.0137      0.022     -0.619      0.536      -0.057       0.030\n",
            "order                -0.2123      0.021     -9.903      0.000      -0.254      -0.170\n",
            "int_corr              0.0266      0.021      1.270      0.204      -0.014       0.068\n",
            "samerace             -0.0114      0.021     -0.552      0.581      -0.052       0.029\n",
            "age_o                 0.0633      0.030      2.136      0.033       0.005       0.121\n",
            "mn_sat_o             -0.0172      0.027     -0.645      0.519      -0.070       0.035\n",
            "tuition_o             0.0065      0.027      0.240      0.810      -0.046       0.059\n",
            "income                0.0348      0.029      1.193      0.233      -0.022       0.092\n",
            "exphappy_o            0.0916      0.022      4.163      0.000       0.048       0.135\n",
            "met_o                -0.0681      0.021     -3.298      0.001      -0.108      -0.028\n",
            "world_rank_o         -0.0769      0.028     -2.781      0.005      -0.131      -0.023\n",
            "masters_o            -0.0535      0.026     -2.020      0.043      -0.105      -0.002\n",
            "sinc1_1              -0.0053      0.033     -0.163      0.871      -0.070       0.059\n",
            "intel1_1              0.0214      0.033      0.642      0.521      -0.044       0.087\n",
            "fun1_1               -0.0107      0.030     -0.356      0.722      -0.070       0.048\n",
            "amb1_1                0.0004      0.030      0.014      0.989      -0.059       0.060\n",
            "shar1_1               0.0672      0.031      2.144      0.032       0.006       0.129\n",
            "age_diff             -0.0199      0.030     -0.662      0.508      -0.079       0.039\n",
            "income_diff           0.0711      0.030      2.403      0.016       0.013       0.129\n",
            "date_diff             0.0022      0.023      0.098      0.922      -0.042       0.047\n",
            "go_out_diff          -0.0011      0.023     -0.048      0.961      -0.046       0.044\n",
            "sports_diff           0.0537      0.029      1.836      0.066      -0.004       0.111\n",
            "tvsport_diff          0.0011      0.027      0.040      0.968      -0.051       0.053\n",
            "exercise_diff         0.0253      0.024      1.070      0.285      -0.021       0.072\n",
            "dining_diff           0.0144      0.025      0.582      0.561      -0.034       0.063\n",
            "art_diff              0.0750      0.027      2.794      0.005       0.022       0.128\n",
            "hiking_diff          -0.1082      0.024     -4.506      0.000      -0.155      -0.061\n",
            "gaming_diff          -0.0099      0.025     -0.402      0.688      -0.058       0.038\n",
            "clubbing_diff        -0.0579      0.022     -2.630      0.009      -0.101      -0.015\n",
            "reading_diff          0.1498      0.022      6.730      0.000       0.106       0.193\n",
            "tv_diff               0.1426      0.027      5.230      0.000       0.089       0.196\n",
            "theater_diff          0.0813      0.030      2.673      0.008       0.022       0.141\n",
            "movies_diff          -0.0226      0.027     -0.849      0.396      -0.075       0.030\n",
            "concerts_diff        -0.0708      0.031     -2.277      0.023      -0.132      -0.010\n",
            "music_diff            0.0600      0.028      2.139      0.032       0.005       0.115\n",
            "shopping_diff         0.0381      0.029      1.307      0.191      -0.019       0.095\n",
            "yoga_diff            -0.0528      0.023     -2.249      0.025      -0.099      -0.007\n",
            "worldrank_diff        0.0635      0.027      2.355      0.019       0.011       0.116\n",
            "(3_1-pf_o)_att       -0.1049      0.038     -2.745      0.006      -0.180      -0.030\n",
            "(3_1-pf_o)_sinc      -0.0640      0.028     -2.318      0.020      -0.118      -0.010\n",
            "(3_1-pf_o)_fun       -0.0565      0.025     -2.235      0.025      -0.106      -0.007\n",
            "(3_1-pf_o)_intel      0.0301      0.028      1.085      0.278      -0.024       0.084\n",
            "(3_1-pf_o)_amb       -0.1561      0.028     -5.510      0.000      -0.212      -0.101\n",
            "(1_1-2_1_o)_sinc      0.0351      0.034      1.037      0.300      -0.031       0.102\n",
            "(1_1-2_1_o)_fun       0.0483      0.031      1.579      0.114      -0.012       0.108\n",
            "(1_1-2_1_o)_intel    -0.0660      0.033     -2.000      0.045      -0.131      -0.001\n",
            "(1_1-2_1_o)_amb       0.0186      0.028      0.652      0.514      -0.037       0.074\n",
            "(1_1-2_1_o)_shar     -0.1109      0.032     -3.415      0.001      -0.175      -0.047\n",
            "from_m               -0.0388      0.020     -1.910      0.056      -0.079       0.001\n",
            "goal_m               -0.0315      0.020     -1.541      0.123      -0.072       0.009\n",
            "imprace_m            -0.0311      0.021     -1.497      0.134      -0.072       0.010\n",
            "imprelig_m           -0.0093      0.021     -0.447      0.655      -0.050       0.032\n",
            "career_c_m            0.0390      0.021      1.858      0.063      -0.002       0.080\n",
            "masters_m             0.0017      0.026      0.065      0.948      -0.049       0.052\n",
            "==============================================================================\n",
            "Omnibus:                      403.081   Durbin-Watson:                   2.048\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              500.600\n",
            "Skew:                          -0.580   Prob(JB):                    1.98e-109\n",
            "Kurtosis:                       3.638   Cond. No.                         5.87\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI8-iRyeH_g6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faf7a4bd-b13c-4221-d264-4c4b9bdb107d"
      },
      "source": [
        "#Check VIF values again\n",
        "sin_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(X_train, sin_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3_1-pf_o)_att       3.559574\n",
              "gender               2.924254\n",
              "(1_1-2_1_o)_sinc     2.796545\n",
              "intel1_1             2.708356\n",
              "(1_1-2_1_o)_intel    2.651283\n",
              "sinc1_1              2.623767\n",
              "(1_1-2_1_o)_shar     2.571546\n",
              "shar1_1              2.392113\n",
              "concerts_diff        2.356906\n",
              "(1_1-2_1_o)_fun      2.279493\n",
              "theater_diff         2.253347\n",
              "amb1_1               2.241606\n",
              "fun1_1               2.205342\n",
              "age_diff             2.202455\n",
              "age_o                2.144205\n",
              "income_diff          2.135653\n",
              "sports_diff          2.089019\n",
              "income               2.078810\n",
              "shopping_diff        2.073500\n",
              "(1_1-2_1_o)_amb      1.974869\n",
              "(3_1-pf_o)_amb       1.957542\n",
              "music_diff           1.921381\n",
              "(3_1-pf_o)_intel     1.875603\n",
              "world_rank_o         1.864028\n",
              "(3_1-pf_o)_sinc      1.858839\n",
              "tv_diff              1.811450\n",
              "worldrank_diff       1.772810\n",
              "tuition_o            1.756946\n",
              "art_diff             1.755120\n",
              "mn_sat_o             1.741688\n",
              "tvsport_diff         1.737984\n",
              "movies_diff          1.730491\n",
              "masters_o            1.710981\n",
              "masters_m            1.633285\n",
              "(3_1-pf_o)_fun       1.558243\n",
              "dining_diff          1.495200\n",
              "gaming_diff          1.472362\n",
              "hiking_diff          1.405057\n",
              "exercise_diff        1.359716\n",
              "yoga_diff            1.341609\n",
              "go_out_diff          1.265287\n",
              "date_diff            1.253827\n",
              "reading_diff         1.207208\n",
              "condtn               1.194612\n",
              "clubbing_diff        1.182771\n",
              "exphappy_o           1.179775\n",
              "order                1.120329\n",
              "career_c_m           1.075579\n",
              "int_corr             1.068107\n",
              "imprelig_m           1.060198\n",
              "imprace_m            1.054499\n",
              "met_o                1.037827\n",
              "samerace             1.034392\n",
              "goal_m               1.017523\n",
              "from_m               1.008431\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pYcDJxqILF4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be9e4eca-2789-4769-94a0-5d61dfa7a75c"
      },
      "source": [
        "#drop '(3_1-pf_o)_att' since it has the highest VIF value now\n",
        "X_train = X_train.drop(columns=['(3_1-pf_o)_att'])\n",
        "\n",
        "sin_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sin_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 sinc_o   R-squared:                       0.069\n",
            "Model:                            OLS   Adj. R-squared:                  0.062\n",
            "Method:                 Least Squares   F-statistic:                     9.315\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           5.30e-71\n",
            "Time:                        23:10:42   Log-Likelihood:                -13231.\n",
            "No. Observations:                6849   AIC:                         2.657e+04\n",
            "Df Residuals:                    6794   BIC:                         2.695e+04\n",
            "Df Model:                          54                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.1616      0.020    353.474      0.000       7.122       7.201\n",
            "gender               -0.1879      0.034     -5.556      0.000      -0.254      -0.122\n",
            "condtn               -0.0160      0.022     -0.724      0.469      -0.059       0.027\n",
            "order                -0.2118      0.021     -9.878      0.000      -0.254      -0.170\n",
            "int_corr              0.0249      0.021      1.191      0.234      -0.016       0.066\n",
            "samerace             -0.0118      0.021     -0.570      0.569      -0.052       0.029\n",
            "age_o                 0.0610      0.030      2.057      0.040       0.003       0.119\n",
            "mn_sat_o             -0.0178      0.027     -0.666      0.505      -0.070       0.035\n",
            "tuition_o             0.0086      0.027      0.319      0.750      -0.044       0.061\n",
            "income                0.0309      0.029      1.058      0.290      -0.026       0.088\n",
            "exphappy_o            0.0916      0.022      4.162      0.000       0.048       0.135\n",
            "met_o                -0.0681      0.021     -3.301      0.001      -0.109      -0.028\n",
            "world_rank_o         -0.0741      0.028     -2.681      0.007      -0.128      -0.020\n",
            "masters_o            -0.0568      0.026     -2.145      0.032      -0.109      -0.005\n",
            "sinc1_1              -0.0169      0.033     -0.520      0.603      -0.081       0.047\n",
            "intel1_1              0.0231      0.033      0.694      0.488      -0.042       0.088\n",
            "fun1_1               -0.0148      0.030     -0.492      0.623      -0.074       0.044\n",
            "amb1_1               -0.0147      0.030     -0.492      0.623      -0.073       0.044\n",
            "shar1_1               0.0489      0.031      1.596      0.110      -0.011       0.109\n",
            "age_diff             -0.0164      0.030     -0.546      0.585      -0.075       0.042\n",
            "income_diff           0.0667      0.030      2.255      0.024       0.009       0.125\n",
            "date_diff            -0.0028      0.023     -0.124      0.901      -0.047       0.042\n",
            "go_out_diff          -0.0034      0.023     -0.151      0.880      -0.048       0.041\n",
            "sports_diff           0.0611      0.029      2.094      0.036       0.004       0.118\n",
            "tvsport_diff          0.0013      0.027      0.050      0.960      -0.051       0.054\n",
            "exercise_diff         0.0289      0.024      1.223      0.221      -0.017       0.075\n",
            "dining_diff           0.0162      0.025      0.655      0.513      -0.032       0.065\n",
            "art_diff              0.0753      0.027      2.806      0.005       0.023       0.128\n",
            "hiking_diff          -0.1067      0.024     -4.442      0.000      -0.154      -0.060\n",
            "gaming_diff          -0.0102      0.025     -0.415      0.678      -0.058       0.038\n",
            "clubbing_diff        -0.0566      0.022     -2.569      0.010      -0.100      -0.013\n",
            "reading_diff          0.1514      0.022      6.805      0.000       0.108       0.195\n",
            "tv_diff               0.1402      0.027      5.145      0.000       0.087       0.194\n",
            "theater_diff          0.0801      0.030      2.634      0.008       0.021       0.140\n",
            "movies_diff          -0.0208      0.027     -0.780      0.436      -0.073       0.031\n",
            "concerts_diff        -0.0769      0.031     -2.477      0.013      -0.138      -0.016\n",
            "music_diff            0.0635      0.028      2.263      0.024       0.008       0.118\n",
            "shopping_diff         0.0472      0.029      1.628      0.104      -0.010       0.104\n",
            "yoga_diff            -0.0527      0.023     -2.245      0.025      -0.099      -0.007\n",
            "worldrank_diff        0.0652      0.027      2.419      0.016       0.012       0.118\n",
            "(3_1-pf_o)_sinc      -0.0221      0.023     -0.959      0.337      -0.067       0.023\n",
            "(3_1-pf_o)_fun       -0.0268      0.023     -1.173      0.241      -0.072       0.018\n",
            "(3_1-pf_o)_intel      0.0748      0.022      3.330      0.001       0.031       0.119\n",
            "(3_1-pf_o)_amb       -0.1152      0.024     -4.778      0.000      -0.162      -0.068\n",
            "(1_1-2_1_o)_sinc      0.0557      0.033      1.687      0.092      -0.009       0.120\n",
            "(1_1-2_1_o)_fun       0.0582      0.030      1.915      0.055      -0.001       0.118\n",
            "(1_1-2_1_o)_intel    -0.0610      0.033     -1.851      0.064      -0.126       0.004\n",
            "(1_1-2_1_o)_amb       0.0308      0.028      1.097      0.273      -0.024       0.086\n",
            "(1_1-2_1_o)_shar     -0.0816      0.031     -2.660      0.008      -0.142      -0.021\n",
            "from_m               -0.0375      0.020     -1.845      0.065      -0.077       0.002\n",
            "goal_m               -0.0328      0.020     -1.605      0.108      -0.073       0.007\n",
            "imprace_m            -0.0312      0.021     -1.499      0.134      -0.072       0.010\n",
            "imprelig_m           -0.0080      0.021     -0.385      0.700      -0.049       0.033\n",
            "career_c_m            0.0387      0.021      1.839      0.066      -0.003       0.080\n",
            "masters_m             0.0016      0.026      0.062      0.950      -0.049       0.052\n",
            "==============================================================================\n",
            "Omnibus:                      399.295   Durbin-Watson:                   2.047\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              494.732\n",
            "Skew:                          -0.578   Prob(JB):                    3.72e-108\n",
            "Kurtosis:                       3.632   Cond. No.                         5.19\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmhf3KilIW8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28cfb1d1-3b58-4ed3-e639-edce29ec029f"
      },
      "source": [
        "#Check VIF values again\n",
        "sin_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(X_train, sin_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gender               2.785052\n",
              "intel1_1             2.707375\n",
              "(1_1-2_1_o)_sinc     2.659193\n",
              "(1_1-2_1_o)_intel    2.643256\n",
              "sinc1_1              2.580332\n",
              "concerts_diff        2.344970\n",
              "(1_1-2_1_o)_shar     2.294173\n",
              "shar1_1              2.284098\n",
              "theater_diff         2.252933\n",
              "(1_1-2_1_o)_fun      2.247841\n",
              "fun1_1               2.199937\n",
              "age_diff             2.198523\n",
              "amb1_1               2.167806\n",
              "age_o                2.142437\n",
              "income_diff          2.129243\n",
              "income               2.073729\n",
              "sports_diff          2.071607\n",
              "shopping_diff        2.046948\n",
              "(1_1-2_1_o)_amb      1.926082\n",
              "music_diff           1.917513\n",
              "world_rank_o         1.861492\n",
              "tv_diff              1.809697\n",
              "worldrank_diff       1.771826\n",
              "tuition_o            1.755498\n",
              "art_diff             1.755080\n",
              "mn_sat_o             1.741580\n",
              "tvsport_diff         1.737964\n",
              "movies_diff          1.729373\n",
              "masters_o            1.707481\n",
              "masters_m            1.633283\n",
              "dining_diff          1.494140\n",
              "gaming_diff          1.472328\n",
              "(3_1-pf_o)_amb       1.416413\n",
              "hiking_diff          1.404334\n",
              "exercise_diff        1.355559\n",
              "yoga_diff            1.341608\n",
              "(3_1-pf_o)_sinc      1.290188\n",
              "(3_1-pf_o)_fun       1.272747\n",
              "go_out_diff          1.263527\n",
              "date_diff            1.245674\n",
              "(3_1-pf_o)_intel     1.228965\n",
              "reading_diff         1.206295\n",
              "condtn               1.192866\n",
              "clubbing_diff        1.182202\n",
              "exphappy_o           1.179775\n",
              "order                1.120266\n",
              "career_c_m           1.075533\n",
              "int_corr             1.067221\n",
              "imprelig_m           1.059648\n",
              "imprace_m            1.054498\n",
              "met_o                1.037824\n",
              "samerace             1.034346\n",
              "goal_m               1.016964\n",
              "from_m               1.007867\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tLrPu8DIgys"
      },
      "source": [
        "All VIF values look small, let remove the large p-value next"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQiAvn5nIpD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a57c31d3-3be7-42a3-c96d-81d790c8bdfe"
      },
      "source": [
        "#from our newest model, let's remove 'tvsport_diff' which has the largest p-value\n",
        "X_train = X_train.drop(columns=['tvsport_diff'])\n",
        "\n",
        "sin_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sin_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 sinc_o   R-squared:                       0.069\n",
            "Model:                            OLS   Adj. R-squared:                  0.062\n",
            "Method:                 Least Squares   F-statistic:                     9.492\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           1.71e-71\n",
            "Time:                        23:10:44   Log-Likelihood:                -13231.\n",
            "No. Observations:                6849   AIC:                         2.657e+04\n",
            "Df Residuals:                    6795   BIC:                         2.694e+04\n",
            "Df Model:                          53                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.1616      0.020    353.500      0.000       7.122       7.201\n",
            "gender               -0.1879      0.034     -5.562      0.000      -0.254      -0.122\n",
            "condtn               -0.0160      0.022     -0.724      0.469      -0.059       0.027\n",
            "order                -0.2119      0.021     -9.881      0.000      -0.254      -0.170\n",
            "int_corr              0.0249      0.021      1.192      0.233      -0.016       0.066\n",
            "samerace             -0.0117      0.021     -0.570      0.569      -0.052       0.029\n",
            "age_o                 0.0610      0.030      2.056      0.040       0.003       0.119\n",
            "mn_sat_o             -0.0179      0.027     -0.669      0.503      -0.070       0.034\n",
            "tuition_o             0.0086      0.027      0.320      0.749      -0.044       0.061\n",
            "income                0.0308      0.029      1.057      0.290      -0.026       0.088\n",
            "exphappy_o            0.0916      0.022      4.162      0.000       0.048       0.135\n",
            "met_o                -0.0681      0.021     -3.301      0.001      -0.109      -0.028\n",
            "world_rank_o         -0.0741      0.028     -2.680      0.007      -0.128      -0.020\n",
            "masters_o            -0.0567      0.026     -2.145      0.032      -0.109      -0.005\n",
            "sinc1_1              -0.0168      0.033     -0.518      0.605      -0.081       0.047\n",
            "intel1_1              0.0231      0.033      0.694      0.488      -0.042       0.088\n",
            "fun1_1               -0.0149      0.030     -0.496      0.620      -0.074       0.044\n",
            "amb1_1               -0.0147      0.030     -0.492      0.623      -0.073       0.044\n",
            "shar1_1               0.0489      0.031      1.596      0.111      -0.011       0.109\n",
            "age_diff             -0.0164      0.030     -0.547      0.585      -0.075       0.042\n",
            "income_diff           0.0667      0.030      2.255      0.024       0.009       0.125\n",
            "date_diff            -0.0027      0.023     -0.121      0.904      -0.047       0.041\n",
            "go_out_diff          -0.0034      0.023     -0.150      0.881      -0.048       0.041\n",
            "sports_diff           0.0617      0.026      2.348      0.019       0.010       0.113\n",
            "exercise_diff         0.0289      0.024      1.229      0.219      -0.017       0.075\n",
            "dining_diff           0.0162      0.025      0.654      0.513      -0.032       0.065\n",
            "art_diff              0.0754      0.027      2.808      0.005       0.023       0.128\n",
            "hiking_diff          -0.1068      0.024     -4.463      0.000      -0.154      -0.060\n",
            "gaming_diff          -0.0100      0.024     -0.412      0.680      -0.058       0.038\n",
            "clubbing_diff        -0.0566      0.022     -2.570      0.010      -0.100      -0.013\n",
            "reading_diff          0.1513      0.022      6.823      0.000       0.108       0.195\n",
            "tv_diff               0.1406      0.026      5.376      0.000       0.089       0.192\n",
            "theater_diff          0.0802      0.030      2.637      0.008       0.021       0.140\n",
            "movies_diff          -0.0209      0.027     -0.788      0.431      -0.073       0.031\n",
            "concerts_diff        -0.0768      0.031     -2.478      0.013      -0.138      -0.016\n",
            "music_diff            0.0635      0.028      2.266      0.023       0.009       0.119\n",
            "shopping_diff         0.0471      0.029      1.627      0.104      -0.010       0.104\n",
            "yoga_diff            -0.0527      0.023     -2.249      0.025      -0.099      -0.007\n",
            "worldrank_diff        0.0652      0.027      2.419      0.016       0.012       0.118\n",
            "(3_1-pf_o)_sinc      -0.0221      0.023     -0.961      0.336      -0.067       0.023\n",
            "(3_1-pf_o)_fun       -0.0269      0.023     -1.177      0.239      -0.072       0.018\n",
            "(3_1-pf_o)_intel      0.0748      0.022      3.330      0.001       0.031       0.119\n",
            "(3_1-pf_o)_amb       -0.1152      0.024     -4.778      0.000      -0.162      -0.068\n",
            "(1_1-2_1_o)_sinc      0.0556      0.033      1.688      0.091      -0.009       0.120\n",
            "(1_1-2_1_o)_fun       0.0583      0.030      1.932      0.053      -0.001       0.118\n",
            "(1_1-2_1_o)_intel    -0.0609      0.033     -1.851      0.064      -0.125       0.004\n",
            "(1_1-2_1_o)_amb       0.0308      0.028      1.096      0.273      -0.024       0.086\n",
            "(1_1-2_1_o)_shar     -0.0816      0.031     -2.661      0.008      -0.142      -0.021\n",
            "from_m               -0.0375      0.020     -1.845      0.065      -0.077       0.002\n",
            "goal_m               -0.0328      0.020     -1.605      0.108      -0.073       0.007\n",
            "imprace_m            -0.0312      0.021     -1.499      0.134      -0.072       0.010\n",
            "imprelig_m           -0.0080      0.021     -0.385      0.700      -0.049       0.033\n",
            "career_c_m            0.0386      0.021      1.839      0.066      -0.003       0.080\n",
            "masters_m             0.0016      0.026      0.063      0.949      -0.049       0.052\n",
            "==============================================================================\n",
            "Omnibus:                      399.282   Durbin-Watson:                   2.047\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              494.718\n",
            "Skew:                          -0.578   Prob(JB):                    3.74e-108\n",
            "Kurtosis:                       3.632   Cond. No.                         5.17\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBrqwFjrKufa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e17c61-7278-4ad3-a7b7-b9170216ae0d"
      },
      "source": [
        "#from our newest model, let's remove 'masters_m' which has the largest p-value\n",
        "X_train = X_train.drop(columns=['masters_m'])\n",
        "\n",
        "sin_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sin_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 sinc_o   R-squared:                       0.069\n",
            "Model:                            OLS   Adj. R-squared:                  0.062\n",
            "Method:                 Least Squares   F-statistic:                     9.676\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           5.47e-72\n",
            "Time:                        23:10:44   Log-Likelihood:                -13231.\n",
            "No. Observations:                6849   AIC:                         2.657e+04\n",
            "Df Residuals:                    6796   BIC:                         2.693e+04\n",
            "Df Model:                          52                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.1616      0.020    353.526      0.000       7.122       7.201\n",
            "gender               -0.1880      0.034     -5.570      0.000      -0.254      -0.122\n",
            "condtn               -0.0160      0.022     -0.722      0.470      -0.059       0.027\n",
            "order                -0.2119      0.021     -9.882      0.000      -0.254      -0.170\n",
            "int_corr              0.0249      0.021      1.191      0.234      -0.016       0.066\n",
            "samerace             -0.0117      0.021     -0.570      0.569      -0.052       0.029\n",
            "age_o                 0.0608      0.029      2.063      0.039       0.003       0.118\n",
            "mn_sat_o             -0.0179      0.027     -0.669      0.504      -0.070       0.034\n",
            "tuition_o             0.0086      0.027      0.321      0.748      -0.044       0.061\n",
            "income                0.0308      0.029      1.057      0.290      -0.026       0.088\n",
            "exphappy_o            0.0916      0.022      4.163      0.000       0.048       0.135\n",
            "met_o                -0.0681      0.021     -3.302      0.001      -0.109      -0.028\n",
            "world_rank_o         -0.0741      0.028     -2.683      0.007      -0.128      -0.020\n",
            "masters_o            -0.0577      0.021     -2.720      0.007      -0.099      -0.016\n",
            "sinc1_1              -0.0168      0.032     -0.517      0.605      -0.080       0.047\n",
            "intel1_1              0.0232      0.033      0.695      0.487      -0.042       0.088\n",
            "fun1_1               -0.0149      0.030     -0.499      0.618      -0.074       0.044\n",
            "amb1_1               -0.0147      0.030     -0.493      0.622      -0.073       0.044\n",
            "shar1_1               0.0489      0.031      1.596      0.110      -0.011       0.109\n",
            "age_diff             -0.0162      0.030     -0.543      0.587      -0.075       0.042\n",
            "income_diff           0.0667      0.030      2.256      0.024       0.009       0.125\n",
            "date_diff            -0.0028      0.023     -0.122      0.903      -0.047       0.041\n",
            "go_out_diff          -0.0034      0.023     -0.151      0.880      -0.048       0.041\n",
            "sports_diff           0.0617      0.026      2.348      0.019       0.010       0.113\n",
            "exercise_diff         0.0289      0.024      1.230      0.219      -0.017       0.075\n",
            "dining_diff           0.0162      0.025      0.655      0.513      -0.032       0.065\n",
            "art_diff              0.0753      0.027      2.807      0.005       0.023       0.128\n",
            "hiking_diff          -0.1068      0.024     -4.470      0.000      -0.154      -0.060\n",
            "gaming_diff          -0.0101      0.024     -0.414      0.679      -0.058       0.038\n",
            "clubbing_diff        -0.0565      0.022     -2.571      0.010      -0.100      -0.013\n",
            "reading_diff          0.1514      0.022      6.829      0.000       0.108       0.195\n",
            "tv_diff               0.1407      0.026      5.380      0.000       0.089       0.192\n",
            "theater_diff          0.0802      0.030      2.637      0.008       0.021       0.140\n",
            "movies_diff          -0.0209      0.027     -0.787      0.431      -0.073       0.031\n",
            "concerts_diff        -0.0768      0.031     -2.478      0.013      -0.138      -0.016\n",
            "music_diff            0.0635      0.028      2.266      0.024       0.009       0.118\n",
            "shopping_diff         0.0471      0.029      1.626      0.104      -0.010       0.104\n",
            "yoga_diff            -0.0528      0.023     -2.251      0.024      -0.099      -0.007\n",
            "worldrank_diff        0.0653      0.027      2.423      0.015       0.012       0.118\n",
            "(3_1-pf_o)_sinc      -0.0221      0.023     -0.963      0.336      -0.067       0.023\n",
            "(3_1-pf_o)_fun       -0.0269      0.023     -1.178      0.239      -0.072       0.018\n",
            "(3_1-pf_o)_intel      0.0748      0.022      3.331      0.001       0.031       0.119\n",
            "(3_1-pf_o)_amb       -0.1152      0.024     -4.779      0.000      -0.162      -0.068\n",
            "(1_1-2_1_o)_sinc      0.0556      0.033      1.689      0.091      -0.009       0.120\n",
            "(1_1-2_1_o)_fun       0.0584      0.030      1.934      0.053      -0.001       0.118\n",
            "(1_1-2_1_o)_intel    -0.0609      0.033     -1.851      0.064      -0.125       0.004\n",
            "(1_1-2_1_o)_amb       0.0309      0.028      1.098      0.272      -0.024       0.086\n",
            "(1_1-2_1_o)_shar     -0.0816      0.031     -2.661      0.008      -0.142      -0.021\n",
            "from_m               -0.0375      0.020     -1.845      0.065      -0.077       0.002\n",
            "goal_m               -0.0328      0.020     -1.606      0.108      -0.073       0.007\n",
            "imprace_m            -0.0311      0.021     -1.498      0.134      -0.072       0.010\n",
            "imprelig_m           -0.0080      0.021     -0.384      0.701      -0.049       0.033\n",
            "career_c_m            0.0387      0.021      1.844      0.065      -0.002       0.080\n",
            "==============================================================================\n",
            "Omnibus:                      399.268   Durbin-Watson:                   2.047\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              494.699\n",
            "Skew:                          -0.578   Prob(JB):                    3.78e-108\n",
            "Kurtosis:                       3.632   Cond. No.                         5.16\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rytq1XcLNp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88a11810-89e9-4cba-c4d2-21f8939fcf06"
      },
      "source": [
        "#from our newest model, let's remove 'date_diff' which has the largest p-value\n",
        "X_train = X_train.drop(columns=['date_diff'])\n",
        "\n",
        "sin_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sin_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 sinc_o   R-squared:                       0.069\n",
            "Model:                            OLS   Adj. R-squared:                  0.062\n",
            "Method:                 Least Squares   F-statistic:                     9.867\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           1.74e-72\n",
            "Time:                        23:10:44   Log-Likelihood:                -13231.\n",
            "No. Observations:                6849   AIC:                         2.657e+04\n",
            "Df Residuals:                    6797   BIC:                         2.692e+04\n",
            "Df Model:                          51                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.1616      0.020    353.551      0.000       7.122       7.201\n",
            "gender               -0.1882      0.034     -5.585      0.000      -0.254      -0.122\n",
            "condtn               -0.0160      0.022     -0.723      0.470      -0.059       0.027\n",
            "order                -0.2119      0.021     -9.883      0.000      -0.254      -0.170\n",
            "int_corr              0.0250      0.021      1.192      0.233      -0.016       0.066\n",
            "samerace             -0.0117      0.021     -0.570      0.569      -0.052       0.029\n",
            "age_o                 0.0607      0.029      2.061      0.039       0.003       0.118\n",
            "mn_sat_o             -0.0178      0.027     -0.668      0.504      -0.070       0.034\n",
            "tuition_o             0.0086      0.027      0.322      0.747      -0.044       0.061\n",
            "income                0.0309      0.029      1.058      0.290      -0.026       0.088\n",
            "exphappy_o            0.0916      0.022      4.164      0.000       0.048       0.135\n",
            "met_o                -0.0681      0.021     -3.302      0.001      -0.109      -0.028\n",
            "world_rank_o         -0.0741      0.028     -2.683      0.007      -0.128      -0.020\n",
            "masters_o            -0.0577      0.021     -2.718      0.007      -0.099      -0.016\n",
            "sinc1_1              -0.0166      0.032     -0.512      0.609      -0.080       0.047\n",
            "intel1_1              0.0233      0.033      0.701      0.483      -0.042       0.089\n",
            "fun1_1               -0.0147      0.030     -0.492      0.623      -0.073       0.044\n",
            "amb1_1               -0.0145      0.030     -0.486      0.627      -0.073       0.044\n",
            "shar1_1               0.0488      0.031      1.594      0.111      -0.011       0.109\n",
            "age_diff             -0.0161      0.030     -0.541      0.588      -0.075       0.042\n",
            "income_diff           0.0666      0.030      2.255      0.024       0.009       0.125\n",
            "go_out_diff          -0.0043      0.022     -0.196      0.844      -0.047       0.038\n",
            "sports_diff           0.0618      0.026      2.355      0.019       0.010       0.113\n",
            "exercise_diff         0.0291      0.024      1.236      0.216      -0.017       0.075\n",
            "dining_diff           0.0163      0.025      0.659      0.510      -0.032       0.065\n",
            "art_diff              0.0754      0.027      2.814      0.005       0.023       0.128\n",
            "hiking_diff          -0.1068      0.024     -4.471      0.000      -0.154      -0.060\n",
            "gaming_diff          -0.0099      0.024     -0.407      0.684      -0.058       0.038\n",
            "clubbing_diff        -0.0565      0.022     -2.570      0.010      -0.100      -0.013\n",
            "reading_diff          0.1513      0.022      6.828      0.000       0.108       0.195\n",
            "tv_diff               0.1405      0.026      5.383      0.000       0.089       0.192\n",
            "theater_diff          0.0803      0.030      2.642      0.008       0.021       0.140\n",
            "movies_diff          -0.0209      0.027     -0.789      0.430      -0.073       0.031\n",
            "concerts_diff        -0.0768      0.031     -2.479      0.013      -0.138      -0.016\n",
            "music_diff            0.0635      0.028      2.266      0.024       0.009       0.118\n",
            "shopping_diff         0.0474      0.029      1.644      0.100      -0.009       0.104\n",
            "yoga_diff            -0.0526      0.023     -2.248      0.025      -0.099      -0.007\n",
            "worldrank_diff        0.0653      0.027      2.424      0.015       0.013       0.118\n",
            "(3_1-pf_o)_sinc      -0.0220      0.023     -0.957      0.338      -0.067       0.023\n",
            "(3_1-pf_o)_fun       -0.0268      0.023     -1.175      0.240      -0.071       0.018\n",
            "(3_1-pf_o)_intel      0.0748      0.022      3.329      0.001       0.031       0.119\n",
            "(3_1-pf_o)_amb       -0.1150      0.024     -4.785      0.000      -0.162      -0.068\n",
            "(1_1-2_1_o)_sinc      0.0556      0.033      1.689      0.091      -0.009       0.120\n",
            "(1_1-2_1_o)_fun       0.0583      0.030      1.932      0.053      -0.001       0.117\n",
            "(1_1-2_1_o)_intel    -0.0613      0.033     -1.867      0.062      -0.126       0.003\n",
            "(1_1-2_1_o)_amb       0.0308      0.028      1.095      0.274      -0.024       0.086\n",
            "(1_1-2_1_o)_shar     -0.0814      0.031     -2.659      0.008      -0.141      -0.021\n",
            "from_m               -0.0375      0.020     -1.846      0.065      -0.077       0.002\n",
            "goal_m               -0.0328      0.020     -1.607      0.108      -0.073       0.007\n",
            "imprace_m            -0.0311      0.021     -1.498      0.134      -0.072       0.010\n",
            "imprelig_m           -0.0080      0.021     -0.384      0.701      -0.049       0.033\n",
            "career_c_m            0.0387      0.021      1.847      0.065      -0.002       0.080\n",
            "==============================================================================\n",
            "Omnibus:                      399.260   Durbin-Watson:                   2.047\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              494.750\n",
            "Skew:                          -0.578   Prob(JB):                    3.68e-108\n",
            "Kurtosis:                       3.632   Cond. No.                         5.14\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfUJb2j6NxYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2042a9-93f6-497d-a1e6-c3858db34ea8"
      },
      "source": [
        "#from our newest model, let's remove 'tuition_o' which has the largest p-value\n",
        "X_train = X_train.drop(columns=['tuition_o'])\n",
        "\n",
        "sin_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sin_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 sinc_o   R-squared:                       0.069\n",
            "Model:                            OLS   Adj. R-squared:                  0.062\n",
            "Method:                 Least Squares   F-statistic:                     10.06\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           5.71e-73\n",
            "Time:                        23:10:45   Log-Likelihood:                -13231.\n",
            "No. Observations:                6849   AIC:                         2.656e+04\n",
            "Df Residuals:                    6798   BIC:                         2.691e+04\n",
            "Df Model:                          50                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.1616      0.020    353.575      0.000       7.122       7.201\n",
            "gender               -0.1880      0.034     -5.578      0.000      -0.254      -0.122\n",
            "condtn               -0.0157      0.022     -0.713      0.476      -0.059       0.028\n",
            "order                -0.2118      0.021     -9.883      0.000      -0.254      -0.170\n",
            "int_corr              0.0249      0.021      1.192      0.233      -0.016       0.066\n",
            "samerace             -0.0115      0.021     -0.557      0.577      -0.052       0.029\n",
            "age_o                 0.0607      0.029      2.061      0.039       0.003       0.118\n",
            "mn_sat_o             -0.0127      0.021     -0.594      0.553      -0.054       0.029\n",
            "income                0.0312      0.029      1.069      0.285      -0.026       0.088\n",
            "exphappy_o            0.0919      0.022      4.182      0.000       0.049       0.135\n",
            "met_o                -0.0681      0.021     -3.303      0.001      -0.109      -0.028\n",
            "world_rank_o         -0.0751      0.027     -2.735      0.006      -0.129      -0.021\n",
            "masters_o            -0.0577      0.021     -2.717      0.007      -0.099      -0.016\n",
            "sinc1_1              -0.0165      0.032     -0.509      0.611      -0.080       0.047\n",
            "intel1_1              0.0234      0.033      0.704      0.481      -0.042       0.089\n",
            "fun1_1               -0.0149      0.030     -0.498      0.618      -0.074       0.044\n",
            "amb1_1               -0.0143      0.030     -0.480      0.631      -0.073       0.044\n",
            "shar1_1               0.0484      0.031      1.584      0.113      -0.012       0.108\n",
            "age_diff             -0.0163      0.030     -0.548      0.584      -0.075       0.042\n",
            "income_diff           0.0669      0.030      2.266      0.023       0.009       0.125\n",
            "go_out_diff          -0.0046      0.022     -0.214      0.831      -0.047       0.038\n",
            "sports_diff           0.0614      0.026      2.340      0.019       0.010       0.113\n",
            "exercise_diff         0.0291      0.024      1.236      0.216      -0.017       0.075\n",
            "dining_diff           0.0159      0.025      0.642      0.521      -0.033       0.064\n",
            "art_diff              0.0754      0.027      2.814      0.005       0.023       0.128\n",
            "hiking_diff          -0.1069      0.024     -4.472      0.000      -0.154      -0.060\n",
            "gaming_diff          -0.0097      0.024     -0.400      0.689      -0.057       0.038\n",
            "clubbing_diff        -0.0566      0.022     -2.576      0.010      -0.100      -0.014\n",
            "reading_diff          0.1510      0.022      6.821      0.000       0.108       0.194\n",
            "tv_diff               0.1401      0.026      5.375      0.000       0.089       0.191\n",
            "theater_diff          0.0799      0.030      2.631      0.009       0.020       0.139\n",
            "movies_diff          -0.0201      0.026     -0.763      0.446      -0.072       0.032\n",
            "concerts_diff        -0.0766      0.031     -2.472      0.013      -0.137      -0.016\n",
            "music_diff            0.0629      0.028      2.251      0.024       0.008       0.118\n",
            "shopping_diff         0.0475      0.029      1.648      0.099      -0.009       0.104\n",
            "yoga_diff            -0.0522      0.023     -2.234      0.026      -0.098      -0.006\n",
            "worldrank_diff        0.0653      0.027      2.422      0.015       0.012       0.118\n",
            "(3_1-pf_o)_sinc      -0.0225      0.023     -0.983      0.326      -0.067       0.022\n",
            "(3_1-pf_o)_fun       -0.0268      0.023     -1.178      0.239      -0.072       0.018\n",
            "(3_1-pf_o)_intel      0.0748      0.022      3.332      0.001       0.031       0.119\n",
            "(3_1-pf_o)_amb       -0.1145      0.024     -4.774      0.000      -0.161      -0.067\n",
            "(1_1-2_1_o)_sinc      0.0556      0.033      1.687      0.092      -0.009       0.120\n",
            "(1_1-2_1_o)_fun       0.0587      0.030      1.948      0.051      -0.000       0.118\n",
            "(1_1-2_1_o)_intel    -0.0612      0.033     -1.867      0.062      -0.126       0.003\n",
            "(1_1-2_1_o)_amb       0.0308      0.028      1.096      0.273      -0.024       0.086\n",
            "(1_1-2_1_o)_shar     -0.0808      0.031     -2.644      0.008      -0.141      -0.021\n",
            "from_m               -0.0374      0.020     -1.842      0.066      -0.077       0.002\n",
            "goal_m               -0.0328      0.020     -1.603      0.109      -0.073       0.007\n",
            "imprace_m            -0.0313      0.021     -1.503      0.133      -0.072       0.009\n",
            "imprelig_m           -0.0081      0.021     -0.390      0.697      -0.049       0.033\n",
            "career_c_m            0.0382      0.021      1.826      0.068      -0.003       0.079\n",
            "==============================================================================\n",
            "Omnibus:                      400.141   Durbin-Watson:                   2.047\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              496.186\n",
            "Skew:                          -0.578   Prob(JB):                    1.80e-108\n",
            "Kurtosis:                       3.634   Cond. No.                         5.13\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA9InzO3On-j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f772bed1-5d18-457a-efc6-59d1dadc7089"
      },
      "source": [
        "#from our newest model, let's remove 'imprelig_m' which has the largest p-value\n",
        "X_train = X_train.drop(columns=['imprelig_m'])\n",
        "\n",
        "sin_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sin_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 sinc_o   R-squared:                       0.069\n",
            "Model:                            OLS   Adj. R-squared:                  0.062\n",
            "Method:                 Least Squares   F-statistic:                     10.27\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           1.89e-73\n",
            "Time:                        23:10:45   Log-Likelihood:                -13231.\n",
            "No. Observations:                6849   AIC:                         2.656e+04\n",
            "Df Residuals:                    6799   BIC:                         2.690e+04\n",
            "Df Model:                          49                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.1616      0.020    353.597      0.000       7.122       7.201\n",
            "gender               -0.1880      0.034     -5.579      0.000      -0.254      -0.122\n",
            "condtn               -0.0151      0.022     -0.685      0.493      -0.058       0.028\n",
            "order                -0.2119      0.021     -9.889      0.000      -0.254      -0.170\n",
            "int_corr              0.0246      0.021      1.178      0.239      -0.016       0.066\n",
            "samerace             -0.0114      0.021     -0.555      0.579      -0.052       0.029\n",
            "age_o                 0.0605      0.029      2.055      0.040       0.003       0.118\n",
            "mn_sat_o             -0.0130      0.021     -0.608      0.543      -0.055       0.029\n",
            "income                0.0310      0.029      1.065      0.287      -0.026       0.088\n",
            "exphappy_o            0.0919      0.022      4.180      0.000       0.049       0.135\n",
            "met_o                -0.0681      0.021     -3.301      0.001      -0.109      -0.028\n",
            "world_rank_o         -0.0759      0.027     -2.775      0.006      -0.130      -0.022\n",
            "masters_o            -0.0577      0.021     -2.722      0.007      -0.099      -0.016\n",
            "sinc1_1              -0.0169      0.032     -0.519      0.603      -0.080       0.047\n",
            "intel1_1              0.0239      0.033      0.719      0.472      -0.041       0.089\n",
            "fun1_1               -0.0150      0.030     -0.503      0.615      -0.074       0.044\n",
            "amb1_1               -0.0140      0.030     -0.470      0.638      -0.072       0.044\n",
            "shar1_1               0.0484      0.031      1.584      0.113      -0.012       0.108\n",
            "age_diff             -0.0162      0.030     -0.543      0.587      -0.075       0.042\n",
            "income_diff           0.0668      0.030      2.261      0.024       0.009       0.125\n",
            "go_out_diff          -0.0047      0.022     -0.217      0.828      -0.047       0.038\n",
            "sports_diff           0.0612      0.026      2.336      0.020       0.010       0.113\n",
            "exercise_diff         0.0291      0.024      1.239      0.215      -0.017       0.075\n",
            "dining_diff           0.0158      0.025      0.640      0.522      -0.033       0.064\n",
            "art_diff              0.0753      0.027      2.810      0.005       0.023       0.128\n",
            "hiking_diff          -0.1068      0.024     -4.469      0.000      -0.154      -0.060\n",
            "gaming_diff          -0.0097      0.024     -0.401      0.689      -0.057       0.038\n",
            "clubbing_diff        -0.0565      0.022     -2.573      0.010      -0.100      -0.013\n",
            "reading_diff          0.1511      0.022      6.826      0.000       0.108       0.194\n",
            "tv_diff               0.1401      0.026      5.377      0.000       0.089       0.191\n",
            "theater_diff          0.0799      0.030      2.631      0.009       0.020       0.139\n",
            "movies_diff          -0.0201      0.026     -0.763      0.446      -0.072       0.032\n",
            "concerts_diff        -0.0766      0.031     -2.473      0.013      -0.137      -0.016\n",
            "music_diff            0.0630      0.028      2.254      0.024       0.008       0.118\n",
            "shopping_diff         0.0475      0.029      1.648      0.099      -0.009       0.104\n",
            "yoga_diff            -0.0522      0.023     -2.233      0.026      -0.098      -0.006\n",
            "worldrank_diff        0.0658      0.027      2.446      0.014       0.013       0.119\n",
            "(3_1-pf_o)_sinc      -0.0224      0.023     -0.978      0.328      -0.067       0.022\n",
            "(3_1-pf_o)_fun       -0.0265      0.023     -1.163      0.245      -0.071       0.018\n",
            "(3_1-pf_o)_intel      0.0748      0.022      3.331      0.001       0.031       0.119\n",
            "(3_1-pf_o)_amb       -0.1146      0.024     -4.778      0.000      -0.162      -0.068\n",
            "(1_1-2_1_o)_sinc      0.0558      0.033      1.694      0.090      -0.009       0.120\n",
            "(1_1-2_1_o)_fun       0.0588      0.030      1.951      0.051      -0.000       0.118\n",
            "(1_1-2_1_o)_intel    -0.0615      0.033     -1.876      0.061      -0.126       0.003\n",
            "(1_1-2_1_o)_amb       0.0306      0.028      1.091      0.275      -0.024       0.086\n",
            "(1_1-2_1_o)_shar     -0.0806      0.031     -2.639      0.008      -0.140      -0.021\n",
            "from_m               -0.0375      0.020     -1.844      0.065      -0.077       0.002\n",
            "goal_m               -0.0327      0.020     -1.600      0.110      -0.073       0.007\n",
            "imprace_m            -0.0327      0.020     -1.599      0.110      -0.073       0.007\n",
            "career_c_m            0.0380      0.021      1.818      0.069      -0.003       0.079\n",
            "==============================================================================\n",
            "Omnibus:                      400.541   Durbin-Watson:                   2.047\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              496.744\n",
            "Skew:                          -0.578   Prob(JB):                    1.36e-108\n",
            "Kurtosis:                       3.634   Cond. No.                         5.13\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCvPgTXwPCsR"
      },
      "source": [
        "I have removed some variables with super large p-values, and I will use the current model as my final linear model for sin_o attribute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efi0cwuWseoC"
      },
      "source": [
        "## Sincerity (Linear Regression - Bootstrap OSR2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBdoGzh1Pslx"
      },
      "source": [
        "#define the current model as the final model \"sin_lr\"\n",
        "sin_lr = sin_model\n",
        "\n",
        "#remove the variables from test set\n",
        "X_test = X_test.drop(columns = ['(1_1-2_1_o)_att','attr1_1','museums_diff','(3_1-pf_o)_att','tvsport_diff','masters_m','date_diff','tuition_o','imprelig_m'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpm23xyelL5K"
      },
      "source": [
        "# compute out-of-sample R-squared using the test set\n",
        "def sin_OSR2(predictions, y_test,y_train):\n",
        "    SSE = np.sum((y_test-predictions)**2)\n",
        "    SST = np.sum((y_test-np.mean(y_train))**2)\n",
        "    r2 = 1-SSE/SST\n",
        "    return r2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzDOaVneVmIG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10686fa6-7d5a-4059-bc88-143bcade948a"
      },
      "source": [
        "# Check OSR^2 with test set\n",
        "sin_test_OSR2 = sin_OSR2(sin_lr.predict(sm.add_constant(X_test)), y_test, y_train)\n",
        "sin_test_OSR2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05559619437240437"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRLj7SupqpgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a3638d-23ad-4d38-c8c1-5206c1aabbb5"
      },
      "source": [
        "sin_bs_output_lr = bootstrap_validation(sm.add_constant(X_test),y_test,y_train,sin_lr,\n",
        "                                 metrics_list=[sin_OSR2],\n",
        "                                 sample = 5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAdwHrfEthTf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "02323224-a76d-4584-8c91-2f5a559fc755"
      },
      "source": [
        "#bootstrap plots\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap OSR2 Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot OSR2 - Test Set OSR2', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].hist(sin_bs_output_lr.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[0].set_xlim([0.0,0.1])\n",
        "axs[1].hist(sin_bs_output_lr.iloc[:,0]-sin_test_OSR2, bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[1].set_xlim([-0.05,0.05])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.05, 0.05)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The bootstrapped mean of linear regression OSR2 is:', np.mean(sin_bs_output_lr)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB_gPgHy0QSW",
        "outputId": "98611f96-8b86-4644-c923-e9507508e14b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The bootstrapped mean of linear regression OSR2 is: 0.05566328871978967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdLtgS3FyLo5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68dc447-4087-42cd-cdd8-9b3ea8db1a62"
      },
      "source": [
        "# The 95% confidence interval\n",
        "CI_0 = np.quantile(sin_bs_output_lr.iloc[:,0]-sin_test_OSR2,np.array([0.025,0.975]))\n",
        "left = sin_test_OSR2 - CI_0[1]\n",
        "right = sin_test_OSR2 - CI_0[0]\n",
        "print(\"The 95-percent confidence interval of the test set OSR2 for the linear regression sincerity model is:\" ,[left, right])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 95-percent confidence interval of the test set OSR2 for the linear regression sincerity model is: [0.03306246610406736, 0.07828432199782392]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eOvysGJsnX0"
      },
      "source": [
        "## Sincerity (LDA - Train Model + Bootstrap Accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDbyJ6Npz3ID"
      },
      "source": [
        "Let's do LDA on sinc_o next"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79HA4hMkQCho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba238307-76fc-4f9e-dc18-0b70454b9ea9"
      },
      "source": [
        "# Run LDA on Sincerity trait.\n",
        "import sklearn\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# redefine y_train/y_test type to int\n",
        "y_train = sin_y_train.astype(int)\n",
        "y_test = sin_y_test.astype(int)\n",
        "\n",
        "# train\n",
        "sin_lda = LinearDiscriminantAnalysis()\n",
        "sin_lda.fit(sm.add_constant(sin_X_train), y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearDiscriminantAnalysis()"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL70D2zHomOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ea6eb7-040e-4c35-d05c-4aac7663fdc4"
      },
      "source": [
        "# compute sincerity accuracy of LDA\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = sin_lda.predict(sm.add_constant(sin_X_test))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix : \\n\", cm)\n",
        "sin_test_lda_acc = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nTest Set Accuracy:\", sin_test_lda_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix : \n",
            " [[  0   0   0   0   0   0   0   5   1   0   0]\n",
            " [  2   0   1   0   0   0   0   5   1   0   2]\n",
            " [  0   2   2   0   2   0   0   6   7   0   1]\n",
            " [  0   0   2   0   2   0   2   9  10   0   0]\n",
            " [  1   1   1   1   0   0   7  20  24   1   0]\n",
            " [  1   0   0   0   1   3   6  73  58   1   8]\n",
            " [  1   0   0   0   4   3  18 118 104   3   8]\n",
            " [  2   1   0   1   3   3  15 181 166   6  16]\n",
            " [  1   0   5   0   0   5  14 175 192   8  23]\n",
            " [  1   0   1   1   0   0   7  69 113   2  11]\n",
            " [  0   1   1   0   0   1   6  55  67   7  25]]\n",
            "\n",
            "Test Set Accuracy: 0.2469352014010508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEG13kqD1PeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5213382a-7ba9-475b-d400-33c258690cc0"
      },
      "source": [
        "sin_bs_output_lda_acc = bootstrap_validation(sm.add_constant(sin_X_test),y_test,y_train,sin_lda,\n",
        "                                 metrics_list=[acc],\n",
        "                                 sample = 5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm0tJuh24rE5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "ad8c68d5-f598-4447-f6bf-74df7d315740"
      },
      "source": [
        "#bootstrap accuracy plots for sincerity\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap Accuracy Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot Acc - Test Set Acc', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].hist(sin_bs_output_lda_acc.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[0].set_xlim([0.2,0.3])\n",
        "axs[1].hist(sin_bs_output_lda_acc.iloc[:,0]-sin_test_lda_acc, bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[1].set_xlim([-0.05,0.05])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.05, 0.05)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The bootstrapped mean of LDA accuracy is:', np.mean(sin_bs_output_lda_acc)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBvAVrc84fHC",
        "outputId": "6632fe4e-5fa7-4c48-f10b-b48d149e0ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The bootstrapped mean of LDA accuracy is: 0.24708768242848905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoMGkEXe5jkJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aefe47b2-fdd7-4829-8bab-b0bdf8413f23"
      },
      "source": [
        "# The 95% confidence interval\n",
        "CI_0 = np.quantile(sin_bs_output_lda_acc.iloc[:,0]-sin_test_lda_acc,np.array([0.025,0.975]))\n",
        "left = sin_test_lda_acc - CI_0[1]\n",
        "right = sin_test_lda_acc - CI_0[0]\n",
        "print(\"The 95-percent confidence interval of the test set accuuracy for the LDA sincerity model is:\",[left, right])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 95-percent confidence interval of the test set accuuracy for the LDA sincerity model is: [0.2265032107413894, 0.26678342089900764]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sincerity (Rescaled OSR^2 & Baseline Comparisons)"
      ],
      "metadata": {
        "id": "O26v_H0JFtsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_sin = 1 - (np.sum((sin_y_test - sin_y_train.value_counts().index[0])**2)/np.sum((sin_y_test - np.mean(sin_y_train))**2))\n",
        "rm1_lr_sin = rescaled_OSR2_1(sin_lr, sin_y_train, sin_test.drop(columns = ['(1_1-2_1_o)_att','attr1_1','museums_diff','(3_1-pf_o)_att','tvsport_diff','masters_m','date_diff','tuition_o','imprelig_m']), 'sinc_o', 'lr')\n",
        "rm2_lr_sin = rescaled_OSR2_2(sin_lr, sin_y_train, sin_test.drop(columns = ['(1_1-2_1_o)_att','attr1_1','museums_diff','(3_1-pf_o)_att','tvsport_diff','masters_m','date_diff','tuition_o','imprelig_m']), 'sinc_o', 'lr')\n",
        "rm1_lda_sin = rescaled_OSR2_1(sin_lda, sin_y_train, sin_test, 'sinc_o', 'lda')\n",
        "rm2_lda_sin = rescaled_OSR2_2(sin_lda, sin_y_train, sin_test, 'sinc_o', 'lda')\n",
        " \n",
        "print('Models for Sincerity:')\n",
        "print('No Rescale Bootstrapped Mean, Linear Regression OSR2 for Sincerity:', np.mean(sin_bs_output_lr)[0])\n",
        "print('No Rescale Bootstrapped Mean, LDA Accuracy for Sincerity:', np.mean(sin_bs_output_lda_acc)[0])\n",
        "print()\n",
        "print('Rescaling Linear Regression Model before OSR2 Calculation:')\n",
        "print('Rescale Method 1, Linear Regression Sincerity OSR2:', rm1_lr_sin)\n",
        "print('Rescale Method 2, Linear Regression Sincerity OSR2:', rm2_lr_sin)\n",
        "print()\n",
        "print('BASELINE:')\n",
        "print('Baseline OSR2:', base_sin)\n",
        "print()\n",
        "print('SINCERITY MODEL COMPARISONS:')\n",
        "print('OSR2 difference between LR model and baseline:', np.amax(np.array([rm1_lr_sin, rm2_lr_sin, rm1_lda_sin, rm2_lda_sin, np.mean(sin_bs_output_lr)[0]])) - base_sin)\n",
        "print('Accuracy difference for LDA and baseline:', np.mean(sin_bs_output_lda_acc)[0] - sin_y_train.value_counts().values[0]/np.sum(sin_y_train.value_counts()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3wGHVNzFsA2",
        "outputId": "17885a68-099e-4ec6-eaa8-1cec96b9c61f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models for Sincerity:\n",
            "No Rescale Bootstrapped Mean, Linear Regression OSR2 for Sincerity: 0.05566328871978967\n",
            "No Rescale Bootstrapped Mean, LDA Accuracy for Sincerity: 0.24708768242848905\n",
            "\n",
            "Rescaling Linear Regression Model before OSR2 Calculation:\n",
            "Rescale Method 1, Linear Regression Sincerity OSR2: -7.85916896679848\n",
            "Rescale Method 2, Linear Regression Sincerity OSR2: -5.948579171435916\n",
            "\n",
            "BASELINE:\n",
            "Baseline OSR2: -0.008430644390108988\n",
            "\n",
            "SINCERITY MODEL COMPARISONS:\n",
            "OSR2 difference between LR model and baseline: 0.06409393310989867\n",
            "Accuracy difference for LDA and baseline: 0.00515455350455854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bMcyJsFr-HO"
      },
      "source": [
        "## Intelligence (Linear Regression + LDA)\n",
        "Make linear model for this trait, perform feature selection.\n",
        "Bootstrap your model and report the mean and 95% CI for OSR^2.\n",
        "\n",
        "Run LDA on this trait. Bootstrap this model and report the mean and 95% CI for accuracy. Calculate OSR^2 as well.\n",
        "\n",
        "Calculate rescaled OSR^2 using the two methods above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX2WZemVtiyu"
      },
      "source": [
        "## Intelligence (Linear Regression - Feature Selection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxoWLOPiLJ_z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "2d4e65e2-411d-485a-e4cb-83247c0622af"
      },
      "source": [
        "inte_train = inte_X_train.copy()\n",
        "inte_train['intel_o'] = inte_y_train\n",
        "inte_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e5ddc8e1-07ce-4161-b7fb-d6c43c29fd16\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>condtn</th>\n",
              "      <th>order</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>mn_sat_o</th>\n",
              "      <th>tuition_o</th>\n",
              "      <th>income</th>\n",
              "      <th>exphappy_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>world_rank_o</th>\n",
              "      <th>masters_o</th>\n",
              "      <th>attr1_1</th>\n",
              "      <th>sinc1_1</th>\n",
              "      <th>intel1_1</th>\n",
              "      <th>fun1_1</th>\n",
              "      <th>amb1_1</th>\n",
              "      <th>shar1_1</th>\n",
              "      <th>age_diff</th>\n",
              "      <th>income_diff</th>\n",
              "      <th>date_diff</th>\n",
              "      <th>go_out_diff</th>\n",
              "      <th>sports_diff</th>\n",
              "      <th>tvsport_diff</th>\n",
              "      <th>exercise_diff</th>\n",
              "      <th>dining_diff</th>\n",
              "      <th>museums_diff</th>\n",
              "      <th>art_diff</th>\n",
              "      <th>hiking_diff</th>\n",
              "      <th>gaming_diff</th>\n",
              "      <th>clubbing_diff</th>\n",
              "      <th>reading_diff</th>\n",
              "      <th>tv_diff</th>\n",
              "      <th>theater_diff</th>\n",
              "      <th>movies_diff</th>\n",
              "      <th>concerts_diff</th>\n",
              "      <th>music_diff</th>\n",
              "      <th>shopping_diff</th>\n",
              "      <th>yoga_diff</th>\n",
              "      <th>worldrank_diff</th>\n",
              "      <th>(3_1-pf_o)_att</th>\n",
              "      <th>(3_1-pf_o)_sinc</th>\n",
              "      <th>(3_1-pf_o)_fun</th>\n",
              "      <th>(3_1-pf_o)_intel</th>\n",
              "      <th>(3_1-pf_o)_amb</th>\n",
              "      <th>(1_1-2_1_o)_att</th>\n",
              "      <th>(1_1-2_1_o)_sinc</th>\n",
              "      <th>(1_1-2_1_o)_fun</th>\n",
              "      <th>(1_1-2_1_o)_intel</th>\n",
              "      <th>(1_1-2_1_o)_amb</th>\n",
              "      <th>(1_1-2_1_o)_shar</th>\n",
              "      <th>from_m</th>\n",
              "      <th>goal_m</th>\n",
              "      <th>imprace_m</th>\n",
              "      <th>imprelig_m</th>\n",
              "      <th>career_c_m</th>\n",
              "      <th>masters_m</th>\n",
              "      <th>intel_o</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>820</th>\n",
              "      <td>-0.984979</td>\n",
              "      <td>-2.276573</td>\n",
              "      <td>-0.328868</td>\n",
              "      <td>1.600201</td>\n",
              "      <td>-0.819524</td>\n",
              "      <td>0.166126</td>\n",
              "      <td>0.081581</td>\n",
              "      <td>0.095712</td>\n",
              "      <td>-0.091583</td>\n",
              "      <td>-0.309392</td>\n",
              "      <td>0.189693</td>\n",
              "      <td>-0.651660</td>\n",
              "      <td>2.526776</td>\n",
              "      <td>-1.024834</td>\n",
              "      <td>0.343429</td>\n",
              "      <td>-0.048345</td>\n",
              "      <td>0.427405</td>\n",
              "      <td>0.711414</td>\n",
              "      <td>0.487724</td>\n",
              "      <td>1.028135</td>\n",
              "      <td>0.598846</td>\n",
              "      <td>-0.482231</td>\n",
              "      <td>0.630148</td>\n",
              "      <td>-0.250311</td>\n",
              "      <td>-0.484409</td>\n",
              "      <td>-0.861744</td>\n",
              "      <td>-0.792903</td>\n",
              "      <td>-0.348690</td>\n",
              "      <td>-0.306966</td>\n",
              "      <td>-1.672676</td>\n",
              "      <td>0.514720</td>\n",
              "      <td>0.580088</td>\n",
              "      <td>0.350358</td>\n",
              "      <td>-0.547450</td>\n",
              "      <td>-1.216970</td>\n",
              "      <td>0.001336</td>\n",
              "      <td>-0.645696</td>\n",
              "      <td>-0.787261</td>\n",
              "      <td>-0.514229</td>\n",
              "      <td>-2.050035</td>\n",
              "      <td>-0.191847</td>\n",
              "      <td>0.032483</td>\n",
              "      <td>0.305499</td>\n",
              "      <td>-0.041020</td>\n",
              "      <td>-0.164303</td>\n",
              "      <td>0.797727</td>\n",
              "      <td>0.006708</td>\n",
              "      <td>0.069671</td>\n",
              "      <td>0.781754</td>\n",
              "      <td>-0.309568</td>\n",
              "      <td>-0.482420</td>\n",
              "      <td>-0.115216</td>\n",
              "      <td>-0.096417</td>\n",
              "      <td>-0.673649</td>\n",
              "      <td>-0.446043</td>\n",
              "      <td>-0.449572</td>\n",
              "      <td>-0.485342</td>\n",
              "      <td>-1.839424</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4086</th>\n",
              "      <td>-0.984979</td>\n",
              "      <td>0.439192</td>\n",
              "      <td>1.680916</td>\n",
              "      <td>-0.054252</td>\n",
              "      <td>-0.819524</td>\n",
              "      <td>-0.388491</td>\n",
              "      <td>0.081581</td>\n",
              "      <td>-2.602266</td>\n",
              "      <td>-0.091583</td>\n",
              "      <td>0.844333</td>\n",
              "      <td>0.189693</td>\n",
              "      <td>0.915164</td>\n",
              "      <td>-0.395703</td>\n",
              "      <td>-0.509266</td>\n",
              "      <td>-0.229582</td>\n",
              "      <td>-0.193132</td>\n",
              "      <td>-0.082893</td>\n",
              "      <td>0.543733</td>\n",
              "      <td>0.965297</td>\n",
              "      <td>0.401691</td>\n",
              "      <td>0.046688</td>\n",
              "      <td>-0.482231</td>\n",
              "      <td>0.630148</td>\n",
              "      <td>-0.250311</td>\n",
              "      <td>-0.729227</td>\n",
              "      <td>0.004518</td>\n",
              "      <td>-1.598408</td>\n",
              "      <td>-2.043085</td>\n",
              "      <td>-1.234100</td>\n",
              "      <td>-0.842046</td>\n",
              "      <td>-0.277041</td>\n",
              "      <td>-0.563039</td>\n",
              "      <td>-0.003362</td>\n",
              "      <td>1.108214</td>\n",
              "      <td>-1.216970</td>\n",
              "      <td>1.247151</td>\n",
              "      <td>-1.626429</td>\n",
              "      <td>-2.001806</td>\n",
              "      <td>-1.281979</td>\n",
              "      <td>-0.776930</td>\n",
              "      <td>1.919175</td>\n",
              "      <td>-0.548965</td>\n",
              "      <td>-0.242844</td>\n",
              "      <td>1.088041</td>\n",
              "      <td>-1.184719</td>\n",
              "      <td>-0.159674</td>\n",
              "      <td>-0.919548</td>\n",
              "      <td>0.896297</td>\n",
              "      <td>1.116715</td>\n",
              "      <td>-1.289000</td>\n",
              "      <td>0.249930</td>\n",
              "      <td>0.816682</td>\n",
              "      <td>-0.096417</td>\n",
              "      <td>-0.673649</td>\n",
              "      <td>-0.446043</td>\n",
              "      <td>-0.449572</td>\n",
              "      <td>2.060100</td>\n",
              "      <td>0.543569</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2359</th>\n",
              "      <td>1.015102</td>\n",
              "      <td>0.439192</td>\n",
              "      <td>0.401962</td>\n",
              "      <td>-1.046923</td>\n",
              "      <td>1.220042</td>\n",
              "      <td>0.443434</td>\n",
              "      <td>0.081581</td>\n",
              "      <td>0.095712</td>\n",
              "      <td>-0.091583</td>\n",
              "      <td>-0.886254</td>\n",
              "      <td>0.189693</td>\n",
              "      <td>-0.651660</td>\n",
              "      <td>-0.395703</td>\n",
              "      <td>-0.485206</td>\n",
              "      <td>0.476654</td>\n",
              "      <td>-0.251046</td>\n",
              "      <td>0.189266</td>\n",
              "      <td>0.535349</td>\n",
              "      <td>-0.048749</td>\n",
              "      <td>-0.015938</td>\n",
              "      <td>-0.238760</td>\n",
              "      <td>0.500920</td>\n",
              "      <td>-1.317538</td>\n",
              "      <td>1.574164</td>\n",
              "      <td>1.474133</td>\n",
              "      <td>0.004518</td>\n",
              "      <td>0.012603</td>\n",
              "      <td>-0.687569</td>\n",
              "      <td>-0.616010</td>\n",
              "      <td>1.096089</td>\n",
              "      <td>-0.277041</td>\n",
              "      <td>2.294779</td>\n",
              "      <td>-0.710802</td>\n",
              "      <td>-1.099338</td>\n",
              "      <td>-0.309682</td>\n",
              "      <td>-0.413936</td>\n",
              "      <td>-0.318785</td>\n",
              "      <td>-0.787261</td>\n",
              "      <td>0.509439</td>\n",
              "      <td>-0.522310</td>\n",
              "      <td>-0.191847</td>\n",
              "      <td>0.672907</td>\n",
              "      <td>0.123175</td>\n",
              "      <td>-0.423288</td>\n",
              "      <td>0.662235</td>\n",
              "      <td>-1.176115</td>\n",
              "      <td>0.065756</td>\n",
              "      <td>0.033506</td>\n",
              "      <td>0.370868</td>\n",
              "      <td>-0.130006</td>\n",
              "      <td>0.127872</td>\n",
              "      <td>-0.540394</td>\n",
              "      <td>-0.096417</td>\n",
              "      <td>1.484236</td>\n",
              "      <td>-0.446043</td>\n",
              "      <td>-0.449572</td>\n",
              "      <td>-0.485342</td>\n",
              "      <td>0.543569</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8359</th>\n",
              "      <td>-0.984979</td>\n",
              "      <td>0.439192</td>\n",
              "      <td>1.498208</td>\n",
              "      <td>-0.054252</td>\n",
              "      <td>1.220042</td>\n",
              "      <td>0.998051</td>\n",
              "      <td>0.081581</td>\n",
              "      <td>0.095712</td>\n",
              "      <td>0.402106</td>\n",
              "      <td>1.421195</td>\n",
              "      <td>0.189693</td>\n",
              "      <td>0.915164</td>\n",
              "      <td>-0.395703</td>\n",
              "      <td>-0.165554</td>\n",
              "      <td>0.343429</td>\n",
              "      <td>-0.048345</td>\n",
              "      <td>-0.423092</td>\n",
              "      <td>-0.965395</td>\n",
              "      <td>1.283679</td>\n",
              "      <td>1.445764</td>\n",
              "      <td>-0.337857</td>\n",
              "      <td>0.009344</td>\n",
              "      <td>-1.317538</td>\n",
              "      <td>1.313524</td>\n",
              "      <td>0.494862</td>\n",
              "      <td>0.293271</td>\n",
              "      <td>1.220861</td>\n",
              "      <td>-1.365327</td>\n",
              "      <td>-2.161233</td>\n",
              "      <td>0.819213</td>\n",
              "      <td>-0.277041</td>\n",
              "      <td>0.008525</td>\n",
              "      <td>-2.125684</td>\n",
              "      <td>-0.547450</td>\n",
              "      <td>-2.124258</td>\n",
              "      <td>-0.829208</td>\n",
              "      <td>-1.953340</td>\n",
              "      <td>-0.382413</td>\n",
              "      <td>1.021273</td>\n",
              "      <td>-0.267689</td>\n",
              "      <td>1.982549</td>\n",
              "      <td>-0.964285</td>\n",
              "      <td>1.265100</td>\n",
              "      <td>-1.170081</td>\n",
              "      <td>0.127245</td>\n",
              "      <td>0.797727</td>\n",
              "      <td>-0.109074</td>\n",
              "      <td>0.586312</td>\n",
              "      <td>-1.562974</td>\n",
              "      <td>0.452212</td>\n",
              "      <td>-0.482420</td>\n",
              "      <td>1.166144</td>\n",
              "      <td>-0.096417</td>\n",
              "      <td>1.484236</td>\n",
              "      <td>-0.446043</td>\n",
              "      <td>-0.449572</td>\n",
              "      <td>-0.485342</td>\n",
              "      <td>0.543569</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3800</th>\n",
              "      <td>-0.984979</td>\n",
              "      <td>0.439192</td>\n",
              "      <td>1.680916</td>\n",
              "      <td>0.640618</td>\n",
              "      <td>-0.819524</td>\n",
              "      <td>-0.665800</td>\n",
              "      <td>-0.002899</td>\n",
              "      <td>-1.413061</td>\n",
              "      <td>2.563324</td>\n",
              "      <td>0.267470</td>\n",
              "      <td>0.189693</td>\n",
              "      <td>0.915164</td>\n",
              "      <td>-0.395703</td>\n",
              "      <td>-0.595194</td>\n",
              "      <td>0.343429</td>\n",
              "      <td>-0.048345</td>\n",
              "      <td>0.427405</td>\n",
              "      <td>0.711414</td>\n",
              "      <td>-0.308230</td>\n",
              "      <td>-0.433567</td>\n",
              "      <td>-1.902352</td>\n",
              "      <td>-1.465383</td>\n",
              "      <td>-0.019081</td>\n",
              "      <td>-0.771590</td>\n",
              "      <td>-0.239591</td>\n",
              "      <td>0.293271</td>\n",
              "      <td>-0.792903</td>\n",
              "      <td>-1.026448</td>\n",
              "      <td>-0.925055</td>\n",
              "      <td>0.265460</td>\n",
              "      <td>-0.013121</td>\n",
              "      <td>-1.991948</td>\n",
              "      <td>0.350358</td>\n",
              "      <td>1.384158</td>\n",
              "      <td>-0.914540</td>\n",
              "      <td>-0.413936</td>\n",
              "      <td>0.008126</td>\n",
              "      <td>0.427284</td>\n",
              "      <td>1.021273</td>\n",
              "      <td>0.241553</td>\n",
              "      <td>1.818650</td>\n",
              "      <td>-0.798157</td>\n",
              "      <td>0.990928</td>\n",
              "      <td>-0.524903</td>\n",
              "      <td>-0.164303</td>\n",
              "      <td>0.159460</td>\n",
              "      <td>0.759290</td>\n",
              "      <td>0.586312</td>\n",
              "      <td>-0.446437</td>\n",
              "      <td>-1.180174</td>\n",
              "      <td>-1.092712</td>\n",
              "      <td>0.583708</td>\n",
              "      <td>-0.096417</td>\n",
              "      <td>-0.673649</td>\n",
              "      <td>-0.446043</td>\n",
              "      <td>-0.449572</td>\n",
              "      <td>2.060100</td>\n",
              "      <td>0.543569</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5ddc8e1-07ce-4161-b7fb-d6c43c29fd16')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e5ddc8e1-07ce-4161-b7fb-d6c43c29fd16 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e5ddc8e1-07ce-4161-b7fb-d6c43c29fd16');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        gender    condtn     order  ...  career_c_m  masters_m  intel_o\n",
              "820  -0.984979 -2.276573 -0.328868  ...   -0.485342  -1.839424      6.0\n",
              "4086 -0.984979  0.439192  1.680916  ...    2.060100   0.543569      9.0\n",
              "2359  1.015102  0.439192  0.401962  ...   -0.485342   0.543569      7.0\n",
              "8359 -0.984979  0.439192  1.498208  ...   -0.485342   0.543569      5.0\n",
              "3800 -0.984979  0.439192  1.680916  ...    2.060100   0.543569      6.0\n",
              "\n",
              "[5 rows x 59 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA_CblJIK2qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73604b2-6c08-49ea-f1ec-fa5c8759ac71"
      },
      "source": [
        "# Fit linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "inte_X_train = sm.add_constant(inte_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "inte_model = sm.OLS(inte_y_train, inte_X_train).fit() #ordinary least square\n",
        "print(inte_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                intel_o   R-squared:                       0.091\n",
            "Model:                            OLS   Adj. R-squared:                  0.083\n",
            "Method:                 Least Squares   F-statistic:                     11.69\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          8.48e-101\n",
            "Time:                        23:11:37   Log-Likelihood:                -12296.\n",
            "No. Observations:                6839   AIC:                         2.471e+04\n",
            "Df Residuals:                    6780   BIC:                         2.511e+04\n",
            "Df Model:                          58                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.3800      0.018    415.984      0.000       7.345       7.415\n",
            "gender                0.0020      0.031      0.066      0.947      -0.058       0.062\n",
            "condtn                0.0120      0.019      0.620      0.535      -0.026       0.050\n",
            "order                -0.1555      0.019     -8.294      0.000      -0.192      -0.119\n",
            "int_corr              0.0775      0.018      4.229      0.000       0.042       0.113\n",
            "samerace              0.0064      0.018      0.355      0.723      -0.029       0.042\n",
            "age_o                 0.0456      0.026      1.778      0.076      -0.005       0.096\n",
            "mn_sat_o             -0.0253      0.024     -1.076      0.282      -0.071       0.021\n",
            "tuition_o            -0.0308      0.024     -1.309      0.191      -0.077       0.015\n",
            "income               -0.0008      0.026     -0.033      0.974      -0.052       0.050\n",
            "exphappy_o            0.1144      0.019      5.931      0.000       0.077       0.152\n",
            "met_o                -0.1459      0.018     -7.997      0.000      -0.182      -0.110\n",
            "world_rank_o         -0.0960      0.024     -3.942      0.000      -0.144      -0.048\n",
            "masters_o            -0.1113      0.023     -4.753      0.000      -0.157      -0.065\n",
            "attr1_1              -0.2535      0.158     -1.605      0.109      -0.563       0.056\n",
            "sinc1_1              -0.2262      0.097     -2.332      0.020      -0.416      -0.036\n",
            "intel1_1             -0.0282      0.097     -0.291      0.771      -0.218       0.162\n",
            "fun1_1               -0.2032      0.082     -2.489      0.013      -0.363      -0.043\n",
            "amb1_1               -0.1199      0.081     -1.477      0.140      -0.279       0.039\n",
            "shar1_1              -0.1061      0.088     -1.207      0.227      -0.278       0.066\n",
            "age_diff             -0.0241      0.026     -0.927      0.354      -0.075       0.027\n",
            "income_diff          -0.0297      0.026     -1.128      0.259      -0.081       0.022\n",
            "date_diff            -0.0215      0.020     -1.080      0.280      -0.060       0.018\n",
            "go_out_diff           0.0353      0.020      1.759      0.079      -0.004       0.075\n",
            "sports_diff           0.0028      0.026      0.111      0.912      -0.047       0.053\n",
            "tvsport_diff          0.0242      0.023      1.032      0.302      -0.022       0.070\n",
            "exercise_diff         0.0416      0.021      2.012      0.044       0.001       0.082\n",
            "dining_diff          -0.0317      0.022     -1.440      0.150      -0.075       0.011\n",
            "museums_diff          0.0240      0.038      0.637      0.524      -0.050       0.098\n",
            "art_diff             -0.0224      0.036     -0.619      0.536      -0.093       0.049\n",
            "hiking_diff          -0.0654      0.021     -3.120      0.002      -0.106      -0.024\n",
            "gaming_diff          -0.0335      0.022     -1.552      0.121      -0.076       0.009\n",
            "clubbing_diff         0.0420      0.019      2.181      0.029       0.004       0.080\n",
            "reading_diff          0.1575      0.020      7.920      0.000       0.119       0.197\n",
            "tv_diff               0.1418      0.024      5.881      0.000       0.095       0.189\n",
            "theater_diff          0.1412      0.027      5.248      0.000       0.088       0.194\n",
            "movies_diff          -0.0444      0.023     -1.894      0.058      -0.090       0.002\n",
            "concerts_diff        -0.0482      0.027     -1.782      0.075      -0.101       0.005\n",
            "music_diff            0.0756      0.024      3.091      0.002       0.028       0.123\n",
            "shopping_diff        -0.0241      0.026     -0.935      0.350      -0.075       0.026\n",
            "yoga_diff            -0.0161      0.021     -0.784      0.433      -0.057       0.024\n",
            "worldrank_diff        0.0740      0.024      3.117      0.002       0.027       0.121\n",
            "(3_1-pf_o)_att       -0.0982      0.034     -2.921      0.003      -0.164      -0.032\n",
            "(3_1-pf_o)_sinc      -0.0879      0.024     -3.599      0.000      -0.136      -0.040\n",
            "(3_1-pf_o)_fun       -0.0520      0.022     -2.331      0.020      -0.096      -0.008\n",
            "(3_1-pf_o)_intel      0.0515      0.024      2.104      0.035       0.004       0.099\n",
            "(3_1-pf_o)_amb       -0.1295      0.025     -5.166      0.000      -0.179      -0.080\n",
            "(1_1-2_1_o)_att       0.2334      0.172      1.360      0.174      -0.103       0.570\n",
            "(1_1-2_1_o)_sinc      0.2342      0.099      2.377      0.018       0.041       0.427\n",
            "(1_1-2_1_o)_fun       0.1935      0.090      2.146      0.032       0.017       0.370\n",
            "(1_1-2_1_o)_intel    -0.0256      0.094     -0.273      0.785      -0.210       0.159\n",
            "(1_1-2_1_o)_amb       0.1100      0.083      1.332      0.183      -0.052       0.272\n",
            "(1_1-2_1_o)_shar      0.1012      0.088      1.151      0.250      -0.071       0.273\n",
            "from_m               -0.0037      0.018     -0.206      0.837      -0.039       0.031\n",
            "goal_m               -0.0339      0.018     -1.888      0.059      -0.069       0.001\n",
            "imprace_m             0.0069      0.018      0.380      0.704      -0.029       0.043\n",
            "imprelig_m            0.0149      0.018      0.813      0.416      -0.021       0.051\n",
            "career_c_m            0.0717      0.018      3.877      0.000       0.035       0.108\n",
            "masters_m            -0.0220      0.023     -0.959      0.337      -0.067       0.023\n",
            "==============================================================================\n",
            "Omnibus:                      403.310   Durbin-Watson:                   2.027\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              538.356\n",
            "Skew:                          -0.547   Prob(JB):                    1.25e-117\n",
            "Kurtosis:                       3.832   Cond. No.                         42.5\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOvGB-PN6zFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c0f0a6-6906-4c11-9a9b-39bfcf0ca1ff"
      },
      "source": [
        "# Check VIF values\n",
        "\n",
        "inte_cols = ['gender', 'condtn', 'order', 'int_corr', 'samerace', 'age_o',\n",
        "       'mn_sat_o', 'tuition_o', 'income', 'exphappy_o', 'met_o',\n",
        "       'world_rank_o', 'masters_o', 'attr1_1', 'sinc1_1', 'intel1_1',\n",
        "       'fun1_1', 'amb1_1', 'shar1_1', 'age_diff', 'income_diff',\n",
        "       'date_diff', 'go_out_diff', 'sports_diff', 'tvsport_diff',\n",
        "       'exercise_diff', 'dining_diff', 'museums_diff', 'art_diff',\n",
        "       'hiking_diff', 'gaming_diff', 'clubbing_diff', 'reading_diff',\n",
        "       'tv_diff', 'theater_diff', 'movies_diff', 'concerts_diff',\n",
        "       'music_diff', 'shopping_diff', 'yoga_diff', 'worldrank_diff',\n",
        "       '(3_1-pf_o)_att', '(3_1-pf_o)_sinc', '(3_1-pf_o)_fun',\n",
        "       '(3_1-pf_o)_intel', '(3_1-pf_o)_amb', '(1_1-2_1_o)_att',\n",
        "       '(1_1-2_1_o)_sinc', '(1_1-2_1_o)_fun', '(1_1-2_1_o)_intel',\n",
        "       '(1_1-2_1_o)_amb', '(1_1-2_1_o)_shar', 'from_m', 'goal_m',\n",
        "       'imprace_m', 'imprelig_m', 'career_c_m', 'masters_m']\n",
        "VIF(inte_X_train, inte_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_att      93.541488\n",
              "attr1_1              79.267005\n",
              "(1_1-2_1_o)_sinc     30.849726\n",
              "sinc1_1              29.877355\n",
              "intel1_1             29.774692\n",
              "(1_1-2_1_o)_intel    28.024515\n",
              "(1_1-2_1_o)_fun      25.820939\n",
              "(1_1-2_1_o)_shar     24.528959\n",
              "shar1_1              24.521743\n",
              "(1_1-2_1_o)_amb      21.663313\n",
              "fun1_1               21.177219\n",
              "amb1_1               20.928744\n",
              "museums_diff          4.528814\n",
              "art_diff              4.163105\n",
              "(3_1-pf_o)_att        3.587513\n",
              "gender                2.992265\n",
              "concerts_diff         2.326093\n",
              "theater_diff          2.299306\n",
              "income_diff           2.196381\n",
              "age_diff              2.139027\n",
              "income                2.121588\n",
              "shopping_diff         2.119148\n",
              "age_o                 2.094273\n",
              "sports_diff           2.092035\n",
              "(3_1-pf_o)_amb        1.996868\n",
              "(3_1-pf_o)_intel      1.900972\n",
              "music_diff            1.897973\n",
              "(3_1-pf_o)_sinc       1.894453\n",
              "world_rank_o          1.886049\n",
              "tv_diff               1.846294\n",
              "worldrank_diff        1.789790\n",
              "tuition_o             1.764028\n",
              "mn_sat_o              1.754915\n",
              "tvsport_diff          1.753171\n",
              "movies_diff           1.741812\n",
              "masters_o             1.740902\n",
              "masters_m             1.669203\n",
              "(3_1-pf_o)_fun        1.577309\n",
              "dining_diff           1.538826\n",
              "gaming_diff           1.477512\n",
              "hiking_diff           1.395008\n",
              "exercise_diff         1.358602\n",
              "yoga_diff             1.346312\n",
              "go_out_diff           1.277616\n",
              "reading_diff          1.256988\n",
              "date_diff             1.255652\n",
              "condtn                1.191382\n",
              "exphappy_o            1.181405\n",
              "clubbing_diff         1.178934\n",
              "order                 1.116638\n",
              "career_c_m            1.087188\n",
              "int_corr              1.067985\n",
              "imprelig_m            1.065917\n",
              "imprace_m             1.060194\n",
              "met_o                 1.056673\n",
              "samerace              1.037542\n",
              "goal_m                1.022389\n",
              "from_m                1.009537\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7kXIyFHEeiQ"
      },
      "source": [
        "# Drop the column with highest VIF\n",
        "\n",
        "inte_X_train = inte_X_train.drop(columns= ['(1_1-2_1_o)_att'])\n",
        "inte_X_test = inte_X_test.drop(columns= ['(1_1-2_1_o)_att'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH_syXkoIMfP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b078a35-49c8-40e4-9565-037be37c522b"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "inte_X_train = sm.add_constant(inte_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "inte_model2 = sm.OLS(inte_y_train, inte_X_train).fit() #ordinary least square\n",
        "print(inte_model2.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                intel_o   R-squared:                       0.091\n",
            "Model:                            OLS   Adj. R-squared:                  0.083\n",
            "Method:                 Least Squares   F-statistic:                     11.86\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          5.72e-101\n",
            "Time:                        23:11:40   Log-Likelihood:                -12297.\n",
            "No. Observations:                6839   AIC:                         2.471e+04\n",
            "Df Residuals:                    6781   BIC:                         2.511e+04\n",
            "Df Model:                          57                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.3800      0.018    415.958      0.000       7.345       7.415\n",
            "gender               -0.0024      0.031     -0.080      0.937      -0.062       0.057\n",
            "condtn                0.0119      0.019      0.614      0.539      -0.026       0.050\n",
            "order                -0.1558      0.019     -8.313      0.000      -0.193      -0.119\n",
            "int_corr              0.0778      0.018      4.243      0.000       0.042       0.114\n",
            "samerace              0.0074      0.018      0.410      0.682      -0.028       0.043\n",
            "age_o                 0.0473      0.026      1.846      0.065      -0.003       0.098\n",
            "mn_sat_o             -0.0239      0.023     -1.016      0.309      -0.070       0.022\n",
            "tuition_o            -0.0326      0.024     -1.386      0.166      -0.079       0.014\n",
            "income                0.0005      0.026      0.021      0.984      -0.050       0.051\n",
            "exphappy_o            0.1139      0.019      5.906      0.000       0.076       0.152\n",
            "met_o                -0.1459      0.018     -8.000      0.000      -0.182      -0.110\n",
            "world_rank_o         -0.0985      0.024     -4.055      0.000      -0.146      -0.051\n",
            "masters_o            -0.1109      0.023     -4.738      0.000      -0.157      -0.065\n",
            "attr1_1              -0.0949      0.107     -0.890      0.373      -0.304       0.114\n",
            "sinc1_1              -0.1329      0.069     -1.938      0.053      -0.267       0.002\n",
            "intel1_1              0.0639      0.069      0.924      0.356      -0.072       0.200\n",
            "fun1_1               -0.1260      0.059     -2.146      0.032      -0.241      -0.011\n",
            "amb1_1               -0.0410      0.057     -0.722      0.470      -0.152       0.070\n",
            "shar1_1              -0.0226      0.063     -0.360      0.719      -0.146       0.101\n",
            "age_diff             -0.0242      0.026     -0.934      0.351      -0.075       0.027\n",
            "income_diff          -0.0279      0.026     -1.062      0.288      -0.079       0.024\n",
            "date_diff            -0.0214      0.020     -1.075      0.282      -0.060       0.018\n",
            "go_out_diff           0.0361      0.020      1.801      0.072      -0.003       0.075\n",
            "sports_diff           0.0035      0.026      0.135      0.893      -0.047       0.054\n",
            "tvsport_diff          0.0228      0.023      0.972      0.331      -0.023       0.069\n",
            "exercise_diff         0.0415      0.021      2.006      0.045       0.001       0.082\n",
            "dining_diff          -0.0311      0.022     -1.412      0.158      -0.074       0.012\n",
            "museums_diff          0.0242      0.038      0.641      0.522      -0.050       0.098\n",
            "art_diff             -0.0241      0.036     -0.666      0.505      -0.095       0.047\n",
            "hiking_diff          -0.0647      0.021     -3.087      0.002      -0.106      -0.024\n",
            "gaming_diff          -0.0320      0.022     -1.484      0.138      -0.074       0.010\n",
            "clubbing_diff         0.0410      0.019      2.129      0.033       0.003       0.079\n",
            "reading_diff          0.1585      0.020      7.974      0.000       0.120       0.197\n",
            "tv_diff               0.1408      0.024      5.841      0.000       0.094       0.188\n",
            "theater_diff          0.1408      0.027      5.232      0.000       0.088       0.193\n",
            "movies_diff          -0.0461      0.023     -1.970      0.049      -0.092      -0.000\n",
            "concerts_diff        -0.0480      0.027     -1.775      0.076      -0.101       0.005\n",
            "music_diff            0.0749      0.024      3.064      0.002       0.027       0.123\n",
            "shopping_diff        -0.0237      0.026     -0.919      0.358      -0.074       0.027\n",
            "yoga_diff            -0.0166      0.021     -0.805      0.421      -0.057       0.024\n",
            "worldrank_diff        0.0741      0.024      3.122      0.002       0.028       0.121\n",
            "(3_1-pf_o)_att       -0.0954      0.034     -2.843      0.004      -0.161      -0.030\n",
            "(3_1-pf_o)_sinc      -0.0881      0.024     -3.608      0.000      -0.136      -0.040\n",
            "(3_1-pf_o)_fun       -0.0525      0.022     -2.355      0.019      -0.096      -0.009\n",
            "(3_1-pf_o)_intel      0.0507      0.024      2.075      0.038       0.003       0.099\n",
            "(3_1-pf_o)_amb       -0.1314      0.025     -5.250      0.000      -0.181      -0.082\n",
            "(1_1-2_1_o)_sinc      0.1064      0.030      3.587      0.000       0.048       0.164\n",
            "(1_1-2_1_o)_fun       0.0765      0.027      2.833      0.005       0.024       0.129\n",
            "(1_1-2_1_o)_intel    -0.1470      0.029     -5.010      0.000      -0.205      -0.089\n",
            "(1_1-2_1_o)_amb       0.0030      0.025      0.119      0.906      -0.046       0.052\n",
            "(1_1-2_1_o)_shar     -0.0120      0.028     -0.420      0.674      -0.068       0.044\n",
            "from_m               -0.0037      0.018     -0.207      0.836      -0.039       0.031\n",
            "goal_m               -0.0345      0.018     -1.924      0.054      -0.070       0.001\n",
            "imprace_m             0.0062      0.018      0.341      0.733      -0.030       0.042\n",
            "imprelig_m            0.0153      0.018      0.837      0.403      -0.021       0.051\n",
            "career_c_m            0.0721      0.018      3.896      0.000       0.036       0.108\n",
            "masters_m            -0.0208      0.023     -0.907      0.364      -0.066       0.024\n",
            "==============================================================================\n",
            "Omnibus:                      401.346   Durbin-Watson:                   2.027\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              534.404\n",
            "Skew:                          -0.546   Prob(JB):                    9.03e-117\n",
            "Kurtosis:                       3.826   Cond. No.                         21.3\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQXF36QU0tKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a7a2d3f-7e58-4323-8619-36b080ed9efe"
      },
      "source": [
        "# Check VIF values again\n",
        "\n",
        "inte_cols.remove('(1_1-2_1_o)_att')\n",
        "VIF(inte_X_train, inte_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "attr1_1              36.121562\n",
              "intel1_1             15.213376\n",
              "sinc1_1              14.923888\n",
              "shar1_1              12.572099\n",
              "fun1_1               10.942565\n",
              "amb1_1               10.256794\n",
              "museums_diff          4.528775\n",
              "art_diff              4.158113\n",
              "(3_1-pf_o)_att        3.573906\n",
              "gender                2.958109\n",
              "(1_1-2_1_o)_sinc      2.792293\n",
              "(1_1-2_1_o)_intel     2.734015\n",
              "(1_1-2_1_o)_shar      2.567960\n",
              "concerts_diff         2.326023\n",
              "(1_1-2_1_o)_fun       2.314390\n",
              "theater_diff          2.298966\n",
              "income_diff           2.191026\n",
              "age_diff              2.138976\n",
              "shopping_diff         2.118851\n",
              "income                2.118341\n",
              "sports_diff           2.091390\n",
              "age_o                 2.089319\n",
              "(3_1-pf_o)_amb        1.990651\n",
              "(1_1-2_1_o)_amb       1.990249\n",
              "(3_1-pf_o)_intel      1.900062\n",
              "music_diff            1.897190\n",
              "(3_1-pf_o)_sinc       1.894370\n",
              "world_rank_o          1.875425\n",
              "tv_diff               1.844494\n",
              "worldrank_diff        1.789757\n",
              "tuition_o             1.758634\n",
              "mn_sat_o              1.751497\n",
              "tvsport_diff          1.749663\n",
              "masters_o             1.740704\n",
              "movies_diff           1.736810\n",
              "masters_m             1.666675\n",
              "(3_1-pf_o)_fun        1.576829\n",
              "dining_diff           1.538123\n",
              "gaming_diff           1.473708\n",
              "hiking_diff           1.394160\n",
              "exercise_diff         1.358569\n",
              "yoga_diff             1.346006\n",
              "go_out_diff           1.276462\n",
              "date_diff             1.255635\n",
              "reading_diff          1.255330\n",
              "condtn                1.191354\n",
              "exphappy_o            1.180980\n",
              "clubbing_diff         1.177137\n",
              "order                 1.116442\n",
              "career_c_m            1.086977\n",
              "int_corr              1.067870\n",
              "imprelig_m            1.065587\n",
              "imprace_m             1.059333\n",
              "met_o                 1.056665\n",
              "samerace              1.035840\n",
              "goal_m                1.021699\n",
              "from_m                1.009537\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw2U26VW_1W6"
      },
      "source": [
        "# Drop the column with highest VIF\n",
        "\n",
        "inte_X_train = inte_X_train.drop(columns= ['attr1_1'])\n",
        "inte_X_test = inte_X_test.drop(columns= ['attr1_1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dz4ovPbGvYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b97f460-aeb5-47d5-dbdb-2866b3732ec3"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "inte_X_train = sm.add_constant(inte_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "inte_model2 = sm.OLS(inte_y_train, inte_X_train).fit() #ordinary least square\n",
        "print(inte_model2.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                intel_o   R-squared:                       0.091\n",
            "Model:                            OLS   Adj. R-squared:                  0.083\n",
            "Method:                 Least Squares   F-statistic:                     12.06\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          2.36e-101\n",
            "Time:                        23:11:42   Log-Likelihood:                -12297.\n",
            "No. Observations:                6839   AIC:                         2.471e+04\n",
            "Df Residuals:                    6782   BIC:                         2.510e+04\n",
            "Df Model:                          56                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.3800      0.018    415.965      0.000       7.345       7.415\n",
            "gender               -0.0047      0.030     -0.155      0.876      -0.064       0.055\n",
            "condtn                0.0129      0.019      0.667      0.505      -0.025       0.051\n",
            "order                -0.1560      0.019     -8.320      0.000      -0.193      -0.119\n",
            "int_corr              0.0777      0.018      4.240      0.000       0.042       0.114\n",
            "samerace              0.0068      0.018      0.375      0.707      -0.029       0.042\n",
            "age_o                 0.0479      0.026      1.868      0.062      -0.002       0.098\n",
            "mn_sat_o             -0.0239      0.023     -1.020      0.308      -0.070       0.022\n",
            "tuition_o            -0.0326      0.024     -1.386      0.166      -0.079       0.014\n",
            "income                0.0007      0.026      0.028      0.978      -0.050       0.051\n",
            "exphappy_o            0.1137      0.019      5.899      0.000       0.076       0.152\n",
            "met_o                -0.1461      0.018     -8.009      0.000      -0.182      -0.110\n",
            "world_rank_o         -0.0990      0.024     -4.073      0.000      -0.147      -0.051\n",
            "masters_o            -0.1107      0.023     -4.728      0.000      -0.157      -0.065\n",
            "sinc1_1              -0.0774      0.029     -2.705      0.007      -0.134      -0.021\n",
            "intel1_1              0.1195      0.030      4.017      0.000       0.061       0.178\n",
            "fun1_1               -0.0793      0.026     -2.997      0.003      -0.131      -0.027\n",
            "amb1_1                0.0037      0.026      0.140      0.889      -0.048       0.056\n",
            "shar1_1               0.0278      0.027      1.019      0.308      -0.026       0.081\n",
            "age_diff             -0.0242      0.026     -0.933      0.351      -0.075       0.027\n",
            "income_diff          -0.0274      0.026     -1.044      0.297      -0.079       0.024\n",
            "date_diff            -0.0216      0.020     -1.084      0.278      -0.061       0.017\n",
            "go_out_diff           0.0364      0.020      1.814      0.070      -0.003       0.076\n",
            "sports_diff           0.0041      0.026      0.159      0.874      -0.046       0.054\n",
            "tvsport_diff          0.0216      0.023      0.920      0.358      -0.024       0.067\n",
            "exercise_diff         0.0416      0.021      2.010      0.044       0.001       0.082\n",
            "dining_diff          -0.0320      0.022     -1.453      0.146      -0.075       0.011\n",
            "museums_diff          0.0242      0.038      0.640      0.522      -0.050       0.098\n",
            "art_diff             -0.0236      0.036     -0.654      0.513      -0.095       0.047\n",
            "hiking_diff          -0.0644      0.021     -3.075      0.002      -0.105      -0.023\n",
            "gaming_diff          -0.0322      0.022     -1.496      0.135      -0.074       0.010\n",
            "clubbing_diff         0.0411      0.019      2.138      0.033       0.003       0.079\n",
            "reading_diff          0.1590      0.020      8.002      0.000       0.120       0.198\n",
            "tv_diff               0.1407      0.024      5.840      0.000       0.093       0.188\n",
            "theater_diff          0.1398      0.027      5.200      0.000       0.087       0.192\n",
            "movies_diff          -0.0469      0.023     -2.006      0.045      -0.093      -0.001\n",
            "concerts_diff        -0.0468      0.027     -1.732      0.083      -0.100       0.006\n",
            "music_diff            0.0738      0.024      3.023      0.003       0.026       0.122\n",
            "shopping_diff        -0.0226      0.026     -0.875      0.382      -0.073       0.028\n",
            "yoga_diff            -0.0169      0.021     -0.821      0.411      -0.057       0.023\n",
            "worldrank_diff        0.0744      0.024      3.133      0.002       0.028       0.121\n",
            "(3_1-pf_o)_att       -0.0953      0.034     -2.842      0.004      -0.161      -0.030\n",
            "(3_1-pf_o)_sinc      -0.0883      0.024     -3.615      0.000      -0.136      -0.040\n",
            "(3_1-pf_o)_fun       -0.0527      0.022     -2.368      0.018      -0.096      -0.009\n",
            "(3_1-pf_o)_intel      0.0511      0.024      2.090      0.037       0.003       0.099\n",
            "(3_1-pf_o)_amb       -0.1317      0.025     -5.261      0.000      -0.181      -0.083\n",
            "(1_1-2_1_o)_sinc      0.1060      0.030      3.575      0.000       0.048       0.164\n",
            "(1_1-2_1_o)_fun       0.0765      0.027      2.833      0.005       0.024       0.129\n",
            "(1_1-2_1_o)_intel    -0.1474      0.029     -5.027      0.000      -0.205      -0.090\n",
            "(1_1-2_1_o)_amb       0.0029      0.025      0.115      0.908      -0.046       0.052\n",
            "(1_1-2_1_o)_shar     -0.0128      0.028     -0.450      0.653      -0.068       0.043\n",
            "from_m               -0.0040      0.018     -0.222      0.825      -0.039       0.031\n",
            "goal_m               -0.0349      0.018     -1.946      0.052      -0.070       0.000\n",
            "imprace_m             0.0063      0.018      0.343      0.731      -0.030       0.042\n",
            "imprelig_m            0.0150      0.018      0.821      0.411      -0.021       0.051\n",
            "career_c_m            0.0725      0.018      3.919      0.000       0.036       0.109\n",
            "masters_m            -0.0209      0.023     -0.912      0.362      -0.066       0.024\n",
            "==============================================================================\n",
            "Omnibus:                      402.238   Durbin-Watson:                   2.027\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              536.083\n",
            "Skew:                          -0.547   Prob(JB):                    3.90e-117\n",
            "Kurtosis:                       3.828   Cond. No.                         6.31\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzXeRzt4HBjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d92101-48b2-4519-f4a2-0a4f918ef535"
      },
      "source": [
        "# Check VIF values again\n",
        "\n",
        "inte_cols.remove('attr1_1')\n",
        "VIF(inte_X_train, inte_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "museums_diff         4.528774\n",
              "art_diff             4.157250\n",
              "(3_1-pf_o)_att       3.573902\n",
              "gender               2.936946\n",
              "intel1_1             2.812037\n",
              "(1_1-2_1_o)_sinc     2.791725\n",
              "(1_1-2_1_o)_intel    2.733193\n",
              "sinc1_1              2.600995\n",
              "(1_1-2_1_o)_shar     2.565174\n",
              "shar1_1              2.364819\n",
              "concerts_diff        2.320016\n",
              "(1_1-2_1_o)_fun      2.314388\n",
              "theater_diff         2.295093\n",
              "amb1_1               2.227525\n",
              "fun1_1               2.226583\n",
              "income_diff          2.190097\n",
              "age_diff             2.138975\n",
              "income               2.118189\n",
              "shopping_diff        2.113484\n",
              "sports_diff          2.089885\n",
              "age_o                2.088097\n",
              "(3_1-pf_o)_amb       1.990347\n",
              "(1_1-2_1_o)_amb      1.990220\n",
              "(3_1-pf_o)_intel     1.899482\n",
              "(3_1-pf_o)_sinc      1.894264\n",
              "music_diff           1.892325\n",
              "world_rank_o         1.874698\n",
              "tv_diff              1.844490\n",
              "worldrank_diff       1.789498\n",
              "tuition_o            1.758633\n",
              "mn_sat_o             1.751477\n",
              "tvsport_diff         1.743310\n",
              "masters_o            1.740450\n",
              "movies_diff          1.734154\n",
              "masters_m            1.666628\n",
              "(3_1-pf_o)_fun       1.576545\n",
              "dining_diff          1.534967\n",
              "gaming_diff          1.473481\n",
              "hiking_diff          1.393901\n",
              "exercise_diff        1.358533\n",
              "yoga_diff            1.345558\n",
              "go_out_diff          1.276165\n",
              "date_diff            1.255491\n",
              "reading_diff         1.254360\n",
              "condtn               1.187338\n",
              "exphappy_o           1.180897\n",
              "clubbing_diff        1.177035\n",
              "order                1.116380\n",
              "career_c_m           1.086355\n",
              "int_corr             1.067856\n",
              "imprelig_m           1.065261\n",
              "imprace_m            1.059326\n",
              "met_o                1.056582\n",
              "samerace             1.034236\n",
              "goal_m               1.021091\n",
              "from_m               1.009250\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAvX9dyOSv5m"
      },
      "source": [
        "# Remove column with highest p value\n",
        "\n",
        "inte_X_train = inte_X_train.drop(columns= ['income'])\n",
        "inte_X_test = inte_X_test.drop(columns= ['income'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOdEuBxCVBOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78bc2c93-76c4-471b-b6ae-62491e101d05"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "inte_X_train = sm.add_constant(inte_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "inte_lr = sm.OLS(inte_y_train, inte_X_train).fit() #ordinary least square\n",
        "print(inte_lr.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                intel_o   R-squared:                       0.091\n",
            "Model:                            OLS   Adj. R-squared:                  0.083\n",
            "Method:                 Least Squares   F-statistic:                     12.28\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          6.68e-102\n",
            "Time:                        23:11:44   Log-Likelihood:                -12297.\n",
            "No. Observations:                6839   AIC:                         2.471e+04\n",
            "Df Residuals:                    6783   BIC:                         2.509e+04\n",
            "Df Model:                          55                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.3800      0.018    415.995      0.000       7.345       7.415\n",
            "gender               -0.0047      0.030     -0.156      0.876      -0.064       0.055\n",
            "condtn                0.0129      0.019      0.666      0.505      -0.025       0.051\n",
            "order                -0.1560      0.019     -8.320      0.000      -0.193      -0.119\n",
            "int_corr              0.0778      0.018      4.242      0.000       0.042       0.114\n",
            "samerace              0.0068      0.018      0.378      0.706      -0.029       0.042\n",
            "age_o                 0.0479      0.026      1.869      0.062      -0.002       0.098\n",
            "mn_sat_o             -0.0239      0.023     -1.020      0.308      -0.070       0.022\n",
            "tuition_o            -0.0326      0.024     -1.386      0.166      -0.079       0.014\n",
            "exphappy_o            0.1137      0.019      5.899      0.000       0.076       0.152\n",
            "met_o                -0.1461      0.018     -8.013      0.000      -0.182      -0.110\n",
            "world_rank_o         -0.0990      0.024     -4.074      0.000      -0.147      -0.051\n",
            "masters_o            -0.1107      0.023     -4.728      0.000      -0.157      -0.065\n",
            "sinc1_1              -0.0774      0.029     -2.706      0.007      -0.134      -0.021\n",
            "intel1_1              0.1195      0.030      4.019      0.000       0.061       0.178\n",
            "fun1_1               -0.0793      0.026     -2.999      0.003      -0.131      -0.027\n",
            "amb1_1                0.0037      0.026      0.140      0.888      -0.048       0.056\n",
            "shar1_1               0.0278      0.027      1.022      0.307      -0.026       0.081\n",
            "age_diff             -0.0242      0.026     -0.933      0.351      -0.075       0.027\n",
            "income_diff          -0.0279      0.018     -1.512      0.130      -0.064       0.008\n",
            "date_diff            -0.0216      0.020     -1.085      0.278      -0.061       0.017\n",
            "go_out_diff           0.0364      0.020      1.815      0.070      -0.003       0.076\n",
            "sports_diff           0.0041      0.026      0.159      0.874      -0.046       0.054\n",
            "tvsport_diff          0.0215      0.023      0.920      0.358      -0.024       0.067\n",
            "exercise_diff         0.0416      0.021      2.011      0.044       0.001       0.082\n",
            "dining_diff          -0.0319      0.022     -1.453      0.146      -0.075       0.011\n",
            "museums_diff          0.0242      0.038      0.640      0.522      -0.050       0.098\n",
            "art_diff             -0.0236      0.036     -0.653      0.514      -0.095       0.047\n",
            "hiking_diff          -0.0644      0.021     -3.076      0.002      -0.105      -0.023\n",
            "gaming_diff          -0.0322      0.022     -1.497      0.135      -0.074       0.010\n",
            "clubbing_diff         0.0411      0.019      2.138      0.033       0.003       0.079\n",
            "reading_diff          0.1590      0.020      8.003      0.000       0.120       0.198\n",
            "tv_diff               0.1407      0.024      5.842      0.000       0.094       0.188\n",
            "theater_diff          0.1398      0.027      5.200      0.000       0.087       0.192\n",
            "movies_diff          -0.0469      0.023     -2.007      0.045      -0.093      -0.001\n",
            "concerts_diff        -0.0468      0.027     -1.732      0.083      -0.100       0.006\n",
            "music_diff            0.0738      0.024      3.023      0.003       0.026       0.122\n",
            "shopping_diff        -0.0226      0.026     -0.875      0.382      -0.073       0.028\n",
            "yoga_diff            -0.0169      0.021     -0.822      0.411      -0.057       0.023\n",
            "worldrank_diff        0.0744      0.024      3.135      0.002       0.028       0.121\n",
            "(3_1-pf_o)_att       -0.0953      0.033     -2.845      0.004      -0.161      -0.030\n",
            "(3_1-pf_o)_sinc      -0.0883      0.024     -3.616      0.000      -0.136      -0.040\n",
            "(3_1-pf_o)_fun       -0.0527      0.022     -2.368      0.018      -0.096      -0.009\n",
            "(3_1-pf_o)_intel      0.0511      0.024      2.093      0.036       0.003       0.099\n",
            "(3_1-pf_o)_amb       -0.1317      0.025     -5.269      0.000      -0.181      -0.083\n",
            "(1_1-2_1_o)_sinc      0.1060      0.030      3.577      0.000       0.048       0.164\n",
            "(1_1-2_1_o)_fun       0.0764      0.027      2.833      0.005       0.024       0.129\n",
            "(1_1-2_1_o)_intel    -0.1475      0.029     -5.029      0.000      -0.205      -0.090\n",
            "(1_1-2_1_o)_amb       0.0029      0.025      0.115      0.909      -0.046       0.052\n",
            "(1_1-2_1_o)_shar     -0.0128      0.028     -0.451      0.652      -0.068       0.043\n",
            "from_m               -0.0040      0.018     -0.222      0.824      -0.039       0.031\n",
            "goal_m               -0.0349      0.018     -1.946      0.052      -0.070       0.000\n",
            "imprace_m             0.0063      0.018      0.343      0.732      -0.030       0.042\n",
            "imprelig_m            0.0150      0.018      0.821      0.411      -0.021       0.051\n",
            "career_c_m            0.0725      0.018      3.919      0.000       0.036       0.109\n",
            "masters_m            -0.0209      0.023     -0.912      0.362      -0.066       0.024\n",
            "==============================================================================\n",
            "Omnibus:                      402.271   Durbin-Watson:                   2.027\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              536.136\n",
            "Skew:                          -0.547   Prob(JB):                    3.80e-117\n",
            "Kurtosis:                       3.828   Cond. No.                         6.30\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYeDK21WHvpU"
      },
      "source": [
        "# Remove column with highest p value\n",
        "inte_X_train = inte_X_train.drop(columns= ['(1_1-2_1_o)_amb'])\n",
        "inte_X_test = inte_X_test.drop(columns= ['(1_1-2_1_o)_amb'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjEyNGTWH6jT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc67706-4034-46c9-9f30-14abc715bdea"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "inte_X_train = sm.add_constant(inte_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "inte_lr = sm.OLS(inte_y_train, inte_X_train).fit() #ordinary least square\n",
        "print(inte_lr.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                intel_o   R-squared:                       0.091\n",
            "Model:                            OLS   Adj. R-squared:                  0.083\n",
            "Method:                 Least Squares   F-statistic:                     12.51\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          1.89e-102\n",
            "Time:                        23:11:44   Log-Likelihood:                -12297.\n",
            "No. Observations:                6839   AIC:                         2.470e+04\n",
            "Df Residuals:                    6784   BIC:                         2.508e+04\n",
            "Df Model:                          54                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.3800      0.018    416.026      0.000       7.345       7.415\n",
            "gender               -0.0038      0.029     -0.129      0.898      -0.061       0.053\n",
            "condtn                0.0128      0.019      0.664      0.507      -0.025       0.051\n",
            "order                -0.1559      0.019     -8.320      0.000      -0.193      -0.119\n",
            "int_corr              0.0777      0.018      4.242      0.000       0.042       0.114\n",
            "samerace              0.0068      0.018      0.377      0.706      -0.029       0.042\n",
            "age_o                 0.0478      0.026      1.868      0.062      -0.002       0.098\n",
            "mn_sat_o             -0.0238      0.023     -1.015      0.310      -0.070       0.022\n",
            "tuition_o            -0.0326      0.024     -1.388      0.165      -0.079       0.013\n",
            "exphappy_o            0.1138      0.019      5.901      0.000       0.076       0.152\n",
            "met_o                -0.1461      0.018     -8.013      0.000      -0.182      -0.110\n",
            "world_rank_o         -0.0989      0.024     -4.073      0.000      -0.147      -0.051\n",
            "masters_o            -0.1104      0.023     -4.739      0.000      -0.156      -0.065\n",
            "sinc1_1              -0.0774      0.029     -2.705      0.007      -0.133      -0.021\n",
            "intel1_1              0.1193      0.030      4.021      0.000       0.061       0.177\n",
            "fun1_1               -0.0788      0.026     -3.026      0.002      -0.130      -0.028\n",
            "amb1_1                0.0057      0.020      0.284      0.777      -0.034       0.045\n",
            "shar1_1               0.0279      0.027      1.025      0.305      -0.025       0.081\n",
            "age_diff             -0.0242      0.026     -0.932      0.351      -0.075       0.027\n",
            "income_diff          -0.0279      0.018     -1.513      0.130      -0.064       0.008\n",
            "date_diff            -0.0215      0.020     -1.082      0.280      -0.060       0.017\n",
            "go_out_diff           0.0364      0.020      1.814      0.070      -0.003       0.076\n",
            "sports_diff           0.0041      0.026      0.160      0.873      -0.046       0.054\n",
            "tvsport_diff          0.0215      0.023      0.918      0.358      -0.024       0.067\n",
            "exercise_diff         0.0416      0.021      2.011      0.044       0.001       0.082\n",
            "dining_diff          -0.0319      0.022     -1.451      0.147      -0.075       0.011\n",
            "museums_diff          0.0239      0.038      0.634      0.526      -0.050       0.098\n",
            "art_diff             -0.0234      0.036     -0.648      0.517      -0.094       0.047\n",
            "hiking_diff          -0.0644      0.021     -3.076      0.002      -0.105      -0.023\n",
            "gaming_diff          -0.0322      0.022     -1.497      0.135      -0.074       0.010\n",
            "clubbing_diff         0.0412      0.019      2.140      0.032       0.003       0.079\n",
            "reading_diff          0.1590      0.020      8.003      0.000       0.120       0.198\n",
            "tv_diff               0.1407      0.024      5.841      0.000       0.093       0.188\n",
            "theater_diff          0.1400      0.027      5.227      0.000       0.088       0.193\n",
            "movies_diff          -0.0470      0.023     -2.013      0.044      -0.093      -0.001\n",
            "concerts_diff        -0.0469      0.027     -1.734      0.083      -0.100       0.006\n",
            "music_diff            0.0739      0.024      3.030      0.002       0.026       0.122\n",
            "shopping_diff        -0.0225      0.026     -0.874      0.382      -0.073       0.028\n",
            "yoga_diff            -0.0169      0.021     -0.820      0.412      -0.057       0.023\n",
            "worldrank_diff        0.0744      0.024      3.134      0.002       0.028       0.121\n",
            "(3_1-pf_o)_att       -0.0958      0.033     -2.894      0.004      -0.161      -0.031\n",
            "(3_1-pf_o)_sinc      -0.0884      0.024     -3.630      0.000      -0.136      -0.041\n",
            "(3_1-pf_o)_fun       -0.0526      0.022     -2.365      0.018      -0.096      -0.009\n",
            "(3_1-pf_o)_intel      0.0507      0.024      2.098      0.036       0.003       0.098\n",
            "(3_1-pf_o)_amb       -0.1313      0.025     -5.297      0.000      -0.180      -0.083\n",
            "(1_1-2_1_o)_sinc      0.1058      0.030      3.576      0.000       0.048       0.164\n",
            "(1_1-2_1_o)_fun       0.0756      0.026      2.918      0.004       0.025       0.126\n",
            "(1_1-2_1_o)_intel    -0.1472      0.029     -5.035      0.000      -0.205      -0.090\n",
            "(1_1-2_1_o)_shar     -0.0130      0.028     -0.457      0.647      -0.069       0.043\n",
            "from_m               -0.0039      0.018     -0.221      0.825      -0.039       0.031\n",
            "goal_m               -0.0349      0.018     -1.950      0.051      -0.070       0.000\n",
            "imprace_m             0.0063      0.018      0.343      0.732      -0.030       0.042\n",
            "imprelig_m            0.0151      0.018      0.822      0.411      -0.021       0.051\n",
            "career_c_m            0.0724      0.018      3.918      0.000       0.036       0.109\n",
            "masters_m            -0.0208      0.023     -0.909      0.363      -0.066       0.024\n",
            "==============================================================================\n",
            "Omnibus:                      402.224   Durbin-Watson:                   2.027\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              536.121\n",
            "Skew:                          -0.547   Prob(JB):                    3.83e-117\n",
            "Kurtosis:                       3.829   Cond. No.                         6.30\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-lBEuh4IWzs"
      },
      "source": [
        "# Remove column with highest p value\n",
        "\n",
        "inte_X_train = inte_X_train.drop(columns= ['gender'])\n",
        "inte_X_test = inte_X_test.drop(columns= ['gender'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGZxiTZ-IiQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a44da7f8-7e29-4955-b7b0-935c12650f2f"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "inte_X_train = sm.add_constant(inte_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "inte_lr = sm.OLS(inte_y_train, inte_X_train).fit() #ordinary least square\n",
        "print(inte_lr.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                intel_o   R-squared:                       0.091\n",
            "Model:                            OLS   Adj. R-squared:                  0.083\n",
            "Method:                 Least Squares   F-statistic:                     12.75\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          5.30e-103\n",
            "Time:                        23:11:44   Log-Likelihood:                -12297.\n",
            "No. Observations:                6839   AIC:                         2.470e+04\n",
            "Df Residuals:                    6785   BIC:                         2.507e+04\n",
            "Df Model:                          53                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.3800      0.018    416.056      0.000       7.345       7.415\n",
            "condtn                0.0129      0.019      0.666      0.506      -0.025       0.051\n",
            "order                -0.1560      0.019     -8.325      0.000      -0.193      -0.119\n",
            "int_corr              0.0778      0.018      4.250      0.000       0.042       0.114\n",
            "samerace              0.0068      0.018      0.378      0.706      -0.029       0.042\n",
            "age_o                 0.0478      0.026      1.867      0.062      -0.002       0.098\n",
            "mn_sat_o             -0.0237      0.023     -1.012      0.312      -0.070       0.022\n",
            "tuition_o            -0.0327      0.023     -1.393      0.164      -0.079       0.013\n",
            "exphappy_o            0.1139      0.019      5.927      0.000       0.076       0.152\n",
            "met_o                -0.1461      0.018     -8.014      0.000      -0.182      -0.110\n",
            "world_rank_o         -0.0989      0.024     -4.072      0.000      -0.147      -0.051\n",
            "masters_o            -0.1102      0.023     -4.743      0.000      -0.156      -0.065\n",
            "sinc1_1              -0.0764      0.028     -2.762      0.006      -0.131      -0.022\n",
            "intel1_1              0.1203      0.029      4.190      0.000       0.064       0.177\n",
            "fun1_1               -0.0785      0.026     -3.027      0.002      -0.129      -0.028\n",
            "amb1_1                0.0062      0.020      0.318      0.751      -0.032       0.045\n",
            "shar1_1               0.0281      0.027      1.034      0.301      -0.025       0.081\n",
            "age_diff             -0.0241      0.026     -0.930      0.353      -0.075       0.027\n",
            "income_diff          -0.0280      0.018     -1.514      0.130      -0.064       0.008\n",
            "date_diff            -0.0216      0.020     -1.086      0.277      -0.060       0.017\n",
            "go_out_diff           0.0365      0.020      1.825      0.068      -0.003       0.076\n",
            "sports_diff           0.0045      0.025      0.177      0.860      -0.045       0.054\n",
            "tvsport_diff          0.0216      0.023      0.925      0.355      -0.024       0.068\n",
            "exercise_diff         0.0412      0.020      2.013      0.044       0.001       0.081\n",
            "dining_diff          -0.0320      0.022     -1.460      0.144      -0.075       0.011\n",
            "museums_diff          0.0244      0.037      0.650      0.516      -0.049       0.098\n",
            "art_diff             -0.0239      0.036     -0.665      0.506      -0.094       0.047\n",
            "hiking_diff          -0.0648      0.021     -3.136      0.002      -0.105      -0.024\n",
            "gaming_diff          -0.0314      0.021     -1.526      0.127      -0.072       0.009\n",
            "clubbing_diff         0.0409      0.019      2.139      0.033       0.003       0.078\n",
            "reading_diff          0.1589      0.020      8.005      0.000       0.120       0.198\n",
            "tv_diff               0.1404      0.024      5.857      0.000       0.093       0.187\n",
            "theater_diff          0.1397      0.027      5.233      0.000       0.087       0.192\n",
            "movies_diff          -0.0467      0.023     -2.010      0.044      -0.092      -0.001\n",
            "concerts_diff        -0.0469      0.027     -1.734      0.083      -0.100       0.006\n",
            "music_diff            0.0739      0.024      3.032      0.002       0.026       0.122\n",
            "shopping_diff        -0.0234      0.025     -0.937      0.349      -0.072       0.026\n",
            "yoga_diff            -0.0171      0.020     -0.836      0.403      -0.057       0.023\n",
            "worldrank_diff        0.0744      0.024      3.139      0.002       0.028       0.121\n",
            "(3_1-pf_o)_att       -0.0966      0.032     -2.974      0.003      -0.160      -0.033\n",
            "(3_1-pf_o)_sinc      -0.0883      0.024     -3.628      0.000      -0.136      -0.041\n",
            "(3_1-pf_o)_fun       -0.0527      0.022     -2.368      0.018      -0.096      -0.009\n",
            "(3_1-pf_o)_intel      0.0507      0.024      2.097      0.036       0.003       0.098\n",
            "(3_1-pf_o)_amb       -0.1309      0.025     -5.316      0.000      -0.179      -0.083\n",
            "(1_1-2_1_o)_sinc      0.1050      0.029      3.627      0.000       0.048       0.162\n",
            "(1_1-2_1_o)_fun       0.0753      0.026      2.918      0.004       0.025       0.126\n",
            "(1_1-2_1_o)_intel    -0.1481      0.028     -5.197      0.000      -0.204      -0.092\n",
            "(1_1-2_1_o)_shar     -0.0127      0.028     -0.449      0.654      -0.068       0.043\n",
            "from_m               -0.0040      0.018     -0.223      0.824      -0.039       0.031\n",
            "goal_m               -0.0349      0.018     -1.949      0.051      -0.070       0.000\n",
            "imprace_m             0.0063      0.018      0.346      0.729      -0.029       0.042\n",
            "imprelig_m            0.0150      0.018      0.821      0.412      -0.021       0.051\n",
            "career_c_m            0.0724      0.018      3.922      0.000       0.036       0.109\n",
            "masters_m            -0.0207      0.023     -0.905      0.365      -0.066       0.024\n",
            "==============================================================================\n",
            "Omnibus:                      402.941   Durbin-Watson:                   2.027\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              537.406\n",
            "Skew:                          -0.547   Prob(JB):                    2.01e-117\n",
            "Kurtosis:                       3.830   Cond. No.                         5.93\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pofwh7biNPVY"
      },
      "source": [
        "# Remove column with highest p value\n",
        "\n",
        "inte_X_train = inte_X_train.drop(columns= ['sports_diff'])\n",
        "inte_X_test = inte_X_test.drop(columns= ['sports_diff'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxHhzgoLNYfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283a8baf-fc72-42d0-bf79-70ece24e5803"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "inte_X_train = sm.add_constant(inte_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "inte_lr = sm.OLS(inte_y_train, inte_X_train).fit() #ordinary least square\n",
        "print(inte_lr.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                intel_o   R-squared:                       0.091\n",
            "Model:                            OLS   Adj. R-squared:                  0.084\n",
            "Method:                 Least Squares   F-statistic:                     13.00\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          1.48e-103\n",
            "Time:                        23:11:45   Log-Likelihood:                -12297.\n",
            "No. Observations:                6839   AIC:                         2.470e+04\n",
            "Df Residuals:                    6786   BIC:                         2.506e+04\n",
            "Df Model:                          52                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.3800      0.018    416.085      0.000       7.345       7.415\n",
            "condtn                0.0128      0.019      0.664      0.507      -0.025       0.051\n",
            "order                -0.1560      0.019     -8.325      0.000      -0.193      -0.119\n",
            "int_corr              0.0778      0.018      4.252      0.000       0.042       0.114\n",
            "samerace              0.0068      0.018      0.378      0.705      -0.028       0.042\n",
            "age_o                 0.0478      0.026      1.868      0.062      -0.002       0.098\n",
            "mn_sat_o             -0.0235      0.023     -1.006      0.315      -0.069       0.022\n",
            "tuition_o            -0.0330      0.023     -1.408      0.159      -0.079       0.013\n",
            "exphappy_o            0.1144      0.019      6.018      0.000       0.077       0.152\n",
            "met_o                -0.1461      0.018     -8.015      0.000      -0.182      -0.110\n",
            "world_rank_o         -0.0989      0.024     -4.075      0.000      -0.147      -0.051\n",
            "masters_o            -0.1101      0.023     -4.740      0.000      -0.156      -0.065\n",
            "sinc1_1              -0.0763      0.028     -2.759      0.006      -0.131      -0.022\n",
            "intel1_1              0.1208      0.029      4.228      0.000       0.065       0.177\n",
            "fun1_1               -0.0783      0.026     -3.022      0.003      -0.129      -0.027\n",
            "amb1_1                0.0064      0.020      0.326      0.745      -0.032       0.045\n",
            "shar1_1               0.0285      0.027      1.049      0.294      -0.025       0.082\n",
            "age_diff             -0.0243      0.026     -0.938      0.348      -0.075       0.026\n",
            "income_diff          -0.0280      0.018     -1.518      0.129      -0.064       0.008\n",
            "date_diff            -0.0218      0.020     -1.099      0.272      -0.061       0.017\n",
            "go_out_diff           0.0361      0.020      1.817      0.069      -0.003       0.075\n",
            "tvsport_diff          0.0235      0.021      1.117      0.264      -0.018       0.065\n",
            "exercise_diff         0.0424      0.019      2.182      0.029       0.004       0.080\n",
            "dining_diff          -0.0323      0.022     -1.476      0.140      -0.075       0.011\n",
            "museums_diff          0.0245      0.037      0.654      0.513      -0.049       0.098\n",
            "art_diff             -0.0240      0.036     -0.668      0.504      -0.094       0.046\n",
            "hiking_diff          -0.0642      0.020     -3.153      0.002      -0.104      -0.024\n",
            "gaming_diff          -0.0314      0.021     -1.525      0.127      -0.072       0.009\n",
            "clubbing_diff         0.0410      0.019      2.141      0.032       0.003       0.078\n",
            "reading_diff          0.1587      0.020      8.007      0.000       0.120       0.198\n",
            "tv_diff               0.1398      0.024      5.895      0.000       0.093       0.186\n",
            "theater_diff          0.1389      0.026      5.282      0.000       0.087       0.190\n",
            "movies_diff          -0.0466      0.023     -2.005      0.045      -0.092      -0.001\n",
            "concerts_diff        -0.0467      0.027     -1.730      0.084      -0.100       0.006\n",
            "music_diff            0.0739      0.024      3.033      0.002       0.026       0.122\n",
            "shopping_diff        -0.0236      0.025     -0.949      0.343      -0.072       0.025\n",
            "yoga_diff            -0.0172      0.020     -0.840      0.401      -0.057       0.023\n",
            "worldrank_diff        0.0743      0.024      3.135      0.002       0.028       0.121\n",
            "(3_1-pf_o)_att       -0.0973      0.032     -3.013      0.003      -0.161      -0.034\n",
            "(3_1-pf_o)_sinc      -0.0885      0.024     -3.644      0.000      -0.136      -0.041\n",
            "(3_1-pf_o)_fun       -0.0528      0.022     -2.372      0.018      -0.096      -0.009\n",
            "(3_1-pf_o)_intel      0.0507      0.024      2.095      0.036       0.003       0.098\n",
            "(3_1-pf_o)_amb       -0.1311      0.025     -5.330      0.000      -0.179      -0.083\n",
            "(1_1-2_1_o)_sinc      0.1050      0.029      3.625      0.000       0.048       0.162\n",
            "(1_1-2_1_o)_fun       0.0751      0.026      2.913      0.004       0.025       0.126\n",
            "(1_1-2_1_o)_intel    -0.1484      0.028     -5.219      0.000      -0.204      -0.093\n",
            "(1_1-2_1_o)_shar     -0.0131      0.028     -0.464      0.643      -0.068       0.042\n",
            "from_m               -0.0040      0.018     -0.224      0.823      -0.039       0.031\n",
            "goal_m               -0.0350      0.018     -1.951      0.051      -0.070       0.000\n",
            "imprace_m             0.0063      0.018      0.345      0.730      -0.029       0.042\n",
            "imprelig_m            0.0150      0.018      0.820      0.412      -0.021       0.051\n",
            "career_c_m            0.0724      0.018      3.923      0.000       0.036       0.109\n",
            "masters_m            -0.0207      0.023     -0.904      0.366      -0.066       0.024\n",
            "==============================================================================\n",
            "Omnibus:                      402.770   Durbin-Watson:                   2.027\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              537.042\n",
            "Skew:                          -0.547   Prob(JB):                    2.41e-117\n",
            "Kurtosis:                       3.830   Cond. No.                         5.85\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg6PA8ohNfSA"
      },
      "source": [
        "# Remove column with highest p value\n",
        "\n",
        "inte_X_train = inte_X_train.drop(columns= ['from_m'])\n",
        "inte_X_test = inte_X_test.drop(columns= ['from_m'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZkiJX-oNmGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dadb375f-3353-47c2-bcdc-c2c2a038d9c1"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "inte_X_train = sm.add_constant(inte_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "inte_lr = sm.OLS(inte_y_train, inte_X_train).fit() #ordinary least square\n",
        "print(inte_lr.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                intel_o   R-squared:                       0.091\n",
            "Model:                            OLS   Adj. R-squared:                  0.084\n",
            "Method:                 Least Squares   F-statistic:                     13.25\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          4.14e-104\n",
            "Time:                        23:11:45   Log-Likelihood:                -12297.\n",
            "No. Observations:                6839   AIC:                         2.470e+04\n",
            "Df Residuals:                    6787   BIC:                         2.505e+04\n",
            "Df Model:                          51                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.3800      0.018    416.115      0.000       7.345       7.415\n",
            "condtn                0.0128      0.019      0.664      0.506      -0.025       0.051\n",
            "order                -0.1560      0.019     -8.325      0.000      -0.193      -0.119\n",
            "int_corr              0.0778      0.018      4.250      0.000       0.042       0.114\n",
            "samerace              0.0067      0.018      0.373      0.709      -0.029       0.042\n",
            "age_o                 0.0479      0.026      1.871      0.061      -0.002       0.098\n",
            "mn_sat_o             -0.0234      0.023     -1.002      0.316      -0.069       0.022\n",
            "tuition_o            -0.0331      0.023     -1.410      0.158      -0.079       0.013\n",
            "exphappy_o            0.1145      0.019      6.023      0.000       0.077       0.152\n",
            "met_o                -0.1461      0.018     -8.018      0.000      -0.182      -0.110\n",
            "world_rank_o         -0.0988      0.024     -4.070      0.000      -0.146      -0.051\n",
            "masters_o            -0.1100      0.023     -4.739      0.000      -0.156      -0.065\n",
            "sinc1_1              -0.0762      0.028     -2.756      0.006      -0.130      -0.022\n",
            "intel1_1              0.1208      0.029      4.229      0.000       0.065       0.177\n",
            "fun1_1               -0.0781      0.026     -3.019      0.003      -0.129      -0.027\n",
            "amb1_1                0.0063      0.020      0.320      0.749      -0.032       0.045\n",
            "shar1_1               0.0285      0.027      1.050      0.294      -0.025       0.082\n",
            "age_diff             -0.0243      0.026     -0.939      0.348      -0.075       0.026\n",
            "income_diff          -0.0280      0.018     -1.519      0.129      -0.064       0.008\n",
            "date_diff            -0.0218      0.020     -1.099      0.272      -0.061       0.017\n",
            "go_out_diff           0.0361      0.020      1.818      0.069      -0.003       0.075\n",
            "tvsport_diff          0.0234      0.021      1.115      0.265      -0.018       0.065\n",
            "exercise_diff         0.0423      0.019      2.182      0.029       0.004       0.080\n",
            "dining_diff          -0.0323      0.022     -1.476      0.140      -0.075       0.011\n",
            "museums_diff          0.0246      0.037      0.656      0.512      -0.049       0.098\n",
            "art_diff             -0.0241      0.036     -0.671      0.503      -0.094       0.046\n",
            "hiking_diff          -0.0642      0.020     -3.151      0.002      -0.104      -0.024\n",
            "gaming_diff          -0.0314      0.021     -1.524      0.127      -0.072       0.009\n",
            "clubbing_diff         0.0409      0.019      2.141      0.032       0.003       0.078\n",
            "reading_diff          0.1587      0.020      8.005      0.000       0.120       0.198\n",
            "tv_diff               0.1397      0.024      5.895      0.000       0.093       0.186\n",
            "theater_diff          0.1390      0.026      5.285      0.000       0.087       0.191\n",
            "movies_diff          -0.0466      0.023     -2.006      0.045      -0.092      -0.001\n",
            "concerts_diff        -0.0467      0.027     -1.731      0.084      -0.100       0.006\n",
            "music_diff            0.0740      0.024      3.034      0.002       0.026       0.122\n",
            "shopping_diff        -0.0237      0.025     -0.951      0.342      -0.072       0.025\n",
            "yoga_diff            -0.0172      0.020     -0.839      0.402      -0.057       0.023\n",
            "worldrank_diff        0.0742      0.024      3.131      0.002       0.028       0.121\n",
            "(3_1-pf_o)_att       -0.0971      0.032     -3.007      0.003      -0.160      -0.034\n",
            "(3_1-pf_o)_sinc      -0.0885      0.024     -3.643      0.000      -0.136      -0.041\n",
            "(3_1-pf_o)_fun       -0.0528      0.022     -2.374      0.018      -0.096      -0.009\n",
            "(3_1-pf_o)_intel      0.0507      0.024      2.095      0.036       0.003       0.098\n",
            "(3_1-pf_o)_amb       -0.1310      0.025     -5.326      0.000      -0.179      -0.083\n",
            "(1_1-2_1_o)_sinc      0.1050      0.029      3.625      0.000       0.048       0.162\n",
            "(1_1-2_1_o)_fun       0.0751      0.026      2.915      0.004       0.025       0.126\n",
            "(1_1-2_1_o)_intel    -0.1482      0.028     -5.215      0.000      -0.204      -0.092\n",
            "(1_1-2_1_o)_shar     -0.0130      0.028     -0.461      0.645      -0.068       0.042\n",
            "goal_m               -0.0349      0.018     -1.949      0.051      -0.070       0.000\n",
            "imprace_m             0.0064      0.018      0.349      0.727      -0.029       0.042\n",
            "imprelig_m            0.0150      0.018      0.818      0.413      -0.021       0.051\n",
            "career_c_m            0.0724      0.018      3.923      0.000       0.036       0.109\n",
            "masters_m            -0.0206      0.023     -0.902      0.367      -0.065       0.024\n",
            "==============================================================================\n",
            "Omnibus:                      402.465   Durbin-Watson:                   2.027\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              536.528\n",
            "Skew:                          -0.547   Prob(JB):                    3.12e-117\n",
            "Kurtosis:                       3.829   Cond. No.                         5.85\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNZSH5piOHp8"
      },
      "source": [
        "# Remove column with highest p value\n",
        "\n",
        "inte_X_train = inte_X_train.drop(columns= ['amb1_1'])\n",
        "inte_X_test = inte_X_test.drop(columns= ['amb1_1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E87QPMB1OGY0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d6702df-cdbd-4cc1-de16-d047a5bc6eea"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "inte_X_train = sm.add_constant(inte_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "inte_lr = sm.OLS(inte_y_train, inte_X_train).fit() #ordinary least square\n",
        "print(inte_lr.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                intel_o   R-squared:                       0.091\n",
            "Model:                            OLS   Adj. R-squared:                  0.084\n",
            "Method:                 Least Squares   F-statistic:                     13.52\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          1.17e-104\n",
            "Time:                        23:11:45   Log-Likelihood:                -12297.\n",
            "No. Observations:                6839   AIC:                         2.470e+04\n",
            "Df Residuals:                    6788   BIC:                         2.505e+04\n",
            "Df Model:                          50                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.3800      0.018    416.142      0.000       7.345       7.415\n",
            "condtn                0.0126      0.019      0.653      0.514      -0.025       0.050\n",
            "order                -0.1560      0.019     -8.330      0.000      -0.193      -0.119\n",
            "int_corr              0.0779      0.018      4.256      0.000       0.042       0.114\n",
            "samerace              0.0065      0.018      0.364      0.716      -0.029       0.042\n",
            "age_o                 0.0481      0.026      1.878      0.060      -0.002       0.098\n",
            "mn_sat_o             -0.0237      0.023     -1.014      0.311      -0.070       0.022\n",
            "tuition_o            -0.0329      0.023     -1.406      0.160      -0.079       0.013\n",
            "exphappy_o            0.1144      0.019      6.020      0.000       0.077       0.152\n",
            "met_o                -0.1461      0.018     -8.017      0.000      -0.182      -0.110\n",
            "world_rank_o         -0.0997      0.024     -4.134      0.000      -0.147      -0.052\n",
            "masters_o            -0.1097      0.023     -4.730      0.000      -0.155      -0.064\n",
            "sinc1_1              -0.0762      0.028     -2.753      0.006      -0.130      -0.022\n",
            "intel1_1              0.1211      0.029      4.242      0.000       0.065       0.177\n",
            "fun1_1               -0.0786      0.026     -3.040      0.002      -0.129      -0.028\n",
            "shar1_1               0.0291      0.027      1.077      0.281      -0.024       0.082\n",
            "age_diff             -0.0239      0.026     -0.924      0.355      -0.075       0.027\n",
            "income_diff          -0.0280      0.018     -1.519      0.129      -0.064       0.008\n",
            "date_diff            -0.0224      0.020     -1.134      0.257      -0.061       0.016\n",
            "go_out_diff           0.0364      0.020      1.834      0.067      -0.002       0.075\n",
            "tvsport_diff          0.0236      0.021      1.125      0.261      -0.018       0.065\n",
            "exercise_diff         0.0423      0.019      2.182      0.029       0.004       0.080\n",
            "dining_diff          -0.0326      0.022     -1.493      0.136      -0.075       0.010\n",
            "museums_diff          0.0245      0.037      0.655      0.513      -0.049       0.098\n",
            "art_diff             -0.0239      0.036     -0.665      0.506      -0.094       0.047\n",
            "hiking_diff          -0.0644      0.020     -3.166      0.002      -0.104      -0.025\n",
            "gaming_diff          -0.0313      0.021     -1.522      0.128      -0.072       0.009\n",
            "clubbing_diff         0.0406      0.019      2.126      0.034       0.003       0.078\n",
            "reading_diff          0.1587      0.020      8.005      0.000       0.120       0.198\n",
            "tv_diff               0.1397      0.024      5.893      0.000       0.093       0.186\n",
            "theater_diff          0.1380      0.026      5.282      0.000       0.087       0.189\n",
            "movies_diff          -0.0464      0.023     -1.998      0.046      -0.092      -0.001\n",
            "concerts_diff        -0.0466      0.027     -1.725      0.084      -0.100       0.006\n",
            "music_diff            0.0742      0.024      3.043      0.002       0.026       0.122\n",
            "shopping_diff        -0.0242      0.025     -0.976      0.329      -0.073       0.024\n",
            "yoga_diff            -0.0173      0.020     -0.847      0.397      -0.057       0.023\n",
            "worldrank_diff        0.0748      0.024      3.167      0.002       0.028       0.121\n",
            "(3_1-pf_o)_att       -0.0963      0.032     -2.992      0.003      -0.159      -0.033\n",
            "(3_1-pf_o)_sinc      -0.0878      0.024     -3.629      0.000      -0.135      -0.040\n",
            "(3_1-pf_o)_fun       -0.0524      0.022     -2.360      0.018      -0.096      -0.009\n",
            "(3_1-pf_o)_intel      0.0515      0.024      2.145      0.032       0.004       0.099\n",
            "(3_1-pf_o)_amb       -0.1304      0.025     -5.318      0.000      -0.178      -0.082\n",
            "(1_1-2_1_o)_sinc      0.1043      0.029      3.612      0.000       0.048       0.161\n",
            "(1_1-2_1_o)_fun       0.0751      0.026      2.913      0.004       0.025       0.126\n",
            "(1_1-2_1_o)_intel    -0.1491      0.028     -5.275      0.000      -0.205      -0.094\n",
            "(1_1-2_1_o)_shar     -0.0132      0.028     -0.467      0.641      -0.068       0.042\n",
            "goal_m               -0.0349      0.018     -1.948      0.051      -0.070       0.000\n",
            "imprace_m             0.0061      0.018      0.334      0.738      -0.030       0.042\n",
            "imprelig_m            0.0149      0.018      0.814      0.416      -0.021       0.051\n",
            "career_c_m            0.0728      0.018      3.951      0.000       0.037       0.109\n",
            "masters_m            -0.0205      0.023     -0.898      0.369      -0.065       0.024\n",
            "==============================================================================\n",
            "Omnibus:                      402.605   Durbin-Watson:                   2.027\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              536.589\n",
            "Skew:                          -0.547   Prob(JB):                    3.03e-117\n",
            "Kurtosis:                       3.829   Cond. No.                         5.81\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBLRCVbBOTCn"
      },
      "source": [
        "# Remove column with highest p value\n",
        "\n",
        "inte_X_train = inte_X_train.drop(columns= ['imprace_m'])\n",
        "inte_X_test = inte_X_test.drop(columns= ['imprace_m'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_87MsXiOa0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d903b7-390a-47cd-a7d3-01ae079db2f5"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "inte_X_train = sm.add_constant(inte_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "inte_lr = sm.OLS(inte_y_train, inte_X_train).fit() #ordinary least square\n",
        "print(inte_lr.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                intel_o   R-squared:                       0.091\n",
            "Model:                            OLS   Adj. R-squared:                  0.084\n",
            "Method:                 Least Squares   F-statistic:                     13.79\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):          3.30e-105\n",
            "Time:                        23:11:45   Log-Likelihood:                -12298.\n",
            "No. Observations:                6839   AIC:                         2.470e+04\n",
            "Df Residuals:                    6789   BIC:                         2.504e+04\n",
            "Df Model:                          49                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 7.3800      0.018    416.169      0.000       7.345       7.415\n",
            "condtn                0.0127      0.019      0.656      0.512      -0.025       0.050\n",
            "order                -0.1561      0.019     -8.337      0.000      -0.193      -0.119\n",
            "int_corr              0.0782      0.018      4.276      0.000       0.042       0.114\n",
            "samerace              0.0064      0.018      0.358      0.721      -0.029       0.042\n",
            "age_o                 0.0484      0.026      1.892      0.059      -0.002       0.098\n",
            "mn_sat_o             -0.0238      0.023     -1.019      0.308      -0.070       0.022\n",
            "tuition_o            -0.0331      0.023     -1.412      0.158      -0.079       0.013\n",
            "exphappy_o            0.1143      0.019      6.015      0.000       0.077       0.152\n",
            "met_o                -0.1461      0.018     -8.019      0.000      -0.182      -0.110\n",
            "world_rank_o         -0.0996      0.024     -4.133      0.000      -0.147      -0.052\n",
            "masters_o            -0.1099      0.023     -4.739      0.000      -0.155      -0.064\n",
            "sinc1_1              -0.0761      0.028     -2.752      0.006      -0.130      -0.022\n",
            "intel1_1              0.1213      0.029      4.250      0.000       0.065       0.177\n",
            "fun1_1               -0.0786      0.026     -3.042      0.002      -0.129      -0.028\n",
            "shar1_1               0.0292      0.027      1.082      0.280      -0.024       0.082\n",
            "age_diff             -0.0241      0.026     -0.933      0.351      -0.075       0.027\n",
            "income_diff          -0.0281      0.018     -1.522      0.128      -0.064       0.008\n",
            "date_diff            -0.0223      0.020     -1.131      0.258      -0.061       0.016\n",
            "go_out_diff           0.0364      0.020      1.834      0.067      -0.003       0.075\n",
            "tvsport_diff          0.0236      0.021      1.125      0.261      -0.018       0.065\n",
            "exercise_diff         0.0424      0.019      2.183      0.029       0.004       0.080\n",
            "dining_diff          -0.0326      0.022     -1.493      0.136      -0.075       0.010\n",
            "museums_diff          0.0243      0.037      0.648      0.517      -0.049       0.098\n",
            "art_diff             -0.0236      0.036     -0.657      0.511      -0.094       0.047\n",
            "hiking_diff          -0.0644      0.020     -3.162      0.002      -0.104      -0.024\n",
            "gaming_diff          -0.0313      0.021     -1.523      0.128      -0.072       0.009\n",
            "clubbing_diff         0.0407      0.019      2.131      0.033       0.003       0.078\n",
            "reading_diff          0.1587      0.020      8.005      0.000       0.120       0.198\n",
            "tv_diff               0.1397      0.024      5.896      0.000       0.093       0.186\n",
            "theater_diff          0.1380      0.026      5.280      0.000       0.087       0.189\n",
            "movies_diff          -0.0464      0.023     -1.999      0.046      -0.092      -0.001\n",
            "concerts_diff        -0.0465      0.027     -1.724      0.085      -0.099       0.006\n",
            "music_diff            0.0741      0.024      3.042      0.002       0.026       0.122\n",
            "shopping_diff        -0.0241      0.025     -0.972      0.331      -0.073       0.025\n",
            "yoga_diff            -0.0173      0.020     -0.844      0.398      -0.057       0.023\n",
            "worldrank_diff        0.0746      0.024      3.162      0.002       0.028       0.121\n",
            "(3_1-pf_o)_att       -0.0964      0.032     -2.996      0.003      -0.159      -0.033\n",
            "(3_1-pf_o)_sinc      -0.0879      0.024     -3.634      0.000      -0.135      -0.040\n",
            "(3_1-pf_o)_fun       -0.0525      0.022     -2.365      0.018      -0.096      -0.009\n",
            "(3_1-pf_o)_intel      0.0515      0.024      2.143      0.032       0.004       0.099\n",
            "(3_1-pf_o)_amb       -0.1301      0.024     -5.311      0.000      -0.178      -0.082\n",
            "(1_1-2_1_o)_sinc      0.1044      0.029      3.615      0.000       0.048       0.161\n",
            "(1_1-2_1_o)_fun       0.0751      0.026      2.916      0.004       0.025       0.126\n",
            "(1_1-2_1_o)_intel    -0.1495      0.028     -5.293      0.000      -0.205      -0.094\n",
            "(1_1-2_1_o)_shar     -0.0132      0.028     -0.467      0.641      -0.068       0.042\n",
            "goal_m               -0.0349      0.018     -1.950      0.051      -0.070       0.000\n",
            "imprelig_m            0.0160      0.018      0.892      0.372      -0.019       0.051\n",
            "career_c_m            0.0727      0.018      3.945      0.000       0.037       0.109\n",
            "masters_m            -0.0203      0.023     -0.887      0.375      -0.065       0.025\n",
            "==============================================================================\n",
            "Omnibus:                      402.146   Durbin-Watson:                   2.027\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              535.818\n",
            "Skew:                          -0.547   Prob(JB):                    4.45e-117\n",
            "Kurtosis:                       3.828   Cond. No.                         5.81\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8qxTFeMUWtA"
      },
      "source": [
        "# Recombine training and test sets without removed columns\n",
        "\n",
        "inte_train = inte_X_train.copy()\n",
        "inte_train['intel_o'] = inte_y_train \n",
        "\n",
        "inte_test = inte_X_test.copy()\n",
        "inte_test['intel_o'] = inte_y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_pYtqjgQDNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4036871-bfa6-4cb6-e4a8-df7e31a3a042"
      },
      "source": [
        "# Check OSR^2 with test set\n",
        "\n",
        "inte_OSR2 = OSR_2(inte_lr, inte_y_train, inte_test, 'intel_o')\n",
        "inte_OSR2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-44.96003325129975"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc3A-TTatwpE"
      },
      "source": [
        "## Intelligence (Linear Regression - Bootstrap)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klX4BBGRSDiP"
      },
      "source": [
        "inte_X_test2 = sm.add_constant(inte_X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute out-of-sample R-squared using the test set\n",
        "def OSR2(predictions, y_test,y_train):\n",
        "    SSE = np.sum((y_test-predictions)**2)\n",
        "    SST = np.sum((y_test-np.mean(y_train))**2)\n",
        "    r2 = 1-SSE/SST\n",
        "    return r2"
      ],
      "metadata": {
        "id": "09fbJ4OR8jBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check OSR^2 with test set\n",
        "test_OSR2 = OSR2(inte_lr.predict(inte_X_test2), inte_y_test, inte_y_train)\n",
        "test_OSR2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMAmwTDx8jJx",
        "outputId": "ed8e20c6-c3c0-47e6-8ed6-1de9481545ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06353026305261955"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U75oGgo6AzWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfad5fa5-9e62-4451-955d-41f4c3229178"
      },
      "source": [
        "# Bootstrap linear regression model\n",
        "\n",
        "inte_lr_output = bootstrap_validation(inte_X_test2, inte_y_test, inte_y_train, inte_lr, [OSR2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TX-EGZXYPMo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "0c50c401-1076-4d87-ad97-8e57e48176ce"
      },
      "source": [
        "#bootstrap plots\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap OSR2 Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot OSR2 - Test Set OSR2', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].hist(inte_lr_output.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[1].hist(inte_lr_output.iloc[:,0]-inte_OSR2, bins=20,edgecolor='green', linewidth=2,color = \"grey\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 5.,  4., 11., 17., 26., 23., 24., 43., 45., 47., 54., 58., 59.,\n",
              "        33., 16., 16.,  8.,  6.,  3.,  2.]),\n",
              " array([44.98757252, 44.99121043, 44.99484834, 44.99848626, 45.00212417,\n",
              "        45.00576208, 45.00939999, 45.01303791, 45.01667582, 45.02031373,\n",
              "        45.02395164, 45.02758956, 45.03122747, 45.03486538, 45.03850329,\n",
              "        45.0421412 , 45.04577912, 45.04941703, 45.05305494, 45.05669285,\n",
              "        45.06033077]),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 126
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LRYrkeqCDJq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb116df4-ec35-4743-80ed-0dc870cd29dd"
      },
      "source": [
        "# Mean of bootstrapped estimate\n",
        "\n",
        "print(\"Mean of Linear Regression estimates: %s\" % np.mean(inte_lr_output)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of Linear Regression estimates: 0.06306201599846706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tkzj4k-t57Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb037fc-1226-483d-c08b-b52fc9251caa"
      },
      "source": [
        "inte_lr_CI = np.quantile(inte_lr_output.iloc[:,0],np.array([0.025,0.975]))\n",
        "\n",
        "print('Mean: %.4f, Std: %.4f'%(statistics.mean(inte_lr_CI), statistics.stdev(inte_lr_CI)))\n",
        "print(\"The 95-percent confidence interval of the test set OSR2 for the linear regression intelligence model is %s\" % inte_lr_CI)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 0.0628, Std: 0.0362\n",
            "The 95-percent confidence interval of the test set OSR2 for the linear regression intelligence model is [0.03717071 0.08842216]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5RE7vU_t6Q3"
      },
      "source": [
        "## Intelligence (LDA - Train Model + Bootstrap Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey-KMPgy1_pH"
      },
      "source": [
        "# Change value type to be used for LDA\n",
        "\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "inte_y_train2 = inte_y_train.astype(int)\n",
        "inte_y_test2 = inte_y_test.astype(int)\n",
        "\n",
        "inte_X_train2 = inte_X_train.drop(columns= ['const'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKnJIuaa9GHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "832856c6-1842-4caa-9222-5885e781c8c3"
      },
      "source": [
        "# Train LDA model\n",
        "\n",
        "inte_lda = LinearDiscriminantAnalysis()\n",
        "inte_lda.fit(sm.add_constant(inte_X_train2), inte_y_train2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearDiscriminantAnalysis()"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7isng3gM5nBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563c659e-b495-4dea-d61f-611a60e22f38"
      },
      "source": [
        "inte_lda_pred = inte_lda.predict(sm.add_constant(inte_X_test))\n",
        "cm_inte_lda = confusion_matrix(inte_y_test2, inte_lda_pred)\n",
        "inte_lda_acc = accuracy_score(inte_y_test2, inte_lda_pred)\n",
        "\n",
        "print (\"Confusion Matrix: \\n\", cm_inte_lda)\n",
        "print (\"\\nLDA Test Accuracy:\", inte_lda_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix: \n",
            " [[  0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   2   0   0   0   0   3   1   0   1]\n",
            " [  0   0   5   0   0   1   0   2   0   0   0]\n",
            " [  0   0   1   0   0   0   1   6   6   0   0]\n",
            " [  0   0   0   1   0   0   0  16  19   2   0]\n",
            " [  0   0   1   0   0   7   7  54  49   5   1]\n",
            " [  0   0   0   0   0   2  18 114 118   4   4]\n",
            " [  2   0   2   0   0   7   8 168 204   8  11]\n",
            " [  0   0   3   0   0   4  10 156 257   9  20]\n",
            " [  0   0   0   0   0   3   3  79 146   8  14]\n",
            " [  0   1   0   0   0   1   1  37  73   8  16]]\n",
            "\n",
            "LDA Test Accuracy: 0.2801169590643275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGRQMNVbt-NA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c26c57-da66-4d04-8447-44fa6b3b8a88"
      },
      "source": [
        "# Bootstrap linear regression model\n",
        "\n",
        "inte_lda_output = bootstrap_validation(sm.add_constant(inte_X_test), inte_y_test2, inte_y_train2, inte_lda, [calc_acc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-3rZ4hXZ0qd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c6aa1b27-afa2-4eee-c35c-8e765782fadc"
      },
      "source": [
        "#bootstrap accuracy plots for sincerity\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap Accuracy Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot Acc - Test Set Acc', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].hist(inte_lda_output.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[0].set_xlim([0.2,0.3])\n",
        "axs[1].hist(inte_lda_output.iloc[:,0]-inte_lda_acc, bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[1].set_xlim([-0.05,0.05])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.05, 0.05)"
            ]
          },
          "metadata": {},
          "execution_count": 133
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6YoqLFgTBij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f827d2-9919-4d25-c4c2-9b6acf856979"
      },
      "source": [
        "# Mean of bootstrapped estimate\n",
        "\n",
        "print(\"Bootsrapped Mean of LDA accuracy estimates: %s\" % np.mean(inte_lda_output)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootsrapped Mean of LDA accuracy estimates: 0.28037777777777756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU0ObTfvTGv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ff8be6-4ebb-4ff5-de14-bb4a3945310d"
      },
      "source": [
        "inte_lda_CI = np.quantile(inte_lda_output.iloc[:,0],np.array([0.025,0.975]))\n",
        "\n",
        "print('Mean: %.4f, Std: %.4f'%(statistics.mean(inte_lda_CI), statistics.stdev(inte_lda_CI)))\n",
        "print(\"The 95-percent confidence interval of the test set accuuracy for the LDA intelligence model is %s\" % inte_lda_CI)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 0.2811, Std: 0.0304\n",
            "The 95-percent confidence interval of the test set accuuracy for the LDA intelligence model is [0.25964912 0.3026462 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intelligence (Rescaled OSR^2 & Baseline Comparisons)"
      ],
      "metadata": {
        "id": "gIs4C33fH3QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_inte = 1 - (np.sum((inte_y_test - inte_y_train.value_counts().index[0])**2)/np.sum((inte_y_test - np.mean(inte_y_train))**2))\n",
        "rm1_lr_inte = rescaled_OSR2_1(inte_lr, inte_y_train, inte_test, 'intel_o', 'lr')\n",
        "rm2_lr_inte = rescaled_OSR2_2(inte_lr, inte_y_train, inte_test, 'intel_o', 'lr')\n",
        "rm1_lda_inte = rescaled_OSR2_1(inte_lda, inte_y_train, inte_test_2.drop(columns= ['(1_1-2_1_o)_att', 'attr1_1', 'income', '(1_1-2_1_o)_amb', 'gender', 'sports_diff', 'from_m', 'amb1_1', 'imprace_m']), 'intel_o', 'lda')\n",
        "rm2_lda_inte = rescaled_OSR2_2(inte_lda, inte_y_train, inte_test_2.drop(columns= ['(1_1-2_1_o)_att', 'attr1_1', 'income', '(1_1-2_1_o)_amb', 'gender', 'sports_diff', 'from_m', 'amb1_1', 'imprace_m']), 'intel_o', 'lda')\n",
        " \n",
        "print('Models for INTELLIGENCE:')\n",
        "print('No Rescale Bootstrapped Mean, Linear Regression OSR2 for INTELLIGENCE:', np.mean(inte_lr_output)[0])\n",
        "print('No Rescale Bootstrapped Mean, LDA Accuracy for INTELLIGENCE:', np.mean(inte_lda_output)[0])\n",
        "print()\n",
        "print('Rescaling Linear Regression Model before OSR2 Calculation:')\n",
        "print('Rescale Method 1, Linear Regression INTELLIGENCE OSR2:', rm1_lr_inte)\n",
        "print('Rescale Method 2, Linear Regression INTELLIGENCE OSR2:', rm2_lr_inte)\n",
        "print()\n",
        "print('BASELINE:')\n",
        "print('Baseline OSR2:', base_inte)\n",
        "print()\n",
        "print('INTELLIGENCE MODEL COMPARISONS:')\n",
        "print('OSR2 difference between LR model and baseline:', np.amax(np.array([rm1_lr_inte, rm2_lr_inte, rm1_lda_inte, rm2_lda_inte, np.mean(inte_lr_output)[0]])) - base_inte)\n",
        "print('Accuracy difference for LDA and baseline:', np.mean(inte_lda_output)[0] - inte_y_train.value_counts().values[0]/np.sum(inte_y_train.value_counts()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1z0dF8JH6GR",
        "outputId": "e51b32ae-af60-4cc3-89d3-b079e805cc71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models for INTELLIGENCE:\n",
            "No Rescale Bootstrapped Mean, Linear Regression OSR2 for INTELLIGENCE: 0.06306201599846706\n",
            "No Rescale Bootstrapped Mean, LDA Accuracy for INTELLIGENCE: 0.28037777777777756\n",
            "\n",
            "Rescaling Linear Regression Model before OSR2 Calculation:\n",
            "Rescale Method 1, Linear Regression INTELLIGENCE OSR2: -9.588281198901678\n",
            "Rescale Method 2, Linear Regression INTELLIGENCE OSR2: -3.859513559592145\n",
            "\n",
            "BASELINE:\n",
            "Baseline OSR2: -0.16399769023522182\n",
            "\n",
            "INTELLIGENCE MODEL COMPARISONS:\n",
            "OSR2 difference between LR model and baseline: 0.22705970623368887\n",
            "Accuracy difference for LDA and baseline: 0.010455274487822908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx97qyXLr-OZ"
      },
      "source": [
        "## Fun (Linear Regression + LDA)\n",
        "\n",
        "Make linear model for this trait, perform feature selection.\n",
        "Bootstrap your model and report the mean and 95% CI for OSR^2.\n",
        "\n",
        "Run LDA on this trait. Bootstrap this model and report the mean and 95% CI for accuracy. Calculate OSR^2 as well.\n",
        "\n",
        "Calculate rescaled OSR^2 using the two methods above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkWf-cIvuMAJ"
      },
      "source": [
        "## Fun (Linear Regression - Feature Selection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osqY1pqRsLls",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "21846ba8-b324-47b9-aff1-ea7aec57ef0c"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "fun_train = fun_X_train.copy()\n",
        "fun_train['fun_o'] = fun_y_train\n",
        "fun_train.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d5a06857-2310-4606-a532-67031a425f52\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>condtn</th>\n",
              "      <th>order</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>mn_sat_o</th>\n",
              "      <th>tuition_o</th>\n",
              "      <th>income</th>\n",
              "      <th>exphappy_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>world_rank_o</th>\n",
              "      <th>masters_o</th>\n",
              "      <th>attr1_1</th>\n",
              "      <th>sinc1_1</th>\n",
              "      <th>intel1_1</th>\n",
              "      <th>fun1_1</th>\n",
              "      <th>amb1_1</th>\n",
              "      <th>shar1_1</th>\n",
              "      <th>age_diff</th>\n",
              "      <th>income_diff</th>\n",
              "      <th>date_diff</th>\n",
              "      <th>go_out_diff</th>\n",
              "      <th>sports_diff</th>\n",
              "      <th>tvsport_diff</th>\n",
              "      <th>exercise_diff</th>\n",
              "      <th>dining_diff</th>\n",
              "      <th>museums_diff</th>\n",
              "      <th>art_diff</th>\n",
              "      <th>hiking_diff</th>\n",
              "      <th>gaming_diff</th>\n",
              "      <th>clubbing_diff</th>\n",
              "      <th>reading_diff</th>\n",
              "      <th>tv_diff</th>\n",
              "      <th>theater_diff</th>\n",
              "      <th>movies_diff</th>\n",
              "      <th>concerts_diff</th>\n",
              "      <th>music_diff</th>\n",
              "      <th>shopping_diff</th>\n",
              "      <th>yoga_diff</th>\n",
              "      <th>worldrank_diff</th>\n",
              "      <th>(3_1-pf_o)_att</th>\n",
              "      <th>(3_1-pf_o)_sinc</th>\n",
              "      <th>(3_1-pf_o)_fun</th>\n",
              "      <th>(3_1-pf_o)_intel</th>\n",
              "      <th>(3_1-pf_o)_amb</th>\n",
              "      <th>(1_1-2_1_o)_att</th>\n",
              "      <th>(1_1-2_1_o)_sinc</th>\n",
              "      <th>(1_1-2_1_o)_fun</th>\n",
              "      <th>(1_1-2_1_o)_intel</th>\n",
              "      <th>(1_1-2_1_o)_amb</th>\n",
              "      <th>(1_1-2_1_o)_shar</th>\n",
              "      <th>from_m</th>\n",
              "      <th>goal_m</th>\n",
              "      <th>imprace_m</th>\n",
              "      <th>imprelig_m</th>\n",
              "      <th>career_c_m</th>\n",
              "      <th>masters_m</th>\n",
              "      <th>fun_o</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6954</th>\n",
              "      <td>-0.991723</td>\n",
              "      <td>-2.255038</td>\n",
              "      <td>-0.149328</td>\n",
              "      <td>0.233881</td>\n",
              "      <td>-0.813387</td>\n",
              "      <td>-0.672881</td>\n",
              "      <td>1.333911</td>\n",
              "      <td>1.099975</td>\n",
              "      <td>0.658140</td>\n",
              "      <td>0.841766</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>0.908585</td>\n",
              "      <td>2.514856</td>\n",
              "      <td>-0.600697</td>\n",
              "      <td>1.062289</td>\n",
              "      <td>-0.042256</td>\n",
              "      <td>-0.418781</td>\n",
              "      <td>-0.131968</td>\n",
              "      <td>0.492035</td>\n",
              "      <td>-0.228575</td>\n",
              "      <td>-0.523066</td>\n",
              "      <td>-2.457600</td>\n",
              "      <td>-0.662995</td>\n",
              "      <td>1.324276</td>\n",
              "      <td>0.006820</td>\n",
              "      <td>0.580234</td>\n",
              "      <td>-0.399607</td>\n",
              "      <td>-0.355632</td>\n",
              "      <td>-1.247353</td>\n",
              "      <td>-0.858932</td>\n",
              "      <td>-0.795239</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>-1.779372</td>\n",
              "      <td>-1.373462</td>\n",
              "      <td>-0.322525</td>\n",
              "      <td>-1.636807</td>\n",
              "      <td>-0.331758</td>\n",
              "      <td>-1.195244</td>\n",
              "      <td>-0.774410</td>\n",
              "      <td>-1.286902</td>\n",
              "      <td>1.958558</td>\n",
              "      <td>-0.228434</td>\n",
              "      <td>1.545946</td>\n",
              "      <td>-0.524867</td>\n",
              "      <td>-2.188444</td>\n",
              "      <td>0.008084</td>\n",
              "      <td>-0.408838</td>\n",
              "      <td>1.822972</td>\n",
              "      <td>1.364387</td>\n",
              "      <td>-2.811072</td>\n",
              "      <td>-1.097606</td>\n",
              "      <td>1.391160</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>-0.677990</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>-0.481827</td>\n",
              "      <td>-1.852308</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8929</th>\n",
              "      <td>1.008198</td>\n",
              "      <td>0.443386</td>\n",
              "      <td>-0.332301</td>\n",
              "      <td>-0.293041</td>\n",
              "      <td>-0.813387</td>\n",
              "      <td>-1.236699</td>\n",
              "      <td>0.074535</td>\n",
              "      <td>0.097131</td>\n",
              "      <td>0.301111</td>\n",
              "      <td>1.419254</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>0.908585</td>\n",
              "      <td>-0.397579</td>\n",
              "      <td>0.085268</td>\n",
              "      <td>-0.516057</td>\n",
              "      <td>-0.187872</td>\n",
              "      <td>-0.248665</td>\n",
              "      <td>-0.131968</td>\n",
              "      <td>0.967695</td>\n",
              "      <td>-0.228575</td>\n",
              "      <td>-0.267093</td>\n",
              "      <td>0.997576</td>\n",
              "      <td>-0.020509</td>\n",
              "      <td>0.800026</td>\n",
              "      <td>1.733087</td>\n",
              "      <td>0.290181</td>\n",
              "      <td>-0.399607</td>\n",
              "      <td>-1.376858</td>\n",
              "      <td>0.607510</td>\n",
              "      <td>1.096346</td>\n",
              "      <td>-0.271043</td>\n",
              "      <td>-0.573890</td>\n",
              "      <td>-1.779372</td>\n",
              "      <td>-0.266190</td>\n",
              "      <td>0.586733</td>\n",
              "      <td>0.407999</td>\n",
              "      <td>0.323659</td>\n",
              "      <td>0.412765</td>\n",
              "      <td>0.249252</td>\n",
              "      <td>-0.779486</td>\n",
              "      <td>-0.196424</td>\n",
              "      <td>0.106404</td>\n",
              "      <td>-0.522898</td>\n",
              "      <td>0.115593</td>\n",
              "      <td>1.293450</td>\n",
              "      <td>0.490224</td>\n",
              "      <td>-0.524354</td>\n",
              "      <td>-0.033636</td>\n",
              "      <td>0.801025</td>\n",
              "      <td>0.356241</td>\n",
              "      <td>0.119167</td>\n",
              "      <td>-0.221040</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>-0.677990</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>-0.481827</td>\n",
              "      <td>0.539787</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2304</th>\n",
              "      <td>1.008198</td>\n",
              "      <td>0.443386</td>\n",
              "      <td>1.131482</td>\n",
              "      <td>0.629072</td>\n",
              "      <td>1.229246</td>\n",
              "      <td>1.300478</td>\n",
              "      <td>0.074535</td>\n",
              "      <td>0.097131</td>\n",
              "      <td>0.255770</td>\n",
              "      <td>-0.313209</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>-0.658143</td>\n",
              "      <td>-0.397579</td>\n",
              "      <td>-0.292012</td>\n",
              "      <td>-0.523231</td>\n",
              "      <td>-0.246118</td>\n",
              "      <td>-0.201033</td>\n",
              "      <td>0.914727</td>\n",
              "      <td>0.694983</td>\n",
              "      <td>1.679816</td>\n",
              "      <td>0.439322</td>\n",
              "      <td>-0.483214</td>\n",
              "      <td>-0.020509</td>\n",
              "      <td>-1.034851</td>\n",
              "      <td>-1.966057</td>\n",
              "      <td>-1.450136</td>\n",
              "      <td>-0.399607</td>\n",
              "      <td>-1.717267</td>\n",
              "      <td>-1.556497</td>\n",
              "      <td>-0.020956</td>\n",
              "      <td>-0.271043</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>0.344005</td>\n",
              "      <td>-1.096644</td>\n",
              "      <td>-1.534870</td>\n",
              "      <td>-0.409924</td>\n",
              "      <td>-0.331758</td>\n",
              "      <td>-1.195244</td>\n",
              "      <td>-0.262579</td>\n",
              "      <td>2.011302</td>\n",
              "      <td>-0.196424</td>\n",
              "      <td>0.776080</td>\n",
              "      <td>-0.522898</td>\n",
              "      <td>-0.524867</td>\n",
              "      <td>-0.157339</td>\n",
              "      <td>-0.956198</td>\n",
              "      <td>0.087880</td>\n",
              "      <td>-0.554518</td>\n",
              "      <td>0.269212</td>\n",
              "      <td>-0.233535</td>\n",
              "      <td>0.274914</td>\n",
              "      <td>0.156675</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>-0.677990</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>2.075128</td>\n",
              "      <td>0.539787</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6168</th>\n",
              "      <td>1.008198</td>\n",
              "      <td>0.443386</td>\n",
              "      <td>2.046347</td>\n",
              "      <td>0.398544</td>\n",
              "      <td>1.229246</td>\n",
              "      <td>1.864296</td>\n",
              "      <td>-1.727672</td>\n",
              "      <td>-0.635368</td>\n",
              "      <td>-0.119396</td>\n",
              "      <td>0.264278</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>0.908585</td>\n",
              "      <td>-0.397579</td>\n",
              "      <td>0.256760</td>\n",
              "      <td>-0.372571</td>\n",
              "      <td>-0.042256</td>\n",
              "      <td>0.431800</td>\n",
              "      <td>-0.131968</td>\n",
              "      <td>-0.300731</td>\n",
              "      <td>0.831642</td>\n",
              "      <td>-0.391599</td>\n",
              "      <td>-0.483214</td>\n",
              "      <td>-0.020509</td>\n",
              "      <td>-1.821226</td>\n",
              "      <td>-0.486399</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>1.210903</td>\n",
              "      <td>1.006004</td>\n",
              "      <td>1.225797</td>\n",
              "      <td>1.375671</td>\n",
              "      <td>-0.008945</td>\n",
              "      <td>-0.573890</td>\n",
              "      <td>-0.009891</td>\n",
              "      <td>0.287446</td>\n",
              "      <td>1.495992</td>\n",
              "      <td>-0.000963</td>\n",
              "      <td>0.979076</td>\n",
              "      <td>0.412765</td>\n",
              "      <td>0.249252</td>\n",
              "      <td>1.250178</td>\n",
              "      <td>1.910670</td>\n",
              "      <td>-0.312143</td>\n",
              "      <td>-0.522898</td>\n",
              "      <td>-0.524867</td>\n",
              "      <td>1.583608</td>\n",
              "      <td>-1.599052</td>\n",
              "      <td>-0.986417</td>\n",
              "      <td>0.585233</td>\n",
              "      <td>-1.001732</td>\n",
              "      <td>-0.626718</td>\n",
              "      <td>0.727554</td>\n",
              "      <td>1.160846</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>-0.677990</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>2.075128</td>\n",
              "      <td>-1.852308</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1179</th>\n",
              "      <td>-0.991723</td>\n",
              "      <td>0.443386</td>\n",
              "      <td>1.314455</td>\n",
              "      <td>-0.260109</td>\n",
              "      <td>1.229246</td>\n",
              "      <td>-0.109064</td>\n",
              "      <td>0.074535</td>\n",
              "      <td>0.097131</td>\n",
              "      <td>-1.508353</td>\n",
              "      <td>0.264278</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>-0.658143</td>\n",
              "      <td>2.514856</td>\n",
              "      <td>-0.171969</td>\n",
              "      <td>1.062289</td>\n",
              "      <td>-0.770334</td>\n",
              "      <td>-0.418781</td>\n",
              "      <td>0.701388</td>\n",
              "      <td>-0.300731</td>\n",
              "      <td>-0.864706</td>\n",
              "      <td>1.731157</td>\n",
              "      <td>-0.976810</td>\n",
              "      <td>-1.947967</td>\n",
              "      <td>1.586401</td>\n",
              "      <td>0.993258</td>\n",
              "      <td>1.160339</td>\n",
              "      <td>-0.399607</td>\n",
              "      <td>-0.696041</td>\n",
              "      <td>-0.629065</td>\n",
              "      <td>-0.020956</td>\n",
              "      <td>0.253153</td>\n",
              "      <td>0.287642</td>\n",
              "      <td>-0.009891</td>\n",
              "      <td>1.117900</td>\n",
              "      <td>-0.322525</td>\n",
              "      <td>0.407999</td>\n",
              "      <td>0.979076</td>\n",
              "      <td>0.010763</td>\n",
              "      <td>0.249252</td>\n",
              "      <td>-0.018362</td>\n",
              "      <td>-0.196424</td>\n",
              "      <td>-1.484076</td>\n",
              "      <td>1.821792</td>\n",
              "      <td>1.396513</td>\n",
              "      <td>-0.012260</td>\n",
              "      <td>0.329511</td>\n",
              "      <td>-0.697627</td>\n",
              "      <td>1.616682</td>\n",
              "      <td>-0.438371</td>\n",
              "      <td>-0.080630</td>\n",
              "      <td>0.119167</td>\n",
              "      <td>0.009274</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>1.474731</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>-0.481827</td>\n",
              "      <td>0.539787</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5a06857-2310-4606-a532-67031a425f52')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5a06857-2310-4606-a532-67031a425f52 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5a06857-2310-4606-a532-67031a425f52');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        gender    condtn     order  ...  career_c_m  masters_m  fun_o\n",
              "6954 -0.991723 -2.255038 -0.149328  ...   -0.481827  -1.852308    5.0\n",
              "8929  1.008198  0.443386 -0.332301  ...   -0.481827   0.539787    9.0\n",
              "2304  1.008198  0.443386  1.131482  ...    2.075128   0.539787    8.0\n",
              "6168  1.008198  0.443386  2.046347  ...    2.075128  -1.852308    6.0\n",
              "1179 -0.991723  0.443386  1.314455  ...   -0.481827   0.539787    8.0\n",
              "\n",
              "[5 rows x 59 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgiPQB1tst9h",
        "outputId": "96591a9c-f506-4ce6-d02a-42ee5a909671"
      },
      "source": [
        "fun_cols = ['gender', 'condtn', 'order', 'int_corr', 'samerace', 'age_o',\n",
        "       'mn_sat_o', 'tuition_o', 'income', 'exphappy_o', 'met_o', 'fun1_1',\n",
        "       'world_rank_o', 'masters_o', 'attr1_1', 'sinc1_1', 'intel1_1',\n",
        "       'amb1_1', 'shar1_1', 'age_diff', 'income_diff',\n",
        "       'date_diff', 'go_out_diff', 'sports_diff', 'tvsport_diff',\n",
        "       'exercise_diff', 'dining_diff', 'museums_diff', 'art_diff',\n",
        "       'hiking_diff', 'gaming_diff', 'clubbing_diff', 'reading_diff',\n",
        "       'tv_diff', 'theater_diff', 'movies_diff', 'concerts_diff',\n",
        "       'music_diff', 'shopping_diff', 'yoga_diff', 'worldrank_diff',\n",
        "       '(3_1-pf_o)_att', '(3_1-pf_o)_sinc', '(3_1-pf_o)_fun',\n",
        "       '(3_1-pf_o)_intel', '(3_1-pf_o)_amb', '(1_1-2_1_o)_att',\n",
        "       '(1_1-2_1_o)_sinc', '(1_1-2_1_o)_fun', '(1_1-2_1_o)_intel',\n",
        "       '(1_1-2_1_o)_amb', '(1_1-2_1_o)_shar', 'from_m', 'goal_m',\n",
        "       'imprace_m', 'imprelig_m', 'career_c_m', 'masters_m']\n",
        "VIF(fun_X_train, fun_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_att      93.323556\n",
              "attr1_1              77.057432\n",
              "(1_1-2_1_o)_sinc     30.864469\n",
              "sinc1_1              28.960588\n",
              "intel1_1             28.700260\n",
              "(1_1-2_1_o)_intel    27.693427\n",
              "(1_1-2_1_o)_fun      25.176605\n",
              "(1_1-2_1_o)_shar     24.970671\n",
              "shar1_1              24.116997\n",
              "(1_1-2_1_o)_amb      21.601501\n",
              "fun1_1               20.607372\n",
              "amb1_1               20.330461\n",
              "museums_diff          4.558421\n",
              "art_diff              4.192226\n",
              "(3_1-pf_o)_att        3.510825\n",
              "gender                2.965416\n",
              "concerts_diff         2.321921\n",
              "theater_diff          2.304156\n",
              "income_diff           2.148007\n",
              "age_diff              2.090670\n",
              "shopping_diff         2.076787\n",
              "income                2.075251\n",
              "sports_diff           2.068414\n",
              "age_o                 2.049487\n",
              "(3_1-pf_o)_amb        1.938342\n",
              "music_diff            1.921185\n",
              "world_rank_o          1.901774\n",
              "(3_1-pf_o)_intel      1.898237\n",
              "(3_1-pf_o)_sinc       1.876609\n",
              "worldrank_diff        1.803230\n",
              "tv_diff               1.800527\n",
              "tuition_o             1.757068\n",
              "movies_diff           1.746792\n",
              "masters_o             1.741663\n",
              "mn_sat_o              1.738060\n",
              "tvsport_diff          1.734790\n",
              "masters_m             1.668791\n",
              "(3_1-pf_o)_fun        1.577595\n",
              "dining_diff           1.545178\n",
              "gaming_diff           1.483252\n",
              "hiking_diff           1.395856\n",
              "exercise_diff         1.371991\n",
              "yoga_diff             1.339267\n",
              "go_out_diff           1.260543\n",
              "reading_diff          1.254712\n",
              "date_diff             1.249815\n",
              "condtn                1.201190\n",
              "exphappy_o            1.181310\n",
              "clubbing_diff         1.177075\n",
              "order                 1.120708\n",
              "career_c_m            1.086638\n",
              "int_corr              1.067790\n",
              "imprelig_m            1.061962\n",
              "imprace_m             1.058205\n",
              "met_o                 1.051466\n",
              "samerace              1.033764\n",
              "goal_m                1.018535\n",
              "from_m                1.006909\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "o2HCuaK1KBgE",
        "outputId": "de39e8d9-13a3-4c8f-c689-62991a12a1a3"
      },
      "source": [
        "fun_X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4050fafc-fe0e-4244-9c89-1b99351ad1a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>condtn</th>\n",
              "      <th>order</th>\n",
              "      <th>int_corr</th>\n",
              "      <th>samerace</th>\n",
              "      <th>age_o</th>\n",
              "      <th>mn_sat_o</th>\n",
              "      <th>tuition_o</th>\n",
              "      <th>income</th>\n",
              "      <th>exphappy_o</th>\n",
              "      <th>met_o</th>\n",
              "      <th>world_rank_o</th>\n",
              "      <th>masters_o</th>\n",
              "      <th>attr1_1</th>\n",
              "      <th>sinc1_1</th>\n",
              "      <th>intel1_1</th>\n",
              "      <th>fun1_1</th>\n",
              "      <th>amb1_1</th>\n",
              "      <th>shar1_1</th>\n",
              "      <th>age_diff</th>\n",
              "      <th>income_diff</th>\n",
              "      <th>date_diff</th>\n",
              "      <th>go_out_diff</th>\n",
              "      <th>sports_diff</th>\n",
              "      <th>tvsport_diff</th>\n",
              "      <th>exercise_diff</th>\n",
              "      <th>dining_diff</th>\n",
              "      <th>museums_diff</th>\n",
              "      <th>art_diff</th>\n",
              "      <th>hiking_diff</th>\n",
              "      <th>gaming_diff</th>\n",
              "      <th>clubbing_diff</th>\n",
              "      <th>reading_diff</th>\n",
              "      <th>tv_diff</th>\n",
              "      <th>theater_diff</th>\n",
              "      <th>movies_diff</th>\n",
              "      <th>concerts_diff</th>\n",
              "      <th>music_diff</th>\n",
              "      <th>shopping_diff</th>\n",
              "      <th>yoga_diff</th>\n",
              "      <th>worldrank_diff</th>\n",
              "      <th>(3_1-pf_o)_att</th>\n",
              "      <th>(3_1-pf_o)_sinc</th>\n",
              "      <th>(3_1-pf_o)_fun</th>\n",
              "      <th>(3_1-pf_o)_intel</th>\n",
              "      <th>(3_1-pf_o)_amb</th>\n",
              "      <th>(1_1-2_1_o)_att</th>\n",
              "      <th>(1_1-2_1_o)_sinc</th>\n",
              "      <th>(1_1-2_1_o)_fun</th>\n",
              "      <th>(1_1-2_1_o)_intel</th>\n",
              "      <th>(1_1-2_1_o)_amb</th>\n",
              "      <th>(1_1-2_1_o)_shar</th>\n",
              "      <th>from_m</th>\n",
              "      <th>goal_m</th>\n",
              "      <th>imprace_m</th>\n",
              "      <th>imprelig_m</th>\n",
              "      <th>career_c_m</th>\n",
              "      <th>masters_m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6954</th>\n",
              "      <td>-0.991723</td>\n",
              "      <td>-2.255038</td>\n",
              "      <td>-0.149328</td>\n",
              "      <td>0.233881</td>\n",
              "      <td>-0.813387</td>\n",
              "      <td>-0.672881</td>\n",
              "      <td>1.333911</td>\n",
              "      <td>1.099975</td>\n",
              "      <td>0.658140</td>\n",
              "      <td>0.841766</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>0.908585</td>\n",
              "      <td>2.514856</td>\n",
              "      <td>-0.600697</td>\n",
              "      <td>1.062289</td>\n",
              "      <td>-0.042256</td>\n",
              "      <td>-0.418781</td>\n",
              "      <td>-0.131968</td>\n",
              "      <td>0.492035</td>\n",
              "      <td>-0.228575</td>\n",
              "      <td>-0.523066</td>\n",
              "      <td>-2.457600</td>\n",
              "      <td>-0.662995</td>\n",
              "      <td>1.324276</td>\n",
              "      <td>0.006820</td>\n",
              "      <td>0.580234</td>\n",
              "      <td>-0.399607</td>\n",
              "      <td>-0.355632</td>\n",
              "      <td>-1.247353</td>\n",
              "      <td>-0.858932</td>\n",
              "      <td>-0.795239</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>-1.779372</td>\n",
              "      <td>-1.373462</td>\n",
              "      <td>-0.322525</td>\n",
              "      <td>-1.636807</td>\n",
              "      <td>-0.331758</td>\n",
              "      <td>-1.195244</td>\n",
              "      <td>-0.774410</td>\n",
              "      <td>-1.286902</td>\n",
              "      <td>1.958558</td>\n",
              "      <td>-0.228434</td>\n",
              "      <td>1.545946</td>\n",
              "      <td>-0.524867</td>\n",
              "      <td>-2.188444</td>\n",
              "      <td>0.008084</td>\n",
              "      <td>-0.408838</td>\n",
              "      <td>1.822972</td>\n",
              "      <td>1.364387</td>\n",
              "      <td>-2.811072</td>\n",
              "      <td>-1.097606</td>\n",
              "      <td>1.391160</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>-0.677990</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>-0.481827</td>\n",
              "      <td>-1.852308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8929</th>\n",
              "      <td>1.008198</td>\n",
              "      <td>0.443386</td>\n",
              "      <td>-0.332301</td>\n",
              "      <td>-0.293041</td>\n",
              "      <td>-0.813387</td>\n",
              "      <td>-1.236699</td>\n",
              "      <td>0.074535</td>\n",
              "      <td>0.097131</td>\n",
              "      <td>0.301111</td>\n",
              "      <td>1.419254</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>0.908585</td>\n",
              "      <td>-0.397579</td>\n",
              "      <td>0.085268</td>\n",
              "      <td>-0.516057</td>\n",
              "      <td>-0.187872</td>\n",
              "      <td>-0.248665</td>\n",
              "      <td>-0.131968</td>\n",
              "      <td>0.967695</td>\n",
              "      <td>-0.228575</td>\n",
              "      <td>-0.267093</td>\n",
              "      <td>0.997576</td>\n",
              "      <td>-0.020509</td>\n",
              "      <td>0.800026</td>\n",
              "      <td>1.733087</td>\n",
              "      <td>0.290181</td>\n",
              "      <td>-0.399607</td>\n",
              "      <td>-1.376858</td>\n",
              "      <td>0.607510</td>\n",
              "      <td>1.096346</td>\n",
              "      <td>-0.271043</td>\n",
              "      <td>-0.573890</td>\n",
              "      <td>-1.779372</td>\n",
              "      <td>-0.266190</td>\n",
              "      <td>0.586733</td>\n",
              "      <td>0.407999</td>\n",
              "      <td>0.323659</td>\n",
              "      <td>0.412765</td>\n",
              "      <td>0.249252</td>\n",
              "      <td>-0.779486</td>\n",
              "      <td>-0.196424</td>\n",
              "      <td>0.106404</td>\n",
              "      <td>-0.522898</td>\n",
              "      <td>0.115593</td>\n",
              "      <td>1.293450</td>\n",
              "      <td>0.490224</td>\n",
              "      <td>-0.524354</td>\n",
              "      <td>-0.033636</td>\n",
              "      <td>0.801025</td>\n",
              "      <td>0.356241</td>\n",
              "      <td>0.119167</td>\n",
              "      <td>-0.221040</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>-0.677990</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>-0.481827</td>\n",
              "      <td>0.539787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2304</th>\n",
              "      <td>1.008198</td>\n",
              "      <td>0.443386</td>\n",
              "      <td>1.131482</td>\n",
              "      <td>0.629072</td>\n",
              "      <td>1.229246</td>\n",
              "      <td>1.300478</td>\n",
              "      <td>0.074535</td>\n",
              "      <td>0.097131</td>\n",
              "      <td>0.255770</td>\n",
              "      <td>-0.313209</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>-0.658143</td>\n",
              "      <td>-0.397579</td>\n",
              "      <td>-0.292012</td>\n",
              "      <td>-0.523231</td>\n",
              "      <td>-0.246118</td>\n",
              "      <td>-0.201033</td>\n",
              "      <td>0.914727</td>\n",
              "      <td>0.694983</td>\n",
              "      <td>1.679816</td>\n",
              "      <td>0.439322</td>\n",
              "      <td>-0.483214</td>\n",
              "      <td>-0.020509</td>\n",
              "      <td>-1.034851</td>\n",
              "      <td>-1.966057</td>\n",
              "      <td>-1.450136</td>\n",
              "      <td>-0.399607</td>\n",
              "      <td>-1.717267</td>\n",
              "      <td>-1.556497</td>\n",
              "      <td>-0.020956</td>\n",
              "      <td>-0.271043</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>0.344005</td>\n",
              "      <td>-1.096644</td>\n",
              "      <td>-1.534870</td>\n",
              "      <td>-0.409924</td>\n",
              "      <td>-0.331758</td>\n",
              "      <td>-1.195244</td>\n",
              "      <td>-0.262579</td>\n",
              "      <td>2.011302</td>\n",
              "      <td>-0.196424</td>\n",
              "      <td>0.776080</td>\n",
              "      <td>-0.522898</td>\n",
              "      <td>-0.524867</td>\n",
              "      <td>-0.157339</td>\n",
              "      <td>-0.956198</td>\n",
              "      <td>0.087880</td>\n",
              "      <td>-0.554518</td>\n",
              "      <td>0.269212</td>\n",
              "      <td>-0.233535</td>\n",
              "      <td>0.274914</td>\n",
              "      <td>0.156675</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>-0.677990</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>2.075128</td>\n",
              "      <td>0.539787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6168</th>\n",
              "      <td>1.008198</td>\n",
              "      <td>0.443386</td>\n",
              "      <td>2.046347</td>\n",
              "      <td>0.398544</td>\n",
              "      <td>1.229246</td>\n",
              "      <td>1.864296</td>\n",
              "      <td>-1.727672</td>\n",
              "      <td>-0.635368</td>\n",
              "      <td>-0.119396</td>\n",
              "      <td>0.264278</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>0.908585</td>\n",
              "      <td>-0.397579</td>\n",
              "      <td>0.256760</td>\n",
              "      <td>-0.372571</td>\n",
              "      <td>-0.042256</td>\n",
              "      <td>0.431800</td>\n",
              "      <td>-0.131968</td>\n",
              "      <td>-0.300731</td>\n",
              "      <td>0.831642</td>\n",
              "      <td>-0.391599</td>\n",
              "      <td>-0.483214</td>\n",
              "      <td>-0.020509</td>\n",
              "      <td>-1.821226</td>\n",
              "      <td>-0.486399</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>1.210903</td>\n",
              "      <td>1.006004</td>\n",
              "      <td>1.225797</td>\n",
              "      <td>1.375671</td>\n",
              "      <td>-0.008945</td>\n",
              "      <td>-0.573890</td>\n",
              "      <td>-0.009891</td>\n",
              "      <td>0.287446</td>\n",
              "      <td>1.495992</td>\n",
              "      <td>-0.000963</td>\n",
              "      <td>0.979076</td>\n",
              "      <td>0.412765</td>\n",
              "      <td>0.249252</td>\n",
              "      <td>1.250178</td>\n",
              "      <td>1.910670</td>\n",
              "      <td>-0.312143</td>\n",
              "      <td>-0.522898</td>\n",
              "      <td>-0.524867</td>\n",
              "      <td>1.583608</td>\n",
              "      <td>-1.599052</td>\n",
              "      <td>-0.986417</td>\n",
              "      <td>0.585233</td>\n",
              "      <td>-1.001732</td>\n",
              "      <td>-0.626718</td>\n",
              "      <td>0.727554</td>\n",
              "      <td>1.160846</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>-0.677990</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>2.075128</td>\n",
              "      <td>-1.852308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1179</th>\n",
              "      <td>-0.991723</td>\n",
              "      <td>0.443386</td>\n",
              "      <td>1.314455</td>\n",
              "      <td>-0.260109</td>\n",
              "      <td>1.229246</td>\n",
              "      <td>-0.109064</td>\n",
              "      <td>0.074535</td>\n",
              "      <td>0.097131</td>\n",
              "      <td>-1.508353</td>\n",
              "      <td>0.264278</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>-0.658143</td>\n",
              "      <td>2.514856</td>\n",
              "      <td>-0.171969</td>\n",
              "      <td>1.062289</td>\n",
              "      <td>-0.770334</td>\n",
              "      <td>-0.418781</td>\n",
              "      <td>0.701388</td>\n",
              "      <td>-0.300731</td>\n",
              "      <td>-0.864706</td>\n",
              "      <td>1.731157</td>\n",
              "      <td>-0.976810</td>\n",
              "      <td>-1.947967</td>\n",
              "      <td>1.586401</td>\n",
              "      <td>0.993258</td>\n",
              "      <td>1.160339</td>\n",
              "      <td>-0.399607</td>\n",
              "      <td>-0.696041</td>\n",
              "      <td>-0.629065</td>\n",
              "      <td>-0.020956</td>\n",
              "      <td>0.253153</td>\n",
              "      <td>0.287642</td>\n",
              "      <td>-0.009891</td>\n",
              "      <td>1.117900</td>\n",
              "      <td>-0.322525</td>\n",
              "      <td>0.407999</td>\n",
              "      <td>0.979076</td>\n",
              "      <td>0.010763</td>\n",
              "      <td>0.249252</td>\n",
              "      <td>-0.018362</td>\n",
              "      <td>-0.196424</td>\n",
              "      <td>-1.484076</td>\n",
              "      <td>1.821792</td>\n",
              "      <td>1.396513</td>\n",
              "      <td>-0.012260</td>\n",
              "      <td>0.329511</td>\n",
              "      <td>-0.697627</td>\n",
              "      <td>1.616682</td>\n",
              "      <td>-0.438371</td>\n",
              "      <td>-0.080630</td>\n",
              "      <td>0.119167</td>\n",
              "      <td>0.009274</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>1.474731</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>-0.481827</td>\n",
              "      <td>0.539787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6494</th>\n",
              "      <td>-0.991723</td>\n",
              "      <td>0.443386</td>\n",
              "      <td>0.765536</td>\n",
              "      <td>-0.095445</td>\n",
              "      <td>-0.813387</td>\n",
              "      <td>0.736661</td>\n",
              "      <td>0.074535</td>\n",
              "      <td>-1.866160</td>\n",
              "      <td>0.108432</td>\n",
              "      <td>0.264278</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>0.908585</td>\n",
              "      <td>-0.397579</td>\n",
              "      <td>-0.343460</td>\n",
              "      <td>0.201373</td>\n",
              "      <td>-0.624718</td>\n",
              "      <td>-0.418781</td>\n",
              "      <td>0.701388</td>\n",
              "      <td>0.809142</td>\n",
              "      <td>1.043685</td>\n",
              "      <td>-0.128951</td>\n",
              "      <td>0.010383</td>\n",
              "      <td>-0.020509</td>\n",
              "      <td>1.848526</td>\n",
              "      <td>0.253430</td>\n",
              "      <td>0.580234</td>\n",
              "      <td>0.003021</td>\n",
              "      <td>-0.696041</td>\n",
              "      <td>-0.629065</td>\n",
              "      <td>-0.300281</td>\n",
              "      <td>1.563643</td>\n",
              "      <td>0.861997</td>\n",
              "      <td>-2.487165</td>\n",
              "      <td>-0.819826</td>\n",
              "      <td>-1.231784</td>\n",
              "      <td>-0.409924</td>\n",
              "      <td>-1.314884</td>\n",
              "      <td>-1.195244</td>\n",
              "      <td>-0.262579</td>\n",
              "      <td>1.757594</td>\n",
              "      <td>1.314240</td>\n",
              "      <td>-0.061015</td>\n",
              "      <td>-0.384975</td>\n",
              "      <td>-1.325442</td>\n",
              "      <td>0.568055</td>\n",
              "      <td>0.008084</td>\n",
              "      <td>0.053226</td>\n",
              "      <td>-0.549361</td>\n",
              "      <td>-0.438371</td>\n",
              "      <td>0.028588</td>\n",
              "      <td>0.727554</td>\n",
              "      <td>0.239589</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>-0.677990</td>\n",
              "      <td>2.212511</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>-0.481827</td>\n",
              "      <td>0.539787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5704</th>\n",
              "      <td>-0.991723</td>\n",
              "      <td>0.443386</td>\n",
              "      <td>-1.430139</td>\n",
              "      <td>-0.161311</td>\n",
              "      <td>-0.813387</td>\n",
              "      <td>0.736661</td>\n",
              "      <td>0.074535</td>\n",
              "      <td>0.097131</td>\n",
              "      <td>-0.654981</td>\n",
              "      <td>1.996741</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>0.908585</td>\n",
              "      <td>2.514856</td>\n",
              "      <td>-0.600697</td>\n",
              "      <td>0.344859</td>\n",
              "      <td>-0.042256</td>\n",
              "      <td>0.431800</td>\n",
              "      <td>1.534744</td>\n",
              "      <td>-1.093497</td>\n",
              "      <td>0.407555</td>\n",
              "      <td>0.418380</td>\n",
              "      <td>0.503980</td>\n",
              "      <td>-0.020509</td>\n",
              "      <td>1.324276</td>\n",
              "      <td>-0.239790</td>\n",
              "      <td>-0.579978</td>\n",
              "      <td>-0.802234</td>\n",
              "      <td>-1.036449</td>\n",
              "      <td>-1.247353</td>\n",
              "      <td>-1.417582</td>\n",
              "      <td>1.039447</td>\n",
              "      <td>0.574820</td>\n",
              "      <td>-0.717684</td>\n",
              "      <td>-1.096644</td>\n",
              "      <td>-2.141042</td>\n",
              "      <td>-1.636807</td>\n",
              "      <td>-0.659466</td>\n",
              "      <td>0.814767</td>\n",
              "      <td>-0.006663</td>\n",
              "      <td>0.235346</td>\n",
              "      <td>-0.196424</td>\n",
              "      <td>-4.497618</td>\n",
              "      <td>1.959715</td>\n",
              "      <td>1.076283</td>\n",
              "      <td>2.018844</td>\n",
              "      <td>1.454506</td>\n",
              "      <td>-3.296734</td>\n",
              "      <td>1.100958</td>\n",
              "      <td>1.815076</td>\n",
              "      <td>1.011547</td>\n",
              "      <td>1.944327</td>\n",
              "      <td>0.585060</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>-0.677990</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>-0.481827</td>\n",
              "      <td>-1.852308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5870</th>\n",
              "      <td>1.008198</td>\n",
              "      <td>0.443386</td>\n",
              "      <td>-1.064193</td>\n",
              "      <td>0.036285</td>\n",
              "      <td>-0.813387</td>\n",
              "      <td>-0.109064</td>\n",
              "      <td>0.074535</td>\n",
              "      <td>-2.478736</td>\n",
              "      <td>-0.081679</td>\n",
              "      <td>-0.313209</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>-0.207826</td>\n",
              "      <td>-0.397579</td>\n",
              "      <td>-0.686442</td>\n",
              "      <td>0.057887</td>\n",
              "      <td>-0.479103</td>\n",
              "      <td>-0.248665</td>\n",
              "      <td>0.701388</td>\n",
              "      <td>1.284801</td>\n",
              "      <td>-0.228575</td>\n",
              "      <td>-1.379418</td>\n",
              "      <td>0.997576</td>\n",
              "      <td>-0.020509</td>\n",
              "      <td>-1.296976</td>\n",
              "      <td>-1.226228</td>\n",
              "      <td>-0.579978</td>\n",
              "      <td>0.808275</td>\n",
              "      <td>-2.057676</td>\n",
              "      <td>-1.865640</td>\n",
              "      <td>-1.696908</td>\n",
              "      <td>-1.319435</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>-2.487165</td>\n",
              "      <td>0.287446</td>\n",
              "      <td>-0.928697</td>\n",
              "      <td>0.407999</td>\n",
              "      <td>-2.953426</td>\n",
              "      <td>-1.195244</td>\n",
              "      <td>0.505168</td>\n",
              "      <td>-1.286902</td>\n",
              "      <td>-0.973524</td>\n",
              "      <td>-0.730691</td>\n",
              "      <td>1.683869</td>\n",
              "      <td>-0.204637</td>\n",
              "      <td>1.438529</td>\n",
              "      <td>-2.724047</td>\n",
              "      <td>-0.466596</td>\n",
              "      <td>0.894668</td>\n",
              "      <td>-0.325698</td>\n",
              "      <td>0.137805</td>\n",
              "      <td>-1.705993</td>\n",
              "      <td>1.736631</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>-0.677990</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>-0.481827</td>\n",
              "      <td>0.539787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>1.008198</td>\n",
              "      <td>-2.255038</td>\n",
              "      <td>-1.430139</td>\n",
              "      <td>0.200948</td>\n",
              "      <td>-0.813387</td>\n",
              "      <td>-0.109064</td>\n",
              "      <td>0.074535</td>\n",
              "      <td>0.097131</td>\n",
              "      <td>0.254727</td>\n",
              "      <td>-0.313209</td>\n",
              "      <td>-4.243097</td>\n",
              "      <td>-0.658143</td>\n",
              "      <td>-0.397579</td>\n",
              "      <td>-0.171969</td>\n",
              "      <td>-0.372571</td>\n",
              "      <td>-0.042256</td>\n",
              "      <td>0.431800</td>\n",
              "      <td>-0.965324</td>\n",
              "      <td>-0.300731</td>\n",
              "      <td>0.831642</td>\n",
              "      <td>-1.061077</td>\n",
              "      <td>0.997576</td>\n",
              "      <td>-0.020509</td>\n",
              "      <td>-0.248475</td>\n",
              "      <td>-0.979618</td>\n",
              "      <td>0.290181</td>\n",
              "      <td>0.405648</td>\n",
              "      <td>-0.015223</td>\n",
              "      <td>-0.010778</td>\n",
              "      <td>0.817020</td>\n",
              "      <td>0.777349</td>\n",
              "      <td>0.861997</td>\n",
              "      <td>0.344005</td>\n",
              "      <td>-0.819826</td>\n",
              "      <td>-0.322525</td>\n",
              "      <td>-0.000963</td>\n",
              "      <td>-0.659466</td>\n",
              "      <td>0.010763</td>\n",
              "      <td>1.784746</td>\n",
              "      <td>0.235346</td>\n",
              "      <td>-0.196424</td>\n",
              "      <td>-0.814400</td>\n",
              "      <td>0.856331</td>\n",
              "      <td>-0.684982</td>\n",
              "      <td>-0.157339</td>\n",
              "      <td>0.008084</td>\n",
              "      <td>-1.852786</td>\n",
              "      <td>0.585233</td>\n",
              "      <td>1.251714</td>\n",
              "      <td>1.011547</td>\n",
              "      <td>-0.489219</td>\n",
              "      <td>0.009274</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>-0.677990</td>\n",
              "      <td>2.212511</td>\n",
              "      <td>2.186300</td>\n",
              "      <td>2.075128</td>\n",
              "      <td>0.539787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7868</th>\n",
              "      <td>1.008198</td>\n",
              "      <td>0.443386</td>\n",
              "      <td>-0.332301</td>\n",
              "      <td>0.727870</td>\n",
              "      <td>-0.813387</td>\n",
              "      <td>0.172844</td>\n",
              "      <td>-2.925683</td>\n",
              "      <td>-1.996745</td>\n",
              "      <td>-0.773028</td>\n",
              "      <td>0.264278</td>\n",
              "      <td>0.172114</td>\n",
              "      <td>0.908585</td>\n",
              "      <td>-0.397579</td>\n",
              "      <td>-0.514951</td>\n",
              "      <td>-0.229085</td>\n",
              "      <td>-0.042256</td>\n",
              "      <td>-0.248665</td>\n",
              "      <td>0.868059</td>\n",
              "      <td>0.650588</td>\n",
              "      <td>-0.864706</td>\n",
              "      <td>0.665147</td>\n",
              "      <td>-0.976810</td>\n",
              "      <td>-0.662995</td>\n",
              "      <td>0.800026</td>\n",
              "      <td>0.993258</td>\n",
              "      <td>1.450392</td>\n",
              "      <td>0.003021</td>\n",
              "      <td>-0.355632</td>\n",
              "      <td>-0.629065</td>\n",
              "      <td>-0.020956</td>\n",
              "      <td>-0.533141</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>-1.425476</td>\n",
              "      <td>0.841082</td>\n",
              "      <td>-0.625611</td>\n",
              "      <td>-0.818885</td>\n",
              "      <td>-0.659466</td>\n",
              "      <td>-0.793242</td>\n",
              "      <td>1.272915</td>\n",
              "      <td>-0.018362</td>\n",
              "      <td>-0.196424</td>\n",
              "      <td>0.357533</td>\n",
              "      <td>0.304640</td>\n",
              "      <td>-1.325442</td>\n",
              "      <td>0.132819</td>\n",
              "      <td>-0.474057</td>\n",
              "      <td>-0.351080</td>\n",
              "      <td>0.688378</td>\n",
              "      <td>-0.889060</td>\n",
              "      <td>-0.080630</td>\n",
              "      <td>0.849231</td>\n",
              "      <td>0.124431</td>\n",
              "      <td>-0.095932</td>\n",
              "      <td>1.474731</td>\n",
              "      <td>-0.451909</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>-0.481827</td>\n",
              "      <td>-1.852308</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6798 rows × 58 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4050fafc-fe0e-4244-9c89-1b99351ad1a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4050fafc-fe0e-4244-9c89-1b99351ad1a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4050fafc-fe0e-4244-9c89-1b99351ad1a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        gender    condtn     order  ...  imprelig_m  career_c_m  masters_m\n",
              "6954 -0.991723 -2.255038 -0.149328  ...   -0.457326   -0.481827  -1.852308\n",
              "8929  1.008198  0.443386 -0.332301  ...   -0.457326   -0.481827   0.539787\n",
              "2304  1.008198  0.443386  1.131482  ...   -0.457326    2.075128   0.539787\n",
              "6168  1.008198  0.443386  2.046347  ...   -0.457326    2.075128  -1.852308\n",
              "1179 -0.991723  0.443386  1.314455  ...   -0.457326   -0.481827   0.539787\n",
              "...        ...       ...       ...  ...         ...         ...        ...\n",
              "6494 -0.991723  0.443386  0.765536  ...   -0.457326   -0.481827   0.539787\n",
              "5704 -0.991723  0.443386 -1.430139  ...   -0.457326   -0.481827  -1.852308\n",
              "5870  1.008198  0.443386 -1.064193  ...   -0.457326   -0.481827   0.539787\n",
              "924   1.008198 -2.255038 -1.430139  ...    2.186300    2.075128   0.539787\n",
              "7868  1.008198  0.443386 -0.332301  ...   -0.457326   -0.481827  -1.852308\n",
              "\n",
              "[6798 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHZs4vYJwxC3"
      },
      "source": [
        "# Drop the column with very high VIF\n",
        "\n",
        "fun_X_train = fun_X_train.drop(columns= ['(1_1-2_1_o)_att'])\n",
        "fun_X_test= fun_X_test.drop(columns= ['(1_1-2_1_o)_att'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV1Byfy-2dMy"
      },
      "source": [
        "fun_X_train = sm.add_constant(fun_X_train)\n",
        "fun_model2 = sm.OLS(fun_y_train, fun_X_train).fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNkFmYUV3W1f",
        "outputId": "606cb97d-899a-4a17-9b30-8c909034a89d"
      },
      "source": [
        "print(fun_model2.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  fun_o   R-squared:                       0.085\n",
            "Model:                            OLS   Adj. R-squared:                  0.077\n",
            "Method:                 Least Squares   F-statistic:                     10.94\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           2.40e-91\n",
            "Time:                        23:11:54   Log-Likelihood:                -13922.\n",
            "No. Observations:                6798   AIC:                         2.796e+04\n",
            "Df Residuals:                    6740   BIC:                         2.836e+04\n",
            "Df Model:                          57                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.3748      0.023    279.024      0.000       6.330       6.420\n",
            "gender               -0.1718      0.039     -4.394      0.000      -0.249      -0.095\n",
            "condtn                0.0121      0.025      0.483      0.629      -0.037       0.061\n",
            "order                -0.0863      0.024     -3.569      0.000      -0.134      -0.039\n",
            "int_corr              0.0486      0.024      2.058      0.040       0.002       0.095\n",
            "samerace              0.0161      0.023      0.693      0.488      -0.029       0.062\n",
            "age_o                -0.0599      0.033     -1.834      0.067      -0.124       0.004\n",
            "mn_sat_o           1.843e-06      0.030   6.12e-05      1.000      -0.059       0.059\n",
            "tuition_o            -0.0468      0.030     -1.548      0.122      -0.106       0.012\n",
            "income               -0.0242      0.033     -0.737      0.461      -0.089       0.040\n",
            "exphappy_o            0.1884      0.025      7.586      0.000       0.140       0.237\n",
            "met_o                -0.2093      0.023     -8.932      0.000      -0.255      -0.163\n",
            "world_rank_o         -0.1268      0.031     -4.036      0.000      -0.188      -0.065\n",
            "masters_o            -0.1069      0.030     -3.546      0.000      -0.166      -0.048\n",
            "attr1_1              -0.2750      0.133     -2.063      0.039      -0.536      -0.014\n",
            "sinc1_1              -0.2889      0.086     -3.376      0.001      -0.457      -0.121\n",
            "intel1_1             -0.1076      0.087     -1.242      0.214      -0.277       0.062\n",
            "fun1_1               -0.0834      0.073     -1.135      0.256      -0.227       0.061\n",
            "amb1_1               -0.1308      0.071     -1.841      0.066      -0.270       0.009\n",
            "shar1_1              -0.3655      0.079     -4.602      0.000      -0.521      -0.210\n",
            "age_diff              0.0911      0.033      2.756      0.006       0.026       0.156\n",
            "income_diff          -0.0181      0.033     -0.542      0.588      -0.084       0.047\n",
            "date_diff             0.0208      0.026      0.813      0.416      -0.029       0.071\n",
            "go_out_diff           0.1073      0.026      4.182      0.000       0.057       0.158\n",
            "sports_diff          -0.0161      0.033     -0.491      0.624      -0.081       0.048\n",
            "tvsport_diff          0.0407      0.030      1.354      0.176      -0.018       0.100\n",
            "exercise_diff        -0.0631      0.027     -2.359      0.018      -0.116      -0.011\n",
            "dining_diff          -0.0786      0.028     -2.767      0.006      -0.134      -0.023\n",
            "museums_diff          0.0832      0.049      1.705      0.088      -0.012       0.179\n",
            "art_diff              0.0076      0.047      0.163      0.871      -0.084       0.099\n",
            "hiking_diff          -0.0300      0.027     -1.111      0.267      -0.083       0.023\n",
            "gaming_diff           0.0480      0.028      1.726      0.084      -0.007       0.102\n",
            "clubbing_diff        -0.0414      0.025     -1.669      0.095      -0.090       0.007\n",
            "reading_diff          0.0687      0.026      2.684      0.007       0.019       0.119\n",
            "tv_diff               0.0603      0.031      1.966      0.049       0.000       0.120\n",
            "theater_diff          0.0595      0.035      1.716      0.086      -0.008       0.128\n",
            "movies_diff           0.0364      0.030      1.208      0.227      -0.023       0.096\n",
            "concerts_diff         0.0282      0.035      0.809      0.419      -0.040       0.096\n",
            "music_diff            0.0243      0.032      0.767      0.443      -0.038       0.086\n",
            "shopping_diff         0.0840      0.033      2.551      0.011       0.019       0.149\n",
            "yoga_diff            -0.0307      0.026     -1.160      0.246      -0.083       0.021\n",
            "worldrank_diff        0.0490      0.031      1.595      0.111      -0.011       0.109\n",
            "(3_1-pf_o)_att       -0.0442      0.043     -1.033      0.302      -0.128       0.040\n",
            "(3_1-pf_o)_sinc      -0.0098      0.031     -0.312      0.755      -0.071       0.052\n",
            "(3_1-pf_o)_fun        0.0605      0.029      2.110      0.035       0.004       0.117\n",
            "(3_1-pf_o)_intel      0.1135      0.031      3.607      0.000       0.052       0.175\n",
            "(3_1-pf_o)_amb       -0.0814      0.032     -2.565      0.010      -0.144      -0.019\n",
            "(1_1-2_1_o)_sinc      0.0159      0.038      0.417      0.677      -0.059       0.091\n",
            "(1_1-2_1_o)_fun       0.0224      0.034      0.651      0.515      -0.045       0.090\n",
            "(1_1-2_1_o)_intel    -0.1143      0.038     -3.038      0.002      -0.188      -0.041\n",
            "(1_1-2_1_o)_amb       0.0833      0.032      2.581      0.010       0.020       0.147\n",
            "(1_1-2_1_o)_shar      0.0387      0.037      1.049      0.294      -0.034       0.111\n",
            "from_m               -0.0182      0.023     -0.793      0.428      -0.063       0.027\n",
            "goal_m                0.0285      0.023      1.237      0.216      -0.017       0.074\n",
            "imprace_m            -0.0417      0.023     -1.775      0.076      -0.088       0.004\n",
            "imprelig_m            0.0358      0.024      1.520      0.129      -0.010       0.082\n",
            "career_c_m            0.0219      0.024      0.920      0.358      -0.025       0.069\n",
            "masters_m            -0.0675      0.029     -2.287      0.022      -0.125      -0.010\n",
            "==============================================================================\n",
            "Omnibus:                      207.516   Durbin-Watson:                   2.040\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              226.297\n",
            "Skew:                          -0.438   Prob(JB):                     7.25e-50\n",
            "Kurtosis:                       3.182   Cond. No.                         20.6\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmspmGQ2x420",
        "outputId": "e39a5296-9caa-4fab-c258-849b9ff84d06"
      },
      "source": [
        "#Check VIF values again\n",
        "fun_cols = fun_X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(fun_X_train, fun_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "attr1_1              34.034627\n",
              "intel1_1             14.376196\n",
              "sinc1_1              14.030403\n",
              "shar1_1              12.082781\n",
              "fun1_1               10.344490\n",
              "amb1_1                9.675734\n",
              "museums_diff          4.558316\n",
              "art_diff              4.187856\n",
              "(3_1-pf_o)_att        3.499852\n",
              "gender                2.930358\n",
              "(1_1-2_1_o)_sinc      2.790377\n",
              "(1_1-2_1_o)_intel     2.709186\n",
              "(1_1-2_1_o)_shar      2.614627\n",
              "concerts_diff         2.321634\n",
              "theater_diff          2.304143\n",
              "(1_1-2_1_o)_fun       2.276835\n",
              "income_diff           2.142464\n",
              "age_diff              2.090607\n",
              "shopping_diff         2.076781\n",
              "income                2.072077\n",
              "sports_diff           2.067187\n",
              "age_o                 2.046327\n",
              "(1_1-2_1_o)_amb       1.993502\n",
              "(3_1-pf_o)_amb        1.931128\n",
              "music_diff            1.920812\n",
              "(3_1-pf_o)_intel      1.895623\n",
              "world_rank_o          1.891305\n",
              "(3_1-pf_o)_sinc       1.876479\n",
              "worldrank_diff        1.803226\n",
              "tv_diff               1.799644\n",
              "tuition_o             1.752823\n",
              "masters_o             1.741245\n",
              "movies_diff           1.741238\n",
              "mn_sat_o              1.735920\n",
              "tvsport_diff          1.730789\n",
              "masters_m             1.666455\n",
              "(3_1-pf_o)_fun        1.576624\n",
              "dining_diff           1.545161\n",
              "gaming_diff           1.480753\n",
              "hiking_diff           1.395143\n",
              "exercise_diff         1.371907\n",
              "yoga_diff             1.339215\n",
              "go_out_diff           1.260125\n",
              "reading_diff          1.253068\n",
              "date_diff             1.249611\n",
              "condtn                1.200983\n",
              "exphappy_o            1.181002\n",
              "clubbing_diff         1.176045\n",
              "order                 1.120642\n",
              "career_c_m            1.086488\n",
              "int_corr              1.067582\n",
              "imprelig_m            1.061461\n",
              "imprace_m             1.057827\n",
              "met_o                 1.051465\n",
              "samerace              1.031464\n",
              "goal_m                1.018137\n",
              "from_m                1.006907\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BcMtOB3g7Dp"
      },
      "source": [
        "# Removing the variable with the highest VIF\n",
        "\n",
        "fun_X_train = fun_X_train.drop(columns= ['attr1_1'])\n",
        "fun_X_test = fun_X_test.drop(columns= ['attr1_1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw4uVR3-hOdT",
        "outputId": "baf90fbc-5ec1-49c2-a454-7f87d370c284"
      },
      "source": [
        "fun_X_train = sm.add_constant(fun_X_train)\n",
        "fun_model2 = sm.OLS(fun_y_train, fun_X_train).fit()\n",
        "print(fun_model2.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  fun_o   R-squared:                       0.084\n",
            "Model:                            OLS   Adj. R-squared:                  0.076\n",
            "Method:                 Least Squares   F-statistic:                     11.05\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           4.98e-91\n",
            "Time:                        23:11:57   Log-Likelihood:                -13924.\n",
            "No. Observations:                6798   AIC:                         2.796e+04\n",
            "Df Residuals:                    6741   BIC:                         2.835e+04\n",
            "Df Model:                          56                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.3748      0.023    278.956      0.000       6.330       6.420\n",
            "gender               -0.1789      0.039     -4.592      0.000      -0.255      -0.103\n",
            "condtn                0.0152      0.025      0.609      0.542      -0.034       0.064\n",
            "order                -0.0864      0.024     -3.572      0.000      -0.134      -0.039\n",
            "int_corr              0.0482      0.024      2.042      0.041       0.002       0.095\n",
            "samerace              0.0145      0.023      0.626      0.532      -0.031       0.060\n",
            "age_o                -0.0589      0.033     -1.803      0.071      -0.123       0.005\n",
            "mn_sat_o             -0.0006      0.030     -0.020      0.984      -0.060       0.058\n",
            "tuition_o            -0.0466      0.030     -1.539      0.124      -0.106       0.013\n",
            "income               -0.0244      0.033     -0.740      0.459      -0.089       0.040\n",
            "exphappy_o            0.1876      0.025      7.555      0.000       0.139       0.236\n",
            "met_o                -0.2096      0.023     -8.946      0.000      -0.256      -0.164\n",
            "world_rank_o         -0.1285      0.031     -4.091      0.000      -0.190      -0.067\n",
            "masters_o            -0.1063      0.030     -3.525      0.000      -0.165      -0.047\n",
            "sinc1_1              -0.1297      0.037     -3.505      0.000      -0.202      -0.057\n",
            "intel1_1              0.0526      0.038      1.369      0.171      -0.023       0.128\n",
            "fun1_1                0.0511      0.034      1.505      0.132      -0.015       0.118\n",
            "amb1_1               -0.0024      0.034     -0.071      0.944      -0.070       0.065\n",
            "shar1_1              -0.2190      0.036     -6.157      0.000      -0.289      -0.149\n",
            "age_diff              0.0919      0.033      2.783      0.005       0.027       0.157\n",
            "income_diff          -0.0174      0.033     -0.520      0.603      -0.083       0.048\n",
            "date_diff             0.0201      0.026      0.786      0.432      -0.030       0.070\n",
            "go_out_diff           0.1083      0.026      4.221      0.000       0.058       0.159\n",
            "sports_diff          -0.0139      0.033     -0.424      0.672      -0.078       0.050\n",
            "tvsport_diff          0.0366      0.030      1.220      0.222      -0.022       0.095\n",
            "exercise_diff        -0.0629      0.027     -2.350      0.019      -0.115      -0.010\n",
            "dining_diff          -0.0814      0.028     -2.869      0.004      -0.137      -0.026\n",
            "museums_diff          0.0835      0.049      1.711      0.087      -0.012       0.179\n",
            "art_diff              0.0083      0.047      0.178      0.858      -0.083       0.100\n",
            "hiking_diff          -0.0291      0.027     -1.079      0.280      -0.082       0.024\n",
            "gaming_diff           0.0473      0.028      1.702      0.089      -0.007       0.102\n",
            "clubbing_diff        -0.0410      0.025     -1.656      0.098      -0.090       0.008\n",
            "reading_diff          0.0701      0.026      2.743      0.006       0.020       0.120\n",
            "tv_diff               0.0603      0.031      1.966      0.049       0.000       0.120\n",
            "theater_diff          0.0559      0.035      1.615      0.106      -0.012       0.124\n",
            "movies_diff           0.0346      0.030      1.147      0.252      -0.025       0.094\n",
            "concerts_diff         0.0315      0.035      0.906      0.365      -0.037       0.100\n",
            "music_diff            0.0215      0.032      0.681      0.496      -0.040       0.084\n",
            "shopping_diff         0.0874      0.033      2.658      0.008       0.023       0.152\n",
            "yoga_diff            -0.0318      0.026     -1.201      0.230      -0.084       0.020\n",
            "worldrank_diff        0.0500      0.031      1.629      0.103      -0.010       0.110\n",
            "(3_1-pf_o)_att       -0.0434      0.043     -1.015      0.310      -0.127       0.040\n",
            "(3_1-pf_o)_sinc      -0.0099      0.031     -0.317      0.751      -0.071       0.051\n",
            "(3_1-pf_o)_fun        0.0598      0.029      2.084      0.037       0.004       0.116\n",
            "(3_1-pf_o)_intel      0.1151      0.031      3.660      0.000       0.053       0.177\n",
            "(3_1-pf_o)_amb       -0.0826      0.032     -2.602      0.009      -0.145      -0.020\n",
            "(1_1-2_1_o)_sinc      0.0154      0.038      0.403      0.687      -0.059       0.090\n",
            "(1_1-2_1_o)_fun       0.0224      0.034      0.650      0.516      -0.045       0.090\n",
            "(1_1-2_1_o)_intel    -0.1160      0.038     -3.084      0.002      -0.190      -0.042\n",
            "(1_1-2_1_o)_amb       0.0832      0.032      2.579      0.010       0.020       0.146\n",
            "(1_1-2_1_o)_shar      0.0364      0.037      0.986      0.324      -0.036       0.109\n",
            "from_m               -0.0187      0.023     -0.817      0.414      -0.064       0.026\n",
            "goal_m                0.0275      0.023      1.194      0.232      -0.018       0.073\n",
            "imprace_m            -0.0422      0.024     -1.797      0.072      -0.088       0.004\n",
            "imprelig_m            0.0352      0.024      1.497      0.134      -0.011       0.081\n",
            "career_c_m            0.0234      0.024      0.982      0.326      -0.023       0.070\n",
            "masters_m            -0.0680      0.030     -2.305      0.021      -0.126      -0.010\n",
            "==============================================================================\n",
            "Omnibus:                      206.762   Durbin-Watson:                   2.041\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              225.396\n",
            "Skew:                          -0.437   Prob(JB):                     1.14e-49\n",
            "Kurtosis:                       3.180   Cond. No.                         6.31\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81sV-4SchiY6",
        "outputId": "9f5461d5-9006-42d2-feda-f767c2d45054"
      },
      "source": [
        "#Check VIF values again\n",
        "fun_cols = fun_X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(fun_X_train, fun_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "museums_diff         4.558268\n",
              "art_diff             4.187613\n",
              "(3_1-pf_o)_att       3.499585\n",
              "gender               2.907653\n",
              "intel1_1             2.827218\n",
              "(1_1-2_1_o)_sinc     2.790252\n",
              "(1_1-2_1_o)_intel    2.707865\n",
              "sinc1_1              2.621246\n",
              "(1_1-2_1_o)_shar     2.612178\n",
              "shar1_1              2.422335\n",
              "concerts_diff        2.316618\n",
              "theater_diff         2.298353\n",
              "(1_1-2_1_o)_fun      2.276835\n",
              "amb1_1               2.256178\n",
              "fun1_1               2.204302\n",
              "income_diff          2.142234\n",
              "age_diff             2.090254\n",
              "income               2.072071\n",
              "shopping_diff        2.071447\n",
              "sports_diff          2.064987\n",
              "age_o                2.045868\n",
              "(1_1-2_1_o)_amb      1.993501\n",
              "(3_1-pf_o)_amb       1.930485\n",
              "music_diff           1.917421\n",
              "(3_1-pf_o)_intel     1.894392\n",
              "world_rank_o         1.889979\n",
              "(3_1-pf_o)_sinc      1.876467\n",
              "worldrank_diff       1.802740\n",
              "tv_diff              1.799644\n",
              "tuition_o            1.752791\n",
              "masters_o            1.741074\n",
              "movies_diff          1.739658\n",
              "mn_sat_o             1.735759\n",
              "tvsport_diff         1.723244\n",
              "masters_m            1.666321\n",
              "(3_1-pf_o)_fun       1.576382\n",
              "dining_diff          1.541582\n",
              "gaming_diff          1.480558\n",
              "hiking_diff          1.394824\n",
              "exercise_diff        1.371885\n",
              "yoga_diff            1.338669\n",
              "go_out_diff          1.259664\n",
              "reading_diff         1.252070\n",
              "date_diff            1.249397\n",
              "condtn               1.196529\n",
              "exphappy_o           1.180738\n",
              "clubbing_diff        1.175999\n",
              "order                1.120638\n",
              "career_c_m           1.085504\n",
              "int_corr             1.067517\n",
              "imprelig_m           1.061332\n",
              "imprace_m            1.057700\n",
              "met_o                1.051402\n",
              "samerace             1.030354\n",
              "goal_m               1.017696\n",
              "from_m               1.006765\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XClObY2ckCbX"
      },
      "source": [
        "# All variables have low VIF, lets remove the column with the high p value\n",
        "\n",
        "fun_X_train = fun_X_train.drop(columns= ['mn_sat_o'])\n",
        "fun_X_test = fun_X_test.drop(columns= ['mn_sat_o'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjHsYyMVg36H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59fj55fClu4J",
        "outputId": "3b908d3a-5305-4211-905a-acebeda61cd5"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "fun_X_train = sm.add_constant(fun_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "fun_model3 = sm.OLS(fun_y_train, fun_X_train).fit() #ordinary least square\n",
        "print(fun_model3.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  fun_o   R-squared:                       0.084\n",
            "Model:                            OLS   Adj. R-squared:                  0.077\n",
            "Method:                 Least Squares   F-statistic:                     11.25\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           1.48e-91\n",
            "Time:                        23:11:59   Log-Likelihood:                -13924.\n",
            "No. Observations:                6798   AIC:                         2.796e+04\n",
            "Df Residuals:                    6742   BIC:                         2.834e+04\n",
            "Df Model:                          55                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.3748      0.023    278.977      0.000       6.330       6.420\n",
            "gender               -0.1789      0.039     -4.595      0.000      -0.255      -0.103\n",
            "condtn                0.0152      0.025      0.609      0.543      -0.034       0.064\n",
            "order                -0.0864      0.024     -3.572      0.000      -0.134      -0.039\n",
            "int_corr              0.0482      0.024      2.042      0.041       0.002       0.094\n",
            "samerace              0.0145      0.023      0.626      0.531      -0.031       0.060\n",
            "age_o                -0.0589      0.033     -1.803      0.071      -0.123       0.005\n",
            "tuition_o            -0.0469      0.024     -1.932      0.053      -0.095       0.001\n",
            "income               -0.0243      0.033     -0.740      0.459      -0.089       0.040\n",
            "exphappy_o            0.1876      0.025      7.578      0.000       0.139       0.236\n",
            "met_o                -0.2096      0.023     -8.948      0.000      -0.256      -0.164\n",
            "world_rank_o         -0.1285      0.031     -4.093      0.000      -0.190      -0.067\n",
            "masters_o            -0.1063      0.030     -3.526      0.000      -0.165      -0.047\n",
            "sinc1_1              -0.1298      0.037     -3.516      0.000      -0.202      -0.057\n",
            "intel1_1              0.0526      0.038      1.369      0.171      -0.023       0.128\n",
            "fun1_1                0.0511      0.034      1.505      0.132      -0.015       0.118\n",
            "amb1_1               -0.0024      0.034     -0.070      0.944      -0.070       0.065\n",
            "shar1_1              -0.2190      0.036     -6.159      0.000      -0.289      -0.149\n",
            "age_diff              0.0919      0.033      2.783      0.005       0.027       0.157\n",
            "income_diff          -0.0174      0.033     -0.520      0.603      -0.083       0.048\n",
            "date_diff             0.0201      0.026      0.786      0.432      -0.030       0.070\n",
            "go_out_diff           0.1083      0.026      4.221      0.000       0.058       0.159\n",
            "sports_diff          -0.0139      0.033     -0.425      0.671      -0.078       0.050\n",
            "tvsport_diff          0.0366      0.030      1.222      0.222      -0.022       0.095\n",
            "exercise_diff        -0.0629      0.027     -2.352      0.019      -0.115      -0.010\n",
            "dining_diff          -0.0814      0.028     -2.870      0.004      -0.137      -0.026\n",
            "museums_diff          0.0835      0.049      1.714      0.087      -0.012       0.179\n",
            "art_diff              0.0083      0.047      0.178      0.859      -0.083       0.100\n",
            "hiking_diff          -0.0291      0.027     -1.080      0.280      -0.082       0.024\n",
            "gaming_diff           0.0473      0.028      1.702      0.089      -0.007       0.102\n",
            "clubbing_diff        -0.0410      0.025     -1.657      0.098      -0.090       0.008\n",
            "reading_diff          0.0701      0.026      2.743      0.006       0.020       0.120\n",
            "tv_diff               0.0603      0.031      1.968      0.049       0.000       0.120\n",
            "theater_diff          0.0560      0.035      1.615      0.106      -0.012       0.124\n",
            "movies_diff           0.0346      0.030      1.149      0.251      -0.024       0.094\n",
            "concerts_diff         0.0315      0.035      0.906      0.365      -0.037       0.100\n",
            "music_diff            0.0215      0.032      0.680      0.496      -0.040       0.084\n",
            "shopping_diff         0.0874      0.033      2.659      0.008       0.023       0.152\n",
            "yoga_diff            -0.0317      0.026     -1.204      0.229      -0.083       0.020\n",
            "worldrank_diff        0.0500      0.031      1.629      0.103      -0.010       0.110\n",
            "(3_1-pf_o)_att       -0.0434      0.043     -1.015      0.310      -0.127       0.040\n",
            "(3_1-pf_o)_sinc      -0.0100      0.031     -0.321      0.749      -0.071       0.051\n",
            "(3_1-pf_o)_fun        0.0598      0.029      2.084      0.037       0.004       0.116\n",
            "(3_1-pf_o)_intel      0.1151      0.031      3.662      0.000       0.053       0.177\n",
            "(3_1-pf_o)_amb       -0.0826      0.032     -2.604      0.009      -0.145      -0.020\n",
            "(1_1-2_1_o)_sinc      0.0154      0.038      0.406      0.685      -0.059       0.090\n",
            "(1_1-2_1_o)_fun       0.0224      0.034      0.650      0.516      -0.045       0.090\n",
            "(1_1-2_1_o)_intel    -0.1160      0.038     -3.084      0.002      -0.190      -0.042\n",
            "(1_1-2_1_o)_amb       0.0832      0.032      2.581      0.010       0.020       0.146\n",
            "(1_1-2_1_o)_shar      0.0364      0.037      0.986      0.324      -0.036       0.109\n",
            "from_m               -0.0187      0.023     -0.817      0.414      -0.064       0.026\n",
            "goal_m                0.0275      0.023      1.195      0.232      -0.018       0.073\n",
            "imprace_m            -0.0422      0.023     -1.797      0.072      -0.088       0.004\n",
            "imprelig_m            0.0352      0.024      1.497      0.134      -0.011       0.081\n",
            "career_c_m            0.0234      0.024      0.982      0.326      -0.023       0.070\n",
            "masters_m            -0.0680      0.029     -2.306      0.021      -0.126      -0.010\n",
            "==============================================================================\n",
            "Omnibus:                      206.759   Durbin-Watson:                   2.041\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              225.392\n",
            "Skew:                          -0.437   Prob(JB):                     1.14e-49\n",
            "Kurtosis:                       3.180   Cond. No.                         6.29\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT2N-HhHmR9V"
      },
      "source": [
        "# Remove columns with high p value\n",
        "\n",
        "fun_X_train = fun_X_train.drop(columns= ['amb1_1'])\n",
        "fun_X_test = fun_X_test.drop(columns= ['amb1_1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcCJ9oOsl4-M",
        "outputId": "bbb01d0b-b505-4cc6-9e3b-8e365ca92570"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "fun_X_train = sm.add_constant(fun_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "fun_model4 = sm.OLS(fun_y_train, fun_X_train).fit() #ordinary least square\n",
        "print(fun_model4.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  fun_o   R-squared:                       0.084\n",
            "Model:                            OLS   Adj. R-squared:                  0.077\n",
            "Method:                 Least Squares   F-statistic:                     11.46\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           4.34e-92\n",
            "Time:                        23:11:59   Log-Likelihood:                -13924.\n",
            "No. Observations:                6798   AIC:                         2.796e+04\n",
            "Df Residuals:                    6743   BIC:                         2.833e+04\n",
            "Df Model:                          54                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.3748      0.023    278.998      0.000       6.330       6.420\n",
            "gender               -0.1780      0.037     -4.854      0.000      -0.250      -0.106\n",
            "condtn                0.0153      0.025      0.611      0.541      -0.034       0.064\n",
            "order                -0.0864      0.024     -3.572      0.000      -0.134      -0.039\n",
            "int_corr              0.0482      0.024      2.041      0.041       0.002       0.094\n",
            "samerace              0.0145      0.023      0.627      0.531      -0.031       0.060\n",
            "age_o                -0.0589      0.033     -1.804      0.071      -0.123       0.005\n",
            "tuition_o            -0.0469      0.024     -1.931      0.054      -0.094       0.001\n",
            "income               -0.0243      0.033     -0.740      0.459      -0.089       0.040\n",
            "exphappy_o            0.1877      0.025      7.584      0.000       0.139       0.236\n",
            "met_o                -0.2096      0.023     -8.949      0.000      -0.256      -0.164\n",
            "world_rank_o         -0.1283      0.031     -4.106      0.000      -0.190      -0.067\n",
            "masters_o            -0.1062      0.030     -3.526      0.000      -0.165      -0.047\n",
            "sinc1_1              -0.1296      0.037     -3.518      0.000      -0.202      -0.057\n",
            "intel1_1              0.0525      0.038      1.368      0.171      -0.023       0.128\n",
            "fun1_1                0.0515      0.033      1.540      0.124      -0.014       0.117\n",
            "shar1_1              -0.2191      0.036     -6.164      0.000      -0.289      -0.149\n",
            "age_diff              0.0918      0.033      2.783      0.005       0.027       0.157\n",
            "income_diff          -0.0174      0.033     -0.521      0.602      -0.083       0.048\n",
            "date_diff             0.0202      0.025      0.796      0.426      -0.030       0.070\n",
            "go_out_diff           0.1082      0.026      4.221      0.000       0.058       0.158\n",
            "sports_diff          -0.0140      0.033     -0.426      0.670      -0.078       0.050\n",
            "tvsport_diff          0.0366      0.030      1.222      0.222      -0.022       0.095\n",
            "exercise_diff        -0.0629      0.027     -2.353      0.019      -0.115      -0.010\n",
            "dining_diff          -0.0814      0.028     -2.870      0.004      -0.137      -0.026\n",
            "museums_diff          0.0835      0.049      1.712      0.087      -0.012       0.179\n",
            "art_diff              0.0083      0.047      0.178      0.859      -0.083       0.100\n",
            "hiking_diff          -0.0291      0.027     -1.079      0.280      -0.082       0.024\n",
            "gaming_diff           0.0474      0.028      1.707      0.088      -0.007       0.102\n",
            "clubbing_diff        -0.0410      0.025     -1.655      0.098      -0.089       0.008\n",
            "reading_diff          0.0701      0.026      2.744      0.006       0.020       0.120\n",
            "tv_diff               0.0602      0.031      1.967      0.049       0.000       0.120\n",
            "theater_diff          0.0563      0.034      1.637      0.102      -0.011       0.124\n",
            "movies_diff           0.0345      0.030      1.147      0.251      -0.024       0.093\n",
            "concerts_diff         0.0314      0.035      0.905      0.366      -0.037       0.100\n",
            "music_diff            0.0216      0.032      0.682      0.495      -0.040       0.084\n",
            "shopping_diff         0.0875      0.033      2.660      0.008       0.023       0.152\n",
            "yoga_diff            -0.0317      0.026     -1.203      0.229      -0.083       0.020\n",
            "worldrank_diff        0.0498      0.031      1.628      0.104      -0.010       0.110\n",
            "(3_1-pf_o)_att       -0.0439      0.042     -1.043      0.297      -0.126       0.039\n",
            "(3_1-pf_o)_sinc      -0.0102      0.031     -0.327      0.743      -0.071       0.051\n",
            "(3_1-pf_o)_fun        0.0598      0.029      2.084      0.037       0.004       0.116\n",
            "(3_1-pf_o)_intel      0.1148      0.031      3.704      0.000       0.054       0.176\n",
            "(3_1-pf_o)_amb       -0.0825      0.032     -2.603      0.009      -0.145      -0.020\n",
            "(1_1-2_1_o)_sinc      0.0154      0.038      0.405      0.686      -0.059       0.090\n",
            "(1_1-2_1_o)_fun       0.0219      0.034      0.649      0.517      -0.044       0.088\n",
            "(1_1-2_1_o)_intel    -0.1158      0.037     -3.089      0.002      -0.189      -0.042\n",
            "(1_1-2_1_o)_amb       0.0817      0.024      3.364      0.001       0.034       0.129\n",
            "(1_1-2_1_o)_shar      0.0364      0.037      0.985      0.325      -0.036       0.109\n",
            "from_m               -0.0188      0.023     -0.818      0.413      -0.064       0.026\n",
            "goal_m                0.0275      0.023      1.194      0.233      -0.018       0.073\n",
            "imprace_m            -0.0422      0.023     -1.796      0.072      -0.088       0.004\n",
            "imprelig_m            0.0353      0.024      1.499      0.134      -0.011       0.081\n",
            "career_c_m            0.0232      0.024      0.980      0.327      -0.023       0.070\n",
            "masters_m            -0.0680      0.029     -2.305      0.021      -0.126      -0.010\n",
            "==============================================================================\n",
            "Omnibus:                      206.696   Durbin-Watson:                   2.041\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              225.318\n",
            "Skew:                          -0.437   Prob(JB):                     1.18e-49\n",
            "Kurtosis:                       3.180   Cond. No.                         6.23\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP1mIVMAoOzs"
      },
      "source": [
        "# Remove columns with high p value\n",
        "\n",
        "fun_X_train = fun_X_train.drop(columns= ['art_diff'])\n",
        "fun_X_test = fun_X_test.drop(columns= ['art_diff'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pvKlNTorl1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2762433-ebff-4b6e-e13d-97f4f5732718"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "fun_X_train = sm.add_constant(fun_X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "fun_model5 = sm.OLS(fun_y_train, fun_X_train).fit() #ordinary least square\n",
        "print(fun_model5.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  fun_o   R-squared:                       0.084\n",
            "Model:                            OLS   Adj. R-squared:                  0.077\n",
            "Method:                 Least Squares   F-statistic:                     11.68\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           1.28e-92\n",
            "Time:                        23:11:59   Log-Likelihood:                -13924.\n",
            "No. Observations:                6798   AIC:                         2.796e+04\n",
            "Df Residuals:                    6744   BIC:                         2.832e+04\n",
            "Df Model:                          53                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.3748      0.023    279.018      0.000       6.330       6.420\n",
            "gender               -0.1775      0.037     -4.856      0.000      -0.249      -0.106\n",
            "condtn                0.0153      0.025      0.614      0.540      -0.034       0.064\n",
            "order                -0.0864      0.024     -3.574      0.000      -0.134      -0.039\n",
            "int_corr              0.0482      0.024      2.043      0.041       0.002       0.094\n",
            "samerace              0.0145      0.023      0.626      0.531      -0.031       0.060\n",
            "age_o                -0.0590      0.033     -1.808      0.071      -0.123       0.005\n",
            "tuition_o            -0.0469      0.024     -1.930      0.054      -0.094       0.001\n",
            "income               -0.0242      0.033     -0.737      0.461      -0.089       0.040\n",
            "exphappy_o            0.1878      0.025      7.593      0.000       0.139       0.236\n",
            "met_o                -0.2097      0.023     -8.951      0.000      -0.256      -0.164\n",
            "world_rank_o         -0.1282      0.031     -4.104      0.000      -0.189      -0.067\n",
            "masters_o            -0.1064      0.030     -3.535      0.000      -0.165      -0.047\n",
            "sinc1_1              -0.1292      0.037     -3.514      0.000      -0.201      -0.057\n",
            "intel1_1              0.0528      0.038      1.375      0.169      -0.022       0.128\n",
            "fun1_1                0.0512      0.033      1.533      0.125      -0.014       0.117\n",
            "shar1_1              -0.2191      0.036     -6.166      0.000      -0.289      -0.149\n",
            "age_diff              0.0920      0.033      2.787      0.005       0.027       0.157\n",
            "income_diff          -0.0171      0.033     -0.513      0.608      -0.083       0.048\n",
            "date_diff             0.0201      0.025      0.792      0.429      -0.030       0.070\n",
            "go_out_diff           0.1084      0.026      4.229      0.000       0.058       0.159\n",
            "sports_diff          -0.0140      0.033     -0.427      0.669      -0.078       0.050\n",
            "tvsport_diff          0.0364      0.030      1.215      0.225      -0.022       0.095\n",
            "exercise_diff        -0.0629      0.027     -2.354      0.019      -0.115      -0.011\n",
            "dining_diff          -0.0816      0.028     -2.884      0.004      -0.137      -0.026\n",
            "museums_diff          0.0901      0.032      2.852      0.004       0.028       0.152\n",
            "hiking_diff          -0.0289      0.027     -1.072      0.284      -0.082       0.024\n",
            "gaming_diff           0.0474      0.028      1.705      0.088      -0.007       0.102\n",
            "clubbing_diff        -0.0407      0.025     -1.649      0.099      -0.089       0.008\n",
            "reading_diff          0.0698      0.025      2.739      0.006       0.020       0.120\n",
            "tv_diff               0.0603      0.031      1.970      0.049       0.000       0.120\n",
            "theater_diff          0.0567      0.034      1.656      0.098      -0.010       0.124\n",
            "movies_diff           0.0346      0.030      1.151      0.250      -0.024       0.094\n",
            "concerts_diff         0.0319      0.035      0.920      0.358      -0.036       0.100\n",
            "music_diff            0.0217      0.032      0.685      0.494      -0.040       0.084\n",
            "shopping_diff         0.0871      0.033      2.654      0.008       0.023       0.151\n",
            "yoga_diff            -0.0312      0.026     -1.191      0.234      -0.083       0.020\n",
            "worldrank_diff        0.0494      0.031      1.619      0.105      -0.010       0.109\n",
            "(3_1-pf_o)_att       -0.0442      0.042     -1.050      0.294      -0.127       0.038\n",
            "(3_1-pf_o)_sinc      -0.0100      0.031     -0.322      0.747      -0.071       0.051\n",
            "(3_1-pf_o)_fun        0.0595      0.029      2.078      0.038       0.003       0.116\n",
            "(3_1-pf_o)_intel      0.1148      0.031      3.705      0.000       0.054       0.176\n",
            "(3_1-pf_o)_amb       -0.0827      0.032     -2.608      0.009      -0.145      -0.021\n",
            "(1_1-2_1_o)_sinc      0.0152      0.038      0.399      0.690      -0.059       0.090\n",
            "(1_1-2_1_o)_fun       0.0223      0.034      0.660      0.509      -0.044       0.088\n",
            "(1_1-2_1_o)_intel    -0.1159      0.037     -3.095      0.002      -0.189      -0.042\n",
            "(1_1-2_1_o)_amb       0.0820      0.024      3.384      0.001       0.034       0.129\n",
            "(1_1-2_1_o)_shar      0.0364      0.037      0.985      0.324      -0.036       0.109\n",
            "from_m               -0.0187      0.023     -0.818      0.414      -0.064       0.026\n",
            "goal_m                0.0275      0.023      1.194      0.233      -0.018       0.073\n",
            "imprace_m            -0.0421      0.023     -1.794      0.073      -0.088       0.004\n",
            "imprelig_m            0.0353      0.024      1.499      0.134      -0.011       0.081\n",
            "career_c_m            0.0232      0.024      0.981      0.327      -0.023       0.070\n",
            "masters_m            -0.0682      0.029     -2.314      0.021      -0.126      -0.010\n",
            "==============================================================================\n",
            "Omnibus:                      206.866   Durbin-Watson:                   2.041\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              225.521\n",
            "Skew:                          -0.437   Prob(JB):                     1.07e-49\n",
            "Kurtosis:                       3.181   Cond. No.                         5.66\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOpdfOyTMCQp"
      },
      "source": [
        "fun_X_train = fun_X_train.drop(columns= ['(3_1-pf_o)_sinc'])\n",
        "fun_X_test = fun_X_test.drop(columns= ['(3_1-pf_o)_sinc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBnY3vE2MgnW",
        "outputId": "c19c6966-a0f8-46c2-803a-a188e57cdf1c"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "fun_X_train = sm.add_constant(fun_X_train)\n",
        "\n",
        "\n",
        "# Fit the data to the model\n",
        "fun_model6 = sm.OLS(fun_y_train, fun_X_train).fit() #ordinary least square\n",
        "print(fun_model6.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  fun_o   R-squared:                       0.084\n",
            "Model:                            OLS   Adj. R-squared:                  0.077\n",
            "Method:                 Least Squares   F-statistic:                     11.91\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           3.87e-93\n",
            "Time:                        23:11:59   Log-Likelihood:                -13924.\n",
            "No. Observations:                6798   AIC:                         2.795e+04\n",
            "Df Residuals:                    6745   BIC:                         2.832e+04\n",
            "Df Model:                          52                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.3748      0.023    279.036      0.000       6.330       6.420\n",
            "gender               -0.1769      0.036     -4.847      0.000      -0.248      -0.105\n",
            "condtn                0.0157      0.025      0.629      0.530      -0.033       0.065\n",
            "order                -0.0864      0.024     -3.571      0.000      -0.134      -0.039\n",
            "int_corr              0.0482      0.024      2.043      0.041       0.002       0.094\n",
            "samerace              0.0142      0.023      0.612      0.540      -0.031       0.060\n",
            "age_o                -0.0591      0.033     -1.809      0.070      -0.123       0.005\n",
            "tuition_o            -0.0467      0.024     -1.923      0.055      -0.094       0.001\n",
            "income               -0.0246      0.033     -0.748      0.455      -0.089       0.040\n",
            "exphappy_o            0.1876      0.025      7.588      0.000       0.139       0.236\n",
            "met_o                -0.2096      0.023     -8.948      0.000      -0.255      -0.164\n",
            "world_rank_o         -0.1280      0.031     -4.099      0.000      -0.189      -0.067\n",
            "masters_o            -0.1067      0.030     -3.546      0.000      -0.166      -0.048\n",
            "sinc1_1              -0.1283      0.037     -3.500      0.000      -0.200      -0.056\n",
            "intel1_1              0.0533      0.038      1.392      0.164      -0.022       0.128\n",
            "fun1_1                0.0518      0.033      1.554      0.120      -0.014       0.117\n",
            "shar1_1              -0.2205      0.035     -6.250      0.000      -0.290      -0.151\n",
            "age_diff              0.0919      0.033      2.786      0.005       0.027       0.157\n",
            "income_diff          -0.0175      0.033     -0.523      0.601      -0.083       0.048\n",
            "date_diff             0.0201      0.025      0.792      0.428      -0.030       0.070\n",
            "go_out_diff           0.1087      0.026      4.248      0.000       0.059       0.159\n",
            "sports_diff          -0.0135      0.033     -0.412      0.680      -0.078       0.051\n",
            "tvsport_diff          0.0366      0.030      1.224      0.221      -0.022       0.095\n",
            "exercise_diff        -0.0630      0.027     -2.356      0.019      -0.115      -0.011\n",
            "dining_diff          -0.0814      0.028     -2.877      0.004      -0.137      -0.026\n",
            "museums_diff          0.0896      0.032      2.841      0.005       0.028       0.151\n",
            "hiking_diff          -0.0285      0.027     -1.058      0.290      -0.081       0.024\n",
            "gaming_diff           0.0472      0.028      1.699      0.089      -0.007       0.102\n",
            "clubbing_diff        -0.0407      0.025     -1.646      0.100      -0.089       0.008\n",
            "reading_diff          0.0702      0.025      2.760      0.006       0.020       0.120\n",
            "tv_diff               0.0603      0.031      1.971      0.049       0.000       0.120\n",
            "theater_diff          0.0573      0.034      1.673      0.094      -0.010       0.124\n",
            "movies_diff           0.0353      0.030      1.175      0.240      -0.024       0.094\n",
            "concerts_diff         0.0316      0.035      0.911      0.362      -0.036       0.100\n",
            "music_diff            0.0220      0.032      0.697      0.486      -0.040       0.084\n",
            "shopping_diff         0.0872      0.033      2.657      0.008       0.023       0.152\n",
            "yoga_diff            -0.0312      0.026     -1.191      0.234      -0.083       0.020\n",
            "worldrank_diff        0.0494      0.031      1.618      0.106      -0.010       0.109\n",
            "(3_1-pf_o)_att       -0.0366      0.035     -1.047      0.295      -0.105       0.032\n",
            "(3_1-pf_o)_fun        0.0632      0.026      2.409      0.016       0.012       0.115\n",
            "(3_1-pf_o)_intel      0.1191      0.028      4.248      0.000       0.064       0.174\n",
            "(3_1-pf_o)_amb       -0.0788      0.029     -2.685      0.007      -0.136      -0.021\n",
            "(1_1-2_1_o)_sinc      0.0141      0.038      0.372      0.710      -0.060       0.088\n",
            "(1_1-2_1_o)_fun       0.0215      0.034      0.637      0.524      -0.045       0.087\n",
            "(1_1-2_1_o)_intel    -0.1160      0.037     -3.097      0.002      -0.189      -0.043\n",
            "(1_1-2_1_o)_amb       0.0820      0.024      3.384      0.001       0.034       0.129\n",
            "(1_1-2_1_o)_shar      0.0385      0.036      1.059      0.290      -0.033       0.110\n",
            "from_m               -0.0187      0.023     -0.817      0.414      -0.064       0.026\n",
            "goal_m                0.0273      0.023      1.186      0.236      -0.018       0.072\n",
            "imprace_m            -0.0421      0.023     -1.794      0.073      -0.088       0.004\n",
            "imprelig_m            0.0354      0.024      1.505      0.132      -0.011       0.081\n",
            "career_c_m            0.0233      0.024      0.984      0.325      -0.023       0.070\n",
            "masters_m            -0.0680      0.029     -2.308      0.021      -0.126      -0.010\n",
            "==============================================================================\n",
            "Omnibus:                      206.192   Durbin-Watson:                   2.041\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              224.721\n",
            "Skew:                          -0.436   Prob(JB):                     1.59e-49\n",
            "Kurtosis:                       3.180   Cond. No.                         5.12\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-oc3RUIuYPN"
      },
      "source": [
        "## Fun (Linear Regression - Bootstrap)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjAQCqLTM5W0"
      },
      "source": [
        "#Most large p-values have been removed, and there removing variables has not impacted the r-squared values. As a result we will use this model as our final model\n",
        "fun_lr= fun_model6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35oZHezgRBEf"
      },
      "source": [
        "fun_train = fun_X_train.copy()\n",
        "fun_train['fun_o'] = fun_y_train \n",
        "\n",
        "fun_test = fun_X_test.copy()\n",
        "fun_test['fun_o'] = fun_y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fg44OMfIOJU"
      },
      "source": [
        "# compute out-of-sample R-squared using the test set\n",
        "def fun_OSR2(predictions, y_test,y_train):\n",
        "    SSE = np.sum((y_test-predictions)**2)\n",
        "    SST = np.sum((y_test-np.mean(y_train))**2)\n",
        "    r2 = 1-SSE/SST\n",
        "    return r2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4Js1LCTJAiY",
        "outputId": "063a6eb2-1a10-4b19-d495-05c9fbab45a4"
      },
      "source": [
        "# Check OSR^2 with test set\n",
        "fun_test_OSR2 = fun_OSR2(fun_lr.predict(sm.add_constant(fun_X_test)), fun_y_test, fun_y_train)\n",
        "fun_test_OSR2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07324627197556655"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNSfrSDcJlBp",
        "outputId": "85610ece-9c37-4710-913c-30c48707bee2"
      },
      "source": [
        "fun_output = bootstrap_validation(sm.add_constant(fun_X_test),fun_y_test,fun_y_train,fun_lr,\n",
        "                                 metrics_list=[fun_OSR2],\n",
        "                                 sample = 5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The bootstrapped mean of linear regression OSR2 is:', np.mean(fun_output)[0])"
      ],
      "metadata": {
        "id": "DCRNO7vX281q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b5ad05-3c67-4a17-fe32-3d5cf58e301a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The bootstrapped mean of linear regression OSR2 is: 0.07314556109946475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "RDjhvyeLKp5J",
        "outputId": "3783e065-143b-4071-a0cb-3de083d8b538"
      },
      "source": [
        "#bootstrap plots\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap OSR2 Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot OSR2 - Test Set OSR2', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].hist(fun_output.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[0].set_xlim([0.0,0.15])\n",
        "axs[1].hist(fun_output.iloc[:,0]-fun_test_OSR2, bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[1].set_xlim([-0.07,0.07])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.07, 0.07)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtgAAAFCCAYAAADLxqq9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debxkVXXo8d+ClllkFAmgLUpQnBBbAg5IRANiBFSiaNRG8ZEENA5xAM2zL1EjaBRE80h4EmmiEQEloCIRWxB9gtIgIjJIyyDdNtCMMiO43h97X7qorjvUvafqVvX9fT+f+lTVObtOrXtO3VWr9tnnnMhMJEmSJDVjjZkOQJIkSVqdWGBLkiRJDbLAliRJkhpkgS1JkiQ1yAJbkiRJapAFtiRJktSgOTMdQNM222yznDt37kyHIUldu/jii2/NzM1nOo5+MmdLGmZj5e3VrsCeO3cuixcvnukwJKlrEXHDTMfQb+ZsScNsrLztEBFJkiSpQRbYkiRJUoMssCVJkqQGWWBLkiRJDbLAliRJkhpkgS1JkiQ1yAJbkiRJapAFtiRJktQgC2xJkiSpQRbYkiRJUoNWu0ulS02JI2JKr8sF2XAkkqRuTDV/gzlczbAHW5IkSWqQPdjSBEYYabSdJKk/usnL5nA1yQJbs8Z0dhlKkmaO+VvDxiEikiRJUoPswdas45APSRpO5m8NC3uwJUmSpAZZYEuSJEkNssCWJEmSGtTXAjsito+IS1tuv4+I90bEJhFxTkRcU+83ru0jIo6NiCURcVlE7NTPeCVpNjNnS9LU9LXAzsyrM3PHzNwReAFwH3A6cBiwKDO3AxbV5wCvArart4OB4/oZryTNZuZsSZqamRwisgfwm8y8AdgXWFinLwT2q4/3BU7K4kJgo4jYsv+hStKsZ86WpEmayQL7AOBr9fEWmbm8Pr4J2KI+3gq4seU1S+s0SVJ/mbMlaZJmpMCOiLWAfYBT2+dlZgLZ5fIOjojFEbF4xYoVDUUpSQJztiR1a6Z6sF8FXJKZN9fnN4/uRqz3t9Tpy4BtWl63dZ32GJl5fGbOy8x5m2++eQ/DlqRZyZwtSV2YqQL7Tazc1QhwJjC/Pp4PnNEy/W31yPRdgLtadktKkvrDnC1JXej7pdIjYn3glcDftEw+EjglIg4CbgDeUKefBewNLKEcvf72PoYqSbOeOVuSutf3Ajsz7wU2bZt2G+UI9fa2CRzap9AkSW3M2ZLUPa/kKEmSJDXIAluSJElqkAW2JEmS1CALbEmSJKlBFtiSJElSgyywJUmSpAb1/TR90uoujoiu2ueCrq4yLUnqoW5zOJjHtSp7sCVJkqQG2YMtNWyEkUbbSZL6p5vcbB7XWOzBliRJkhpkgS1JkiQ1yAJbkiRJapAFtiRJktQgC2xJkiSpQRbYkiRJUoMssCVJkqQGWWBLkiRJDfJCMxpaU7mcrSRpZpm7NRvYgy1JkiQ1yB5sDT0vTS5Jw8dLkmt1Zg+2JEmS1CALbEmSJKlBFtiSJElSgyywJUmSpAZZYEuSJEkNssCWJEmSGmSBLUmSJDWo7wV2RGwUEadFxFURcWVE7BoRm0TEORFxTb3fuLaNiDg2IpZExGURsVO/45Wk2cycLUndm4ke7M8DZ2fmM4DnAVcChwGLMnM7YFF9DvAqYLt6Oxg4rv/hStKsZs6WpC71tcCOiCcAuwEnAGTmQ5l5J7AvsLA2WwjsVx/vC5yUxYXARhGxZT9jlqTZypwtSVPT7x7spwIrgC9HxM8j4ksRsT6wRWYur21uAraoj7cCbmx5/dI67TEi4uCIWBwRi1esWNHD8CVpVjFnS9IU9LvAngPsBByXmc8H7mXlrkUAMjOB7GahmXl8Zs7LzHmbb755Y8FK0ixnzpakKeh3gb0UWJqZP63PT6Mk75tHdyPW+1vq/GXANi2v37pOkyT1njlbkqagrwV2Zt4E3BgR29dJewBXAGcC8+u0+cAZ9fGZwNvqkem7AHe17JaUJPWQOVuSpmbODLznu4GvRsRawLXA2ymF/ikRcRBwA/CG2vYsYG9gCXBfbStJ6h9ztiR1qe8FdmZeCszrMGuPDm0TOLTnQUmSOjJnS1L3vJKjJEmS1CALbEmSJKlBFtiSJElSgyywJUmSpAZZYEuSJEkNssCWJEmSGmSBLUmSJDXIAluSJElqkAW2JEmS1CALbEmSJKlBFtiSJElSgyywJUmSpAZZYEuSJEkNssCWJEmSGmSBLUmSJDXIAluSJElqkAW2JEmS1CALbEmSJKlBFtiSJElSgyywJUmSpAZZYEuSJEkNssCWJEmSGmSBLUmSJDXIAluSJElqkAW2JEmS1CALbEmSJKlBfS+wI+L6iPhlRFwaEYvrtE0i4pyIuKbeb1ynR0QcGxFLIuKyiNip3/FK0mxmzpak7s1UD/afZ+aOmTmvPj8MWJSZ2wGL6nOAVwHb1dvBwHF9j1SSZM6WpC4MyhCRfYGF9fFCYL+W6SdlcSGwUURsORMBSpIeZc6WpHHMmYH3TOB7EZHAv2fm8cAWmbm8zr8J2KI+3gq4seW1S+u05UiriTgiJt02F2QPI5E6MmdLE+gmj48yn6/eZqLAfklmLouIJwLnRMRVrTMzM2sin7SIOJiyO5InP/nJzUUqSTJnS1KX+l5gZ+ayen9LRJwO7AzcHBFbZubyujvxltp8GbBNy8u3rtPal3k8cDzAvHnz/EmooTLCSCNtpF4wZ0sT6yZHm89nh76OwY6I9SPi8aOPgb8ALgfOBObXZvOBM+rjM4G31SPTdwHuatktKUnqIXO2JE1Nv3uwtwBOj4jR9/6vzDw7Ii4CTomIg4AbgDfU9mcBewNLgPuAt/c5XkmazczZkjQFfS2wM/Na4Hkdpt8G7NFhegKH9iE0SVIbc7YkTc2gnKZPkiRJWi1YYEuSJEkNssCWJEmSGmSBLUmSJDXIAluSJElqkAW2JEmS1CALbEmSJKlBFtiSJElSgyywJUmSpAZZYEuSJEkN6uul0qXxxBEx0yFIkrpg3pY6swdbkiRJapA92Bo4I4w02k6S1Fvd5GNzt2aDSfdgR8RuEbHBGPM2iIjdmgtLkiRJGk7dDBE5F9hhjHnb1/mSJEnSrNZNgT3ekQxrA49MMxZJkiRp6I07Bjsi5gLbtkya12GYyLrAO4DfNhqZJEmSNIQmOshxPrAAyHr7Ao/tyc76/GHg0F4EKEmSJA2TiQrsE4HzKEX0DyhF9BVtbR4Efp2ZtzcdnCRJkjRsxi2wM/MG4AaAiPhz4JLMvLsfgUmSJEnDaNLnwc7MH/YyEEmSJGl10M15sNeKiAURcVVE3BcRj7TdHu5loJIkSdIw6OZKjp+hjMH+LvBNythrSZIkSS26KbD3BxZk5id7FYwkSZI07Lq50MwGwAW9CkSSJElaHXRTYH8L2K1XgUiSJEmrg24K7C8Ab4qIj0XEvIjYtv022QVFxJoR8fOI+HZ9/tSI+GlELImIr0fEWnX62vX5kjp/bjd/nCRp+szZktSdbgrsC4DtgBHgp8A1HW6T9R7gypbnRwFHZ+bTgTuAg+r0g4A76vSjaztJUn+ZsyWpC90c5PgOyqXRpyUitgZeDXwSeH9EBPBy4M21yUJKEX8csG99DHAa8MWIiMycdhySpImZsyWpe91caObEht7zGOBDwOPr802BOzNz9DzaS4Gt6uOtgBvr+z8cEXfV9rc2FIskaXzmbEnqUjdDRKYtIv4SuCUzL254uQdHxOKIWLxixYomFy1Js5Y5W5KmZtI92BHxHxM0ycw8aII2Lwb2iYi9gXWADYHPAxtFxJzaI7I1sKy2XwZsAyyNiDnAE4DbOrzx8cDxAPPmzXNXpCQ1w5wtSVPQzRjsl7PqGOxNKLsN76y3cWXm4cDhABGxO/CBzPzriDiVciGbk4H5wBn1JWfW5xfU+T9wLJ8k9Yc5W5KmZtJDRDJzbmY+te32BGB34Cbg9dOI48OUg2eWUMbrnVCnnwBsWqe/HzhsGu8hSWqGOVuSxtFND3ZHmXl+RBxNOU/2S7p43XnAefXxtcDOHdo8APzVdGOUJE2POVuSJq+pgxyvBZ7f0LIkSZKkoTXtArseyHIg5VRNkiRJ0qzWzVlEftBh8lrAn1LG4P1tU0FJkiRJw6qbMdhrsOpZRO4GvgmcXMfnSZIkSbNaN1dy3L2HcUiSJEmrhb5eyVGSJEla3XVVYEfEcyLitIhYEREP1/tTIuI5vQpQkiRJGibdHOT4QuCHwP2Uq3XdBDwJeA3w6ojYLTMv7kmUkiRJ0pDo5iDHTwGXA3tk5t2jEyPi8cD36/y/aDY8SZIkabh0M0RkF+BTrcU1QH1+FLBrk4FJkiRJw6ibArv9FH3dzpckSZJWe90U2D8FPlKHhDwqItYHPgxc2GRgkiRJ0jDqZgz2R4DzgBsi4tvAcspBjnsD6wMvazw6SZIkach0c6GZn0XELsDHgD2BTYDbgXOBj2fmL3sToiRJkjQ8xi2wI2IN4NXAdZl5eWZeBuzf1uY5wFzAAluSJEmz3kRjsN8CfA24d5w2dwNfi4g3NRaVJEmSNKQmGiLyFuDLmXndWA0y8/qIOAGYTynGJfVIHBFdtc8FntxHkgaR+Xz1NlEP9k7A9yaxnO8D86YfjiRJkjTcJurBfjxwxySWc0dtK6mHRhhptJ0kaWaYz1dvE/Vg3wo8ZRLLeXJtK0mSJM1qExXYP6aMrZ7IgbWtJEmSNKtNVGAfA+wREUdHxFrtMyPicRFxDPBy4OheBChJkiQNk3HHYGfmBRHxD8Bngb+OiO8BN9TZTwFeCWwK/ENmeql0SZIkzXoTXskxM4+JiEuADwOvBdats+6nXDr9yMz8Uc8ilCRJkobIpC6VnpnnA+fXKztuVifflpmP9CwyDb1uz/EpSZo55mypOZMqsEdl5h+BW3oUiyRJkjT0uiqwpysi1gHOB9au731aZi6IiKcCJ1PGc18MvDUzH4qItYGTgBcAtwFvzMzr+xmzps9zfUrDyZw9O3WTi83bUmcTnUWkaQ8CL8/M5wE7AntFxC7AUcDRmfl0ykVrDqrtDwLuqNOPru0kSf1hzpakKehrgZ3FPfXp4+otKaf5O61OXwjsVx/vW59T5+8REQ4Sk6Q+MGdL0tT0uwebiFgzIi6ljOU+B/gNcGdmPlybLAW2qo+3Am4EqPPvouySlCT1gTlbkrrX9wI7Mx/JzB2BrYGdgWdMd5kRcXBELI6IxStWrJh2jJKkwpwtSd3re4E9KjPvBM4FdgU2iojRAy63BpbVx8uAbQDq/CdQDpxpX9bxmTkvM+dtvvnmPY9dkmYbc7YkTV5fC+yI2DwiNqqP16VcCfJKStLevzabD5xRH59Zn1Pn/yAzs38RS9LsZc6WpKnp62n6gC2BhRGxJqW4PyUzvx0RVwAnR8QngJ8DJ9T2JwD/GRFLgNuBA/ocryTNZuZsSZqCvhbYmXkZ8PwO06+ljO1rn/4A8Fd9CE2S1MacLUlTM2NjsCVJkqTVkQW2JEmS1CALbEmSJKlBFtiSJElSgyywJUmSpAZZYEuSJEkNssCWJEmSGmSBLUmSJDXIAluSJElqkAW2JEmS1CALbEmSJKlBFtiSJElSgyywJUmSpAZZYEuSJEkNssCWJEmSGmSBLUmSJDXIAluSJElqkAW2JEmS1CALbEmSJKlBFtiSJElSgyywJUmSpAZZYEuSJEkNssCWJEmSGmSBLUmSJDXIAluSJElqkAW2JEmS1CALbEmSJKlBfS2wI2KbiDg3Iq6IiF9FxHvq9E0i4pyIuKbeb1ynR0QcGxFLIuKyiNipn/FK0mxmzpakqel3D/bDwD9k5g7ALsChEbEDcBiwKDO3AxbV5wCvArart4OB4/ocryTNZuZsSZqCvhbYmbk8My+pj+8GrgS2AvYFFtZmC4H96uN9gZOyuBDYKCK27GfMkjRbmbMlaWpmbAx2RMwFng/8FNgiM5fXWTcBW9THWwE3trxsaZ3WvqyDI2JxRCxesWJFz2KWpNnKnC1JkzcjBXZEbAB8A3hvZv6+dV5mJpDdLC8zj8/MeZk5b/PNN28wUkmSOVuSutP3AjsiHkdJ1F/NzG/WyTeP7kas97fU6cuAbVpevnWdJknqA3O2JHWv32cRCeAE4MrM/FzLrDOB+fXxfOCMlulvq0em7wLc1bJbUpLUQ+ZsSZqaOX1+vxcDbwV+GRGX1mkfAY4ETomIg4AbgDfUeWcBewNLgPuAt/c3XEma1czZkjQFfS2wM/PHQIwxe48O7RM4tKdBSZI6MmdL0tR4JUdJkiSpQRbYkiRJUoP6PQZbUh/FEWPt3e8sF3R1tjVJUp90m8/BnD6T7MGWJEmSGmQPtrQaG2Gk0XaSpJnRTZ42p888e7AlSZKkBllgS5IkSQ1yiIgmbSoHWEiSZoY5W5o59mBLkiRJDbIHW13zwDlJGh7mbKn/7MGWJEmSGmSBLUmSJDXIAluSJElqkAW2JEmS1CALbEmSJKlBFtiSJElSgyywJUmSpAZZYEuSJEkNssCWJEmSGmSBLUmSJDXIAluSJElqkAW2JEmS1CALbEmSJKlBFtiSJElSgyywJUmSpAZZYEuSJEkN6muBHRH/ERG3RMTlLdM2iYhzIuKaer9xnR4RcWxELImIyyJip37GKkkyb0vSVPS7B/tEYK+2aYcBizJzO2BRfQ7wKmC7ejsYOK5PMUqSVjoR87YkdaWvBXZmng/c3jZ5X2BhfbwQ2K9l+klZXAhsFBFb9idSSRKYtyVpKgZhDPYWmbm8Pr4J2KI+3gq4saXd0jpNkjSzzNuSNI5BKLAflZkJZLevi4iDI2JxRCxesWJFDyKTJHUylbxtzpa0upsz0wEAN0fElpm5vO5KvKVOXwZs09Ju6zptFZl5PHA8wLx587ou0CUVcUR01T4X+O82S00rb5uzpf7oNqeDeb0pg9CDfSYwvz6eD5zRMv1t9aj0XYC7WnZJSpJmjnlbksbR1x7siPgasDuwWUQsBRYARwKnRMRBwA3AG2rzs4C9gSXAfcDb+xmrNBuNMNJoOw0/87Y0vLrJ1eb1ZvW1wM7MN40xa48ObRM4tLcRSZLGY96WpO4NwhARSZIkabUxCAc5aoZM5eAHSVL/ma+l4WIPtiRJktQge7DlgW2SNCQ8aE0aDvZgS5IkSQ2ywJYkSZIaZIEtSZIkNcgCW5IkSWqQBbYkSZLUIAtsSZIkqUEW2JIkSVKDLLAlSZKkBllgS5IkSQ3ySo6rkTgiZjoEzTLdfuZyQfYoEml4mKs1yKby+TS3r8oebEmSJKlB9mCvhkYYabSdNBY/a9LUdfN/4f+Q+sXPZTPswZYkSZIaZIEtSZIkNcgCW5IkSWqQBbYkSZLUIAtsSZIkqUEW2JIkSVKDLLAlSZKkBllgS5IkSQ2ywJYkSZIa5JUcJfVNHBFdtc8F2aNIJElN6Ta3w+qf3we+wI6IvYDPA2sCX8rMI2c4pCmbygdQkobNMOdt87SkJgx0gR0RawL/CrwSWApcFBFnZuYVMxuZpKkYYaTRdho85m1p9ukmZ8+W/D7QBTawM7AkM68FiIiTgX2BgUnUU+nt6LbIsCjRbNXr3sTVfRflDBmYvD2dz0+v8nS3baXVVT/2Fs1kjo/Mwf2CiYj9gb0y8531+VuBP8vMd431mnnz5uXixYv7FaK7E6UhNmgFdkRcnJnzZjqO6eg2b/cyZ5ufpdmtHzl+rLw96D3YkxIRBwMH16cPRsTlMxnPJGwG3DrTQYxj0OODwY9x0OMDYyRGpl2ANR3fUxpc1sBqy9n3RMTVPXibYfh8txqmeI21d4Yp3oGPtS3H9yrejnl70AvsZcA2Lc+3rtMeIzOPB44HiIjFg94DNOgxDnp8MPgxDnp8YIxNGPT4ZsiEebs1Z/fKsG2bYYrXWHtnmOIdplih//EO+nmwLwK2i4inRsRawAHAmTMckyRpbOZtSbPeQPdgZ+bDEfEu4H8op3v6j8z81QyHJUkag3lbkga8wAbIzLOAs7p4SU93OzZk0GMc9Phg8GMc9PjAGJsw6PHNiCnk7V4Ytm0zTPEaa+8MU7zDFCv0Od6BPouIJEmSNGwGfQy2JEmSNFSGqsCOiL0i4uqIWBIRh3WYv3ZEfL3O/2lEzG2Zd3idfnVE7DlI8UXEKyPi4oj4Zb1/eS/im06MLfOfHBH3RMQHBi2+iHhuRFwQEb+q63KdQYoxIh4XEQtrbFdGxOG9iG+SMe4WEZdExMP1vMWt8+ZHxDX1Nn+Q4ouIHVu28WUR8cZexDedGFvmbxgRSyPii72KcbaLiE0i4pz6WT0nIjYeo13Hz3RErBURx0fEryPiqoh4/aDG2jL/zOjDqWinE29ErBcR36nr9FcRcWSPYhzomqCpeKOPNcJ0Y22Z39NaoalYo5d1Q2YOxY1ysMxvgG2BtYBfADu0tTkE+Lf6+ADg6/XxDrX92sBT63LWHKD4ng/8SX38bGDZoK3DlvmnAacCHxik+CjHE1wGPK8+37TpbdxAjG8GTq6P1wOuB+bOUIxzgecCJwH7t0zfBLi23m9cH288QPH9KbBdffwnwHJgo0Fahy3zPw/8F/DFpuPz9ug6/jRwWH18GHBUhzZjfqaBI4BP1MdrAJsNaqx1/uvqZ+ryQV63Nb/9eW2zFvAj4FUNxzfQNUHD8falRmgi1pb5PasVGlyvPa0bhqkH+9HL72bmQ8Do5Xdb7QssrI9PA/aIiKjTT87MBzPzOmBJXd5AxJeZP8/M39XpvwLWjYi1G45vWjECRMR+wHU1xl6YTnx/AVyWmb8AyMzbMvORAYsxgfUjYg6wLvAQ8PuZiDEzr8/My4A/tr12T+CczLw9M+8AzgH2GpT4MvPXmXlNffw74BZg84bjm1aMABHxAmAL4Hs9iE0rtf6vLQT269BmvM/0O4BPAWTmHzOzlxfNmFasEbEB8H7gEz2MsdWU483M+zLzXID6/3MJ5XzoTRr0mqCxePtYI0w7VuhLrdBUrD2tG4apwN4KuLHl+dI6rWObzHwYuIvyi2Qyr53J+Fq9HrgkMx9sOL5pxViT+4cpPT69Mp11+KdARsT/1N32HxrAGE8D7qX0uv4W+JfMvH2GYuzFayerkfeIiJ0pPRa/aSiuVlOOMSLWAD4L9HzXqNgiM5fXxzdRftS067gtI2Kj+vzjNWecGhGdXj/jsY7GSflc3dezCB9ruvECUNfza4BFDcc36DVBu2GoEVaJoxq0WqFjHNXA1A0Df5q+2SQingUcRflVNWhGgKMz8576I3XQzAFeAryQ8gW0KCIuzsymk/p07Aw8QhnasDHwo4j4fmZeO7NhDZ+I2BL4T2B+Zq7SgzzDDgHOysylA/q/MlQi4vvAkzrM+mjrk8zMiOjmtFhzKL2qP8nM90fE+4F/Ad46aLFGxI7A0zLzfe1jXaejh+t2dPlzgK8Bx5rnpm/Aa4RRIwx2rdCqp3XDMBXYk7ls+mibpfUf+wnAbZN87UzGR0RsDZwOvC0ze9EjN90Y/wzYPyI+DWwE/DEiHsjMJg/gmk58S4HzR3fxRsRZwE4032synRjfDJydmX8AbomI/wfMo4xb7HeM471297bXntdIVI99jyn/P0bEhsB3gI9m5oUNxzZqOjHuCrw0Ig4BNgDWioh7MnOVg280scx8xVjzIuLmiNgyM5fXH123dGg21mf6NsqX6jfr9FOBgwY01l2BeRFxPeV7+4kRcV5m7s409DDeUccD12TmMdOJcwyDXhO0G4YaoYlY+1ErNBVrb+uGpgZz9/pGSSrXUg5IGB3I/qy2Nofy2IHsp9THz+KxBzRcS/MHOU4nvo1q+9cN6jpsazNCbw5ynM463Jgyzm+9upzvA68esBg/DHy5Pl4fuAJ47kzE2NL2RFY9yPG6uj43ro83GaD41qIkv/c2vd6airFt3oF4kGMvt9NneOyBeJ/u0GbMzzRlvObLW7bVqYMaa0ubufTnIMfprttPAN8A1uhRfANdEzQcb19qhCZibWszQu8PchzYuqHnG6rhFbk38GvKmMuP1mn/BOxTH69D6YVYAvwM2LbltR+tr7uaho9mnm58wD9SxuZe2nJ74iDF2LaMnv3TTHMbv4VyUMXldPgymOkYKb2Zp9YYrwA+OIMxvpDy6/1eyi/5X7W89h019iXA2wcpvrqN/9D2v7LjIMXYtowDscDu2Y0yjnIRcA3ly3G0uJsHfKmlXcfPNPAU4HzKmQQWAU8e1Fhb5s+lPwX2lOOl9CImcGXL/+k7exDjQNcETcVLH2uEJtZtyzJG6HGB3cDnoGd1g1dylCRJkho0TGcRkSRJkgaeBbYkSZLUIAtsSZIkqUEW2JIkSVKDLLAlSZKkBllgD7iIODAisuX2SEQsi4hTImL7Hr7vRhExEhE7TeG1+9Wrog2kiNg+IhbW9fhQvf/PTuszItaOiPdFxC8i4u6I+H1EXFVfv11Lu5G27fRgRFwRER+sl85uXeb+EfGNiLghIu6PiKsj4lMR8fhJxD637X3abzt2sR5GIuLlHaafWC9o0Vf1s/6Ofr+vNFXDlp8j4oCI+GFE3BkR90XELyPiIxGxboe229ZccG3NZ7dExAUR8fG2dte3rYM7I+KciHhJW7sNI+JjEfGTiLittvtJROw3tbXQ8e/bfYL8OHo7sYH36up7brLrc5LLmlu3/7ZdvGbNiPi7iPhZ/S67JyIuiohDImLNDu13rN9Tv63xLo+IcyPi79vata7XP0bErRFxRpSrTra227J+zy2u235FRCyKiN26/fuHhafpG3ARcSDwZeCvKOfcXRN4GvC/gXUpJ1S/qwfvO5dy0YD/lZlf6vK1JwKvyMytm45ruiLiFcAZlHO7Hk35G+cC7wW2B/bNzO+3tP8m5bK0nwYupKz/Z1K2x5GZeUZtNwIsoFx29RHKBRgOrO0+kJmfbVnmhcBvaxxLgedTzhd6FfCiHOfS3y3b5VPAmR2aXJaZ901yXSTwycz8x7bpTwM2zMyfT2Y5TYmI84A5mfmSidpKg2CY8nNE/DvwvygXRvoG5SqWuwEfoJyv+hWZ+fva9imUcy3fABwDXA9sAewM7J2Zz2xZ7vWU3DVC6bTbjpILN6NcSOv62sZ3+YEAAA+zSURBVO7ZlPNpf5ly7vE/Am8C5gPvysx/7XY9dPgbNwR2aJm0JeVKne35ckVO82qI3XzPdbM+J/neuwPnAq9s/b4ap/3jgP8GXgl8ETibcp7yvYB3AecA+2Xmw7X9C4EfAT8FjgNuopzb/CXA9pn55y3LTspn6t8pF2t5DuUc1A8Az8nMO2u7vwSOpWz/CykXhTkEeBXlfNXf7mYdDIVenwDc27RPoH4g5R/h6W3TX1Gn9+qiOXPr8ru+OADln23pJNuu3cd1uSlwK/ATYJ22eevU6bcCm9Zp29Z18J4xlrdGy+OR2nZO63zKF89Vba/bvMOy3lZf//JebZcOy0rgE/1a/5OI5zzgxzMdhzdvk70NS35uiXOVXEa5YNKD1KvM1mn/RLmg06Yd2q/R9vx64Ctt015c3++wlmnrA+t1WN4i4LeDsJ66XHY333OTXp+TXN7u9e96xSTbj34/7dth3r513oKWaSdRiupVvp87bP9VvkeAv67TD2iZtlHr92OdNodyoZ/ze7H9Z/rmEJHh9ft6/7jWiRGxV93tdH9E3BUR/92+qzKK90UZmvBQ3fXzxfrrv7V3BOD/tuz+ObDO37Pu2rur7ma6OiI+VuedSOmR2KrlddfXeaO7714XEf83IlYAN9d5T48yTOO6Gvu1EXFcRGzcFvuJEbE0Il5Ud289EGUX5bsnsc7eSSmy35OZD7TOqM/fW+e/s07epN7f1GlhOU5Pc8v8XwBPbpu+okPzi+r9VuMtc7IiYk5EfDwiflPX0a0R8eOou21rrwPAR1u200id95ghIrFyWMrf1l18N9VdjF+JiPXqtvuf+llYEhHz22KZcNvW3uuXAS9uiee8lvlPjYiv1t2KD0bEpRHx2ibWldQDM5afx/BhytXqjm2fkZkXAScAb42IP6mTN6H0QN7Zof24ea+6pN4/mvsy897svHdtMfAnHab3TES8LMrwhLsj4t6av57d1mZK33NjmPT6rLn78ChDER+MiN9FxGcjYp06f3dK7zXAOS3vv/sYf+valO+2s7LucW17/zOA7wLvrW1H470jMx+cKN4xdNr+d2btIW+Z9jClZ7+R771BY4E9PNas/3hrR8QzgX8GbqH0+gEleQPfAe4B3gj8HfBs4McR0foB/iTwOcpuoddQhj8cCHwnynjh5cDrattPAbvW23eijPk6k5Lg3wjsU5e1fm3/ceAsYEXL69oLoS8AAby1vi+UBHsjJRHsSfnFv0ddVrsNga8DC4H96jo4doIvGOrybqpfKKvIzJ9RCv7RcclXUb4oj4yIt0TEFhMsv5O5lMu3TuRl9f7KSS53jfp5aL21jqP7MPA+yhfqnsDbKT1Foz8adq33J7JyO020q/lwynaaD3yMsv3/DTid8rl7LeWS01+Ox46/m8y2PQT4eX39aDyHAETENpRdlc+rf9M+lAT+jYjYZ4KYpX4YiPzcKbBaND8D+FbWbsMOzqQMbxnNQz8DNgC+HhG7tRRekzW33k8m9+1GybV9ERGvpuTCeyiXyX4z8HjgRzXX0ND3XKtu1udXKJdG/y/g1ZRtfBDw1Tr/EuDQ+vjvW97/Ejp7AfAEOg8pHHUmpYd5dEz/z4BnRMS/RcTOETFnnNd2Mrfej7v9I2ItSuyT/d4bLjPdhe5t/Bsrd+2135YBL2xru5gytrh1mMJTKbumPlefb0LZHXhi22vfUpe7T30+lw671oD96/QNx4n5RDrsOmPlbq3TJ/F3z6GM90rg+W3Lfsyupzr9HMr4thhnmVcCF0zwvhcCV7Q8fw0liY6u999QxrA9o+11I3X+2jX2zSkF6cOUsW3jvedWlC/jcyaxXubS+fOQwD0t7b4NfHOCZXUcIlLX8fUd3vMHbe2+Wae/pWXaxvVvXjCFbXseHYaIUHrXVtC2e7Vu80t7+f/nzdt4NwYsP48R45/Vtn8zTptn1DYfqs+D8uP5j3X6g5Qxuf/AqsPrrqcUf3Mo42p3AH4I/BrYeILYDq7L/+sebZ9V1hOwBFjU1m5DyvDAY+rzKX/PjdF2UusTeGmd/7a2148OudixPt+dSQ4RofxASGDPcdrsVdu8oT5fl9JxMvp5vg/4HmUMf6chIp+s238dypCjXwIXAI+bILZ/ruvkpb3Y/jN9swd7eLyW8sHdmdJrewVwVu0tISLWp/z6/Hq27IbJzOuA/8fKnoldKEnwK23LP5lSGL2M8V1K+UI4OcrZMJ44hb/l9PYJEbFWlKPZr4qI++t7/KjObj8a/xHKQTqtTqbsjmp0V1NmfouSpF9H6Xm/k9rbGuWAyXYPUGK/hZI8Ds/M/x5r+RGxAeVgx4cpvcyT9QnK56H19tKW+RcBe0fEJyPiJbWnYLq+2/Z8tNfpf0YnZOYdlL99m9FpXW7bTvai9Bbd1dpjX9/3eaO7zqUZNCj5uRFZ/C3lgM13U/Lt04F/AX4Wq5515M2U/+sHKUNRng28puaDjuqQhmOBkzLzq2O1q21jnL11kxblzE9PA77alkvuoxSEo2e0aOJ77lFdrM+9gIeA09ri+16d35czbmTm/Zn5WuBZwAcpuX8ecDzw3YiItpd8hLK+7mdlb/0+mfmHsd4jIt4MHAZ8PDN/NFa7YWaBPTwuz8zFmXlRljFT+1B+FY/U+RvX58s7vPYmVg4NGL1/TLua9G9rmd9RZi6h7OZfA/hP4KaIuDAiukn8nWL8FOVv+Qplt9jOrNwNuk5b2zs6/OPeXO/HK7CXsnLX1VjmUoYzPCrL2MHTM/PvM/MFwIsoRf6RHV6/S439tZRddkeOMzZuXeBblIMp98zMpRPE1uqG+nlovbWe9eOfKUfy70MpZm+LiC9HxGZdvEe79i/Lh8aZ3rrNutm2nTyRchDoH9pun6nzN53EMqReGoj8PIbRvDJ3nDaj89pz33WZ+cXMfDPlLBKfppwl4qC213+X8gPjRZShYOsC3xwdN9wuylkqzgR+wMpjXsYzn8f+70/1DCCjhfIJrJpP/pKaSxr6nlvFJNbnEyk/sO5ti+2WOn8quW462/+KzPyXzHw9ZajfVyhn1Xp12+v/g5WdPCOUzq6TOxTiAETEayh7AE7IzAWT/DuGTrfjajQgMvP+iLgWeG6ddAdlV82TOjR/EnB7fXx7y7RfjTaov5I3bZk/3nufC5xbx5G9mDKm9jsRMTczb51M+B2mHUDpyfhES0wbjPH6jSPicW1F9uj46GXjvO8i4BUR8cLsMA47Inauy/nBuMFnXhgR36P0NrS7uH4ZXhQRP6b08n4hIp6XLQeHRDlt0mmUXoFXZuYvx3vPbtV1cxRwVEQ8ifLl8TlgPcouw37qZtt2chvlR8JRY8z/3TRikxo3k/m5QyzLIuJqynC3w8dotg+l0+CH4yznkYj4JPAhHnsqPIDbM3NxfXxBRNxFOR3bu1n5Q3j0b3kOZe/TpcDrx+vlbPEtSgE3apWD7ybptnp/OOWUge1GOw2a+J4b1xjr8zbKXtCXjvGyqeS6xZRjifahnEqvk32Auxh7HDeZ+UBEfIYyXGkHyjDEUctbtv+Pa2G9gDLU5tTW5UTEHnXa6cDfdP3XDBF7sIdURKxH2d20AkovK3Ax8Fetu8+inH/zRaw82OZCShI5oG2Rb6T84BptN5rAVrkAwajMfDAzf0D5Fb4+ZTzh6GvHfN0Y1qP8Um811pCJNYHXt007gHJu6fEK7C9Rvug+396zUp8fQ/kC+1Kd9vi6a5e2tmtSzvXaqTfqUTUJ/xNld+mj8dYDlb5KOZhyv8y8cLzlTFdm3pTlXLnfr7GMeojut9NUTHbbjvW5OZtSqPyqQ6/94uxwpLs0kwYhP7f5DPCsaLtISI3hhdSD6DLzd3XalmMs5xn1ftzcRzkA/RLgg3VdjL7XdpRjJ64F/jIz759M8Jl5W9v//FQ7JK6mjBl/1hi55LIO7z3t77ku1ufZlL16TxgjvtECe9Lbv+bHYylDBvftENu+lHNRf340lzaw/Y+i/Bj4WGsvdkTsShkSuYhy7M5kzkgytOzBHh471t37QTl5/rsouwu/0NLmf1OOJP92RPwfyjioIyi/TD8LkJm3R8RngcMj4l7K2NZnUsb0/piVR6LfTPk1fUBEXEbZZXUd5YIKu9XX3Ui5mMDhlH+my+trrwA2iYi/o/x6fmASCfFsYH5E/JJyEMrrKF88ndwNfLquj2soFyt4BXBgZo51lDyZeWtEvInyy/mCiGi90Mz7KMnjtZk52suxPXB2RHyN8sV2C2Xdv5NSqB4ywd8Epcfgg8A/RsRpNb5/pazHTwL3RsQuLe2XTnKoyLZtrxv167qNz6CcIvASyo+K51N63Ft7MK4AXh0RZ9c2v2tJ4E2a7La9AjgkIt5I2QV8d2ZeTTljyc+A8yPii5QvyI0p22DbzPTqj5ppA5GfW3LXY2TmCRHxIuCYiHgeZQzw/ZSe0g9Qcvd7Wl7y0dr+ZFaOR34upbf1Nkrv9JgyM6Oc0u7blLOlfLaOYz6HMgRiAbBD2wiCn/f6x3KN61DgjCjHpZxCObhxC0pO+m1mfi4i/pZmv+cmtT4z87z6fXNaRHyOkvf+SPmO2hv4cGb+mnIA6cPAOyLidkrBfXVm3j3G+/8TZW/pKRHxr5QhPUn5Tng3JUd/oqX98VGObflG/XvXpOxB+BAlN69yHFWrugfnnyknBHgd5YxPz6B8fm+l/OB7Qev273VH04zIATjS0tvYNzofpX4LZRjDKkcFU/5hLqAkz7sovxa3b2sTlILyakpvyXJK0bdhW7vRg3X+UN/3QMopdc6gJJ0H62tPbX0Pyq/8r7Fyt+j1dfrujHHkMyWBnVxfcwelh/eFo+/b0u5EypiyF1EO5HuAcvaQv+9inT6TMq7udy1//1eBHdrabUQp7s6vbf5QYzsX2L+t7UiNdU6H9xs9Uv619fn1Hbbp6G1kgtjnjvPaHI2LcnT6hZTkfX/d1iO0HNVN2e15cV2Hj743Y59FpP2MMh3/ZtouPNHFtn0S5Qvt7jrvvJZ5W1P2LCxr2Wbn0HIGE2/e+n1jwPLzJOJ9c81nv68xXE45Jdx6be3+jHLWi8spB3b/gbKH8ETgaW1tH/P/3jbvJzX+dVmZ/8e6ze3B9hkrd+1KKf7vqPnv+pqjdm2ZP6XvuTHi6GZ9rkH5sfOLGttd9fGnKT3bo+3+hrIn4OH6/rtPsC7mUE7vdxHlB9m9lB8G72LVHL4nZS/E1ZR8/CArz6C1RVvbpPPZqNaq6/Xn9TN94Hjbf6b/l3tx81LpGioxwJdhlyRJAsdgS5IkSY2ywJYkSZIa5BARSZIkqUH2YEuSJEkNssCWJEmSGmSBLUmSJDXIAluSJElqkAW2JEmS1CALbEmSJKlB/x+BPa+wxnF3swAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oecyC48BMPZK",
        "outputId": "007ac130-589a-4bbb-bbf0-e84e3dc3d6a2"
      },
      "source": [
        "# The 95% confidence interval\n",
        "CI_0 = np.quantile(fun_output.iloc[:,0]-fun_test_OSR2,np.array([0.025,0.975]))\n",
        "left = fun_test_OSR2 - CI_0[1]\n",
        "right = fun_test_OSR2 - CI_0[0]\n",
        "print(\"The 95-percent confidence interval of the test set OSR2 for the linear regression fun model is: \",[left, right])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 95-percent confidence interval of the test set OSR2 for the linear regression fun model is:  [0.04317136944899776, 0.10387740402975806]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy-VBlVBuc1-"
      },
      "source": [
        "## Fun (LDA - Train Model + Bootstrap Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fUuonPJt371",
        "outputId": "26633d51-aadb-4d5a-d5ae-260044ac2116"
      },
      "source": [
        "# Run LDA on Sincerity trait.\n",
        "import sklearn\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# redefine y_train/y_test type to int\n",
        "y_train = fun_y_train.astype(int)\n",
        "y_test = fun_y_test.astype(int)\n",
        "\n",
        "# train\n",
        "fun_lda = LinearDiscriminantAnalysis()\n",
        "fun_lda.fit(fun_X_train, y_train)\n",
        "\n",
        "#fun_X_test.drop(columns=['const'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearDiscriminantAnalysis()"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L764i1ulMgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5782b47f-3333-45d7-9808-92f405ffddb9"
      },
      "source": [
        "# compute ambition accuracy of LDA\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = fun_lda.predict(sm.add_constant(fun_X_test))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix : \\n\", cm)\n",
        "fun_lda_acc = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nTest Set Accuracy:\", fun_lda_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix : \n",
            " [[  0   0   0   0   0   0   0   1   0   0   0   0]\n",
            " [  0   0   1   1   1   4   4   5   4   0   2   0]\n",
            " [  1   0   4   3   0   5  16  10   3   0   1   0]\n",
            " [  2   0   0   1   0  10  19  20   5   0   0   0]\n",
            " [  0   1   5   1   2  18  48  29  15   0   3   0]\n",
            " [  1   0   5   5   2  37  60  71  48   2   1   0]\n",
            " [  1   0   1   0   2  38 115  99  56   1   6   0]\n",
            " [  1   4   0   3   5  43  92 140  79   3  13   0]\n",
            " [  1   0   1   1   1  19  85 109  79   0  13   1]\n",
            " [  0   0   1   0   0   7  23  51  31   2   8   0]\n",
            " [  0   0   0   0   0   2  11  37  22   1  14   1]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0]]\n",
            "\n",
            "Test Set Accuracy: 0.23176470588235293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB9C6snevmI5"
      },
      "source": [
        "# Fit new linear model\n",
        "\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "fun_X_test = sm.add_constant(fun_X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mplUnNO3vTws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c2311eb-6602-4f64-933a-754347737548"
      },
      "source": [
        "# compute sincerity accuracy of LDA\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = fun_lda.predict(fun_X_test)\n",
        "fun_lda_acc = accuracy_score(y_test, y_pred)\n",
        "fun_lda_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23176470588235293"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkeNbpV-1jdV"
      },
      "source": [
        "def fun_accuracy(y_pred, y_test, y_train):\n",
        "  return accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uBtTD6U2M-h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ce943e-abff-4f17-8e1c-74bf0878a096"
      },
      "source": [
        "fun_lda_output = bootstrap_validation(fun_X_test,y_test,y_train,fun_lda,\n",
        "                                 metrics_list=[fun_accuracy],\n",
        "                                 sample = 5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PURfGlKY2dWs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "048a1132-b8e6-4c48-cc6c-b35e9e3f96c7"
      },
      "source": [
        "#bootstrap plots\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap Accuracy Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot Accuracy - Test Set Accuracy', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].hist(fun_lda_output.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[0].set_xlim([0.15,0.3])\n",
        "axs[1].hist(fun_lda_output.iloc[:,0]-fun_lda_acc, bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[1].set_xlim([-.05,.05])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.05, 0.05)"
            ]
          },
          "metadata": {},
          "execution_count": 169
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean of bootstrapped estimate\n",
        "print(\"Bootsrapped Mean of LDA estimates: %s\" % np.mean(fun_lda_output)[0])"
      ],
      "metadata": {
        "id": "cIrFqOfL3Bh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6da9986-4b71-49b8-8b38-a57acadc7f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootsrapped Mean of LDA estimates: 0.23172341176471756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqfwVB6Y28Mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2ab24d2-5fc1-4773-f3a3-3ffcf270ec73"
      },
      "source": [
        "# The 95% confidence interval\n",
        "CI_0 = np.quantile(fun_lda_output.iloc[:,0]-fun_lda_acc,np.array([0.025,0.975]))\n",
        "left = fun_lda_acc - CI_0[1]\n",
        "right = fun_lda_acc - CI_0[0]\n",
        "print(\"The 95-percent confidence interval of the test set accuuracy for the LDA attractiveness model is: \",[left, right])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 95-percent confidence interval of the test set accuuracy for the LDA attractiveness model is:  [0.2117647058823529, 0.2517647058823529]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fun (Rescaled OSR^2 & Baseline Comparisons)"
      ],
      "metadata": {
        "id": "HAp9uSdBKqCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_fun = 1 - (np.sum((fun_y_test - fun_y_train.value_counts().index[0])**2)/np.sum((fun_y_test - np.mean(fun_y_train))**2))\n",
        "rm1_lr_fun = rescaled_OSR2_1(fun_lr, fun_y_train, fun_test, 'fun_o', 'lr')\n",
        "rm2_lr_fun = rescaled_OSR2_2(fun_lr, fun_y_train, fun_test, 'fun_o', 'lr')\n",
        "rm1_lda_fun = rescaled_OSR2_1(fun_lda, fun_y_train, fun_test, 'fun_o', 'lda')\n",
        "rm2_lda_fun = rescaled_OSR2_2(fun_lda, fun_y_train, fun_test, 'fun_o', 'lda')\n",
        "\n",
        "print('Models for FUN:')\n",
        "print('No Rescale Bootstrapped Mean, Linear Regression OSR2 for FUN:', np.mean(fun_output)[0])\n",
        "print('No Rescale Bootstrapped Mean, LDA Accuracy for FUN:', np.mean(fun_lda_output)[0])\n",
        "print()\n",
        "print('Rescaling Linear Regression Model before OSR2 Calculation:')\n",
        "print('Rescale Method 1, Linear Regression OSR2 for FUN:', rm1_lr_fun)\n",
        "print('Rescale Method 2, Linear Regression OSR2 for FUN:', rm2_lr_fun)\n",
        "print()\n",
        "print('BASELINE:')\n",
        "print('Baseline OSR2:', base_fun)\n",
        "print()\n",
        "print('FUN MODEL COMPARISONS:')\n",
        "print('OSR2 difference between LR model and baseline:', np.amax(np.array([rm1_lr_fun, rm2_lr_fun, rm1_lda_fun, rm2_lda_fun, np.mean(fun_output)[0]])) - base_fun)\n",
        "print('Accuracy difference for LDA and baseline:', np.mean(fun_lda_output)[0] - fun_y_train.value_counts().values[0]/np.sum(fun_y_train.value_counts()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzmdW67TKsPz",
        "outputId": "76586864-d67c-4985-dff8-a367eb661078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models for FUN:\n",
            "No Rescale Bootstrapped Mean, Linear Regression OSR2 for FUN: 0.07314556109946475\n",
            "No Rescale Bootstrapped Mean, LDA Accuracy for FUN: 0.23172341176471756\n",
            "\n",
            "Rescaling Linear Regression Model before OSR2 Calculation:\n",
            "Rescale Method 1, Linear Regression OSR2 for FUN: -5.971486763801201\n",
            "Rescale Method 2, Linear Regression OSR2 for FUN: -4.369468773069706\n",
            "\n",
            "BASELINE:\n",
            "Baseline OSR2: -0.07529598291473372\n",
            "\n",
            "FUN MODEL COMPARISONS:\n",
            "OSR2 difference between LR model and baseline: 0.14844154401419846\n",
            "Accuracy difference for LDA and baseline: 0.02622179364174021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edIazBcCsNdI"
      },
      "source": [
        "## Ambition (Linear Regression + LDA)\n",
        "\n",
        "Make linear model for this trait, perform feature selection.\n",
        "Bootstrap your model and report the mean and 95% CI for OSR^2.\n",
        "\n",
        "Run LDA on this trait. Bootstrap this model and report the mean and 95% CI for accuracy. Calculate OSR^2 as well.\n",
        "\n",
        "Calculate rescaled OSR^2 using the two methods above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXzALhJCuxW6"
      },
      "source": [
        "## Ambition (Linear Regression - Feature Selection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIwNGnyu4I83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d66ff020-5eb7-44e6-b412-6a8d0288e2ab"
      },
      "source": [
        "# Fit linear model for amb_o\n",
        "X_train = amb_X_train.copy()\n",
        "y_train = amb_y_train.copy()\n",
        "X_test = amb_X_test.copy()\n",
        "y_test = amb_y_test.copy()\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "X_train = sm.add_constant(X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "amb_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(amb_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  amb_o   R-squared:                       0.087\n",
            "Model:                            OLS   Adj. R-squared:                  0.078\n",
            "Method:                 Least Squares   F-statistic:                     10.49\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           6.84e-88\n",
            "Time:                        23:12:52   Log-Likelihood:                -12686.\n",
            "No. Observations:                6475   AIC:                         2.549e+04\n",
            "Df Residuals:                    6416   BIC:                         2.589e+04\n",
            "Df Model:                          58                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.7514      0.021    315.041      0.000       6.709       6.793\n",
            "gender                0.1526      0.037      4.122      0.000       0.080       0.225\n",
            "condtn                0.0206      0.024      0.876      0.381      -0.025       0.067\n",
            "order                -0.1565      0.023     -6.890      0.000      -0.201      -0.112\n",
            "int_corr              0.0794      0.022      3.587      0.000       0.036       0.123\n",
            "samerace              0.0209      0.022      0.960      0.337      -0.022       0.064\n",
            "age_o                -0.0114      0.031     -0.370      0.712      -0.072       0.049\n",
            "mn_sat_o             -0.0673      0.028     -2.384      0.017      -0.123      -0.012\n",
            "tuition_o            -0.0614      0.028     -2.162      0.031      -0.117      -0.006\n",
            "income               -0.0025      0.031     -0.083      0.934      -0.062       0.057\n",
            "exphappy_o            0.1757      0.023      7.558      0.000       0.130       0.221\n",
            "met_o                -0.1245      0.022     -5.679      0.000      -0.167      -0.082\n",
            "world_rank_o         -0.1534      0.029     -5.218      0.000      -0.211      -0.096\n",
            "masters_o            -0.0957      0.028     -3.383      0.001      -0.151      -0.040\n",
            "attr1_1              -0.2683      0.189     -1.423      0.155      -0.638       0.101\n",
            "sinc1_1              -0.2602      0.116     -2.239      0.025      -0.488      -0.032\n",
            "intel1_1             -0.1910      0.115     -1.657      0.098      -0.417       0.035\n",
            "fun1_1               -0.2034      0.097     -2.098      0.036      -0.393      -0.013\n",
            "amb1_1               -0.0875      0.096     -0.915      0.360      -0.275       0.100\n",
            "shar1_1              -0.1496      0.104     -1.442      0.149      -0.353       0.054\n",
            "age_diff             -0.0213      0.031     -0.688      0.491      -0.082       0.039\n",
            "income_diff          -0.0380      0.031     -1.224      0.221      -0.099       0.023\n",
            "date_diff            -0.0202      0.024     -0.844      0.399      -0.067       0.027\n",
            "go_out_diff           0.0213      0.024      0.882      0.378      -0.026       0.069\n",
            "sports_diff           0.0102      0.031      0.332      0.740      -0.050       0.070\n",
            "tvsport_diff          0.0047      0.028      0.165      0.869      -0.051       0.060\n",
            "exercise_diff        -0.0068      0.025     -0.273      0.785      -0.056       0.042\n",
            "dining_diff          -0.1111      0.027     -4.189      0.000      -0.163      -0.059\n",
            "museums_diff          0.0929      0.046      2.034      0.042       0.003       0.182\n",
            "art_diff             -0.0319      0.044     -0.727      0.467      -0.118       0.054\n",
            "hiking_diff          -0.0689      0.025     -2.739      0.006      -0.118      -0.020\n",
            "gaming_diff           0.0420      0.026      1.611      0.107      -0.009       0.093\n",
            "clubbing_diff        -0.0125      0.023     -0.538      0.591      -0.058       0.033\n",
            "reading_diff          0.1362      0.024      5.634      0.000       0.089       0.184\n",
            "tv_diff               0.0676      0.029      2.336      0.020       0.011       0.124\n",
            "theater_diff         -0.0426      0.032     -1.312      0.189      -0.106       0.021\n",
            "movies_diff           0.0956      0.028      3.373      0.001       0.040       0.151\n",
            "concerts_diff         0.0891      0.033      2.714      0.007       0.025       0.153\n",
            "music_diff            0.0244      0.030      0.824      0.410      -0.034       0.082\n",
            "shopping_diff         0.0309      0.031      1.003      0.316      -0.029       0.091\n",
            "yoga_diff            -0.0222      0.025     -0.891      0.373      -0.071       0.027\n",
            "worldrank_diff        0.0012      0.029      0.041      0.967      -0.055       0.057\n",
            "(3_1-pf_o)_att       -0.1178      0.040     -2.963      0.003      -0.196      -0.040\n",
            "(3_1-pf_o)_sinc      -0.0575      0.029     -1.962      0.050      -0.115   -4.28e-05\n",
            "(3_1-pf_o)_fun        0.0572      0.027      2.115      0.034       0.004       0.110\n",
            "(3_1-pf_o)_intel      0.0494      0.029      1.694      0.090      -0.008       0.107\n",
            "(3_1-pf_o)_amb       -0.1736      0.030     -5.843      0.000      -0.232      -0.115\n",
            "(1_1-2_1_o)_att       0.2162      0.203      1.068      0.286      -0.181       0.613\n",
            "(1_1-2_1_o)_sinc      0.1323      0.117      1.132      0.258      -0.097       0.361\n",
            "(1_1-2_1_o)_fun       0.1419      0.105      1.350      0.177      -0.064       0.348\n",
            "(1_1-2_1_o)_intel     0.0727      0.111      0.653      0.514      -0.146       0.291\n",
            "(1_1-2_1_o)_amb       0.0496      0.095      0.521      0.603      -0.137       0.236\n",
            "(1_1-2_1_o)_shar      0.0954      0.104      0.919      0.358      -0.108       0.299\n",
            "from_m               -0.0200      0.022     -0.930      0.352      -0.062       0.022\n",
            "goal_m               -0.0102      0.022     -0.472      0.637      -0.053       0.032\n",
            "imprace_m            -0.0262      0.022     -1.183      0.237      -0.070       0.017\n",
            "imprelig_m            0.0379      0.022      1.712      0.087      -0.006       0.081\n",
            "career_c_m            0.0208      0.022      0.934      0.350      -0.023       0.064\n",
            "masters_m            -0.0211      0.028     -0.763      0.446      -0.075       0.033\n",
            "==============================================================================\n",
            "Omnibus:                      170.266   Durbin-Watson:                   1.988\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              186.819\n",
            "Skew:                          -0.384   Prob(JB):                     2.71e-41\n",
            "Kurtosis:                       3.319   Cond. No.                         41.4\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReNe7IL47VgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1568d59-9ccf-48a5-b8c0-89c36a9006e7"
      },
      "source": [
        "#Check VIF values\n",
        "amb_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(X_train, amb_cols).sort_values(ascending=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_att      89.299014\n",
              "attr1_1              77.414182\n",
              "(1_1-2_1_o)_sinc     29.739853\n",
              "sinc1_1              29.403058\n",
              "intel1_1             28.937455\n",
              "(1_1-2_1_o)_intel    27.027151\n",
              "(1_1-2_1_o)_fun      24.053179\n",
              "(1_1-2_1_o)_shar     23.444541\n",
              "shar1_1              23.431900\n",
              "fun1_1               20.464547\n",
              "amb1_1               19.940871\n",
              "(1_1-2_1_o)_amb      19.726604\n",
              "museums_diff          4.541827\n",
              "art_diff              4.204010\n",
              "(3_1-pf_o)_att        3.440607\n",
              "gender                2.985648\n",
              "concerts_diff         2.344634\n",
              "theater_diff          2.296274\n",
              "income_diff           2.094196\n",
              "age_diff              2.086902\n",
              "age_o                 2.071458\n",
              "shopping_diff         2.059868\n",
              "sports_diff           2.049688\n",
              "income                2.032331\n",
              "(3_1-pf_o)_amb        1.922232\n",
              "music_diff            1.911222\n",
              "world_rank_o          1.882794\n",
              "(3_1-pf_o)_sinc       1.872812\n",
              "(3_1-pf_o)_intel      1.851110\n",
              "tv_diff               1.822716\n",
              "worldrank_diff        1.791883\n",
              "tvsport_diff          1.761662\n",
              "tuition_o             1.756166\n",
              "movies_diff           1.747641\n",
              "masters_o             1.741678\n",
              "mn_sat_o              1.734829\n",
              "masters_m             1.666461\n",
              "(3_1-pf_o)_fun        1.594171\n",
              "dining_diff           1.529714\n",
              "gaming_diff           1.478897\n",
              "hiking_diff           1.379437\n",
              "exercise_diff         1.359695\n",
              "yoga_diff             1.355901\n",
              "reading_diff          1.271794\n",
              "go_out_diff           1.268264\n",
              "date_diff             1.244904\n",
              "condtn                1.203463\n",
              "clubbing_diff         1.179283\n",
              "exphappy_o            1.176059\n",
              "order                 1.123988\n",
              "career_c_m            1.079578\n",
              "imprelig_m            1.068648\n",
              "imprace_m             1.066911\n",
              "int_corr              1.066846\n",
              "met_o                 1.046508\n",
              "samerace              1.036439\n",
              "goal_m                1.021339\n",
              "from_m                1.010421\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMul6tM874zX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c465619c-bde9-4d8e-e1eb-2cbb0943fd3b"
      },
      "source": [
        "#drop '(1_1-2_1_o)_att' since it has the highest VIF value\n",
        "X_train = X_train.drop(columns=['(1_1-2_1_o)_att'])\n",
        "\n",
        "amb_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(amb_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  amb_o   R-squared:                       0.086\n",
            "Model:                            OLS   Adj. R-squared:                  0.078\n",
            "Method:                 Least Squares   F-statistic:                     10.65\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           3.50e-88\n",
            "Time:                        23:12:55   Log-Likelihood:                -12687.\n",
            "No. Observations:                6475   AIC:                         2.549e+04\n",
            "Df Residuals:                    6417   BIC:                         2.588e+04\n",
            "Df Model:                          57                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.7514      0.021    315.038      0.000       6.709       6.793\n",
            "gender                0.1484      0.037      4.030      0.000       0.076       0.221\n",
            "condtn                0.0205      0.024      0.871      0.384      -0.026       0.067\n",
            "order                -0.1569      0.023     -6.907      0.000      -0.201      -0.112\n",
            "int_corr              0.0798      0.022      3.604      0.000       0.036       0.123\n",
            "samerace              0.0219      0.022      1.003      0.316      -0.021       0.065\n",
            "age_o                -0.0102      0.031     -0.330      0.741      -0.071       0.050\n",
            "mn_sat_o             -0.0662      0.028     -2.348      0.019      -0.122      -0.011\n",
            "tuition_o            -0.0630      0.028     -2.221      0.026      -0.119      -0.007\n",
            "income               -0.0011      0.031     -0.036      0.972      -0.061       0.059\n",
            "exphappy_o            0.1753      0.023      7.543      0.000       0.130       0.221\n",
            "met_o                -0.1246      0.022     -5.682      0.000      -0.168      -0.082\n",
            "world_rank_o         -0.1561      0.029     -5.326      0.000      -0.214      -0.099\n",
            "masters_o            -0.0953      0.028     -3.370      0.001      -0.151      -0.040\n",
            "attr1_1              -0.1200      0.127     -0.941      0.347      -0.370       0.130\n",
            "sinc1_1              -0.1726      0.082     -2.097      0.036      -0.334      -0.011\n",
            "intel1_1             -0.1051      0.083     -1.273      0.203      -0.267       0.057\n",
            "fun1_1               -0.1318      0.070     -1.883      0.060      -0.269       0.005\n",
            "amb1_1               -0.0152      0.068     -0.224      0.822      -0.148       0.117\n",
            "shar1_1              -0.0729      0.075     -0.974      0.330      -0.220       0.074\n",
            "age_diff             -0.0216      0.031     -0.697      0.486      -0.082       0.039\n",
            "income_diff          -0.0364      0.031     -1.174      0.240      -0.097       0.024\n",
            "date_diff            -0.0198      0.024     -0.830      0.407      -0.067       0.027\n",
            "go_out_diff           0.0217      0.024      0.899      0.368      -0.026       0.069\n",
            "sports_diff           0.0108      0.031      0.353      0.724      -0.049       0.071\n",
            "tvsport_diff          0.0033      0.028      0.116      0.907      -0.052       0.059\n",
            "exercise_diff        -0.0069      0.025     -0.275      0.783      -0.056       0.042\n",
            "dining_diff          -0.1106      0.027     -4.175      0.000      -0.163      -0.059\n",
            "museums_diff          0.0928      0.046      2.031      0.042       0.003       0.182\n",
            "art_diff             -0.0338      0.044     -0.769      0.442      -0.120       0.052\n",
            "hiking_diff          -0.0685      0.025     -2.721      0.007      -0.118      -0.019\n",
            "gaming_diff           0.0432      0.026      1.660      0.097      -0.008       0.094\n",
            "clubbing_diff        -0.0134      0.023     -0.577      0.564      -0.059       0.032\n",
            "reading_diff          0.1373      0.024      5.688      0.000       0.090       0.185\n",
            "tv_diff               0.0668      0.029      2.309      0.021       0.010       0.123\n",
            "theater_diff         -0.0426      0.032     -1.312      0.189      -0.106       0.021\n",
            "movies_diff           0.0936      0.028      3.311      0.001       0.038       0.149\n",
            "concerts_diff         0.0896      0.033      2.731      0.006       0.025       0.154\n",
            "music_diff            0.0237      0.030      0.801      0.423      -0.034       0.082\n",
            "shopping_diff         0.0308      0.031      1.000      0.317      -0.030       0.091\n",
            "yoga_diff            -0.0223      0.025     -0.895      0.371      -0.071       0.027\n",
            "worldrank_diff        0.0017      0.029      0.059      0.953      -0.055       0.058\n",
            "(3_1-pf_o)_att       -0.1146      0.040     -2.891      0.004      -0.192      -0.037\n",
            "(3_1-pf_o)_sinc      -0.0573      0.029     -1.955      0.051      -0.115       0.000\n",
            "(3_1-pf_o)_fun        0.0567      0.027      2.094      0.036       0.004       0.110\n",
            "(3_1-pf_o)_intel      0.0488      0.029      1.673      0.094      -0.008       0.106\n",
            "(3_1-pf_o)_amb       -0.1754      0.030     -5.913      0.000      -0.234      -0.117\n",
            "(1_1-2_1_o)_sinc      0.0135      0.036      0.377      0.706      -0.057       0.084\n",
            "(1_1-2_1_o)_fun       0.0353      0.033      1.078      0.281      -0.029       0.099\n",
            "(1_1-2_1_o)_intel    -0.0399      0.036     -1.118      0.264      -0.110       0.030\n",
            "(1_1-2_1_o)_amb      -0.0465      0.031     -1.497      0.134      -0.107       0.014\n",
            "(1_1-2_1_o)_shar     -0.0089      0.035     -0.257      0.798      -0.077       0.059\n",
            "from_m               -0.0201      0.022     -0.932      0.351      -0.062       0.022\n",
            "goal_m               -0.0109      0.022     -0.506      0.613      -0.053       0.031\n",
            "imprace_m            -0.0269      0.022     -1.214      0.225      -0.070       0.017\n",
            "imprelig_m            0.0384      0.022      1.736      0.083      -0.005       0.082\n",
            "career_c_m            0.0210      0.022      0.943      0.346      -0.023       0.065\n",
            "masters_m            -0.0199      0.028     -0.721      0.471      -0.074       0.034\n",
            "==============================================================================\n",
            "Omnibus:                      168.336   Durbin-Watson:                   1.988\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              184.525\n",
            "Skew:                          -0.382   Prob(JB):                     8.53e-41\n",
            "Kurtosis:                       3.316   Cond. No.                         20.9\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok0PBHAZ8Oa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7195103-d3ff-45cd-9ba8-79dbfcf3ebb2"
      },
      "source": [
        "#Check VIF values again\n",
        "amb_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(X_train, amb_cols).sort_values(ascending=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "attr1_1              35.383296\n",
              "intel1_1             14.826750\n",
              "sinc1_1              14.752754\n",
              "shar1_1              12.195339\n",
              "fun1_1               10.673675\n",
              "amb1_1                9.938157\n",
              "museums_diff          4.541786\n",
              "art_diff              4.197629\n",
              "(3_1-pf_o)_att        3.420827\n",
              "gender                2.951148\n",
              "(1_1-2_1_o)_sinc      2.812828\n",
              "(1_1-2_1_o)_intel     2.778155\n",
              "(1_1-2_1_o)_shar      2.647926\n",
              "concerts_diff         2.344020\n",
              "(1_1-2_1_o)_fun       2.332530\n",
              "theater_diff          2.296274\n",
              "(1_1-2_1_o)_amb       2.098858\n",
              "income_diff           2.089281\n",
              "age_diff              2.086770\n",
              "age_o                 2.068579\n",
              "shopping_diff         2.059851\n",
              "sports_diff           2.048906\n",
              "income                2.028366\n",
              "(3_1-pf_o)_amb        1.916104\n",
              "music_diff            1.910314\n",
              "(3_1-pf_o)_sinc       1.872739\n",
              "world_rank_o          1.869574\n",
              "(3_1-pf_o)_intel      1.850387\n",
              "tv_diff               1.821495\n",
              "worldrank_diff        1.791383\n",
              "tvsport_diff          1.758025\n",
              "tuition_o             1.751415\n",
              "masters_o             1.741420\n",
              "movies_diff           1.740465\n",
              "mn_sat_o              1.732637\n",
              "masters_m             1.663845\n",
              "(3_1-pf_o)_fun        1.593508\n",
              "dining_diff           1.529402\n",
              "gaming_diff           1.476057\n",
              "hiking_diff           1.379047\n",
              "exercise_diff         1.359690\n",
              "yoga_diff             1.355878\n",
              "reading_diff          1.269180\n",
              "go_out_diff           1.267923\n",
              "date_diff             1.244692\n",
              "condtn                1.203430\n",
              "clubbing_diff         1.177694\n",
              "exphappy_o            1.175810\n",
              "order                 1.123725\n",
              "career_c_m            1.079502\n",
              "imprelig_m            1.068133\n",
              "int_corr              1.066583\n",
              "imprace_m             1.066012\n",
              "met_o                 1.046500\n",
              "samerace              1.034777\n",
              "goal_m                1.020364\n",
              "from_m                1.010417\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTIzEgIL8k7M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65dcae85-7c43-4c20-a47e-fb7aa279f444"
      },
      "source": [
        "#drop 'attr1_1' since it has the highest VIF value now\n",
        "X_train = X_train.drop(columns=['attr1_1'])\n",
        "\n",
        "amb_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(amb_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  amb_o   R-squared:                       0.086\n",
            "Model:                            OLS   Adj. R-squared:                  0.078\n",
            "Method:                 Least Squares   F-statistic:                     10.83\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           1.58e-88\n",
            "Time:                        23:12:57   Log-Likelihood:                -12687.\n",
            "No. Observations:                6475   AIC:                         2.549e+04\n",
            "Df Residuals:                    6418   BIC:                         2.587e+04\n",
            "Df Model:                          56                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.7514      0.021    315.041      0.000       6.709       6.793\n",
            "gender                0.1455      0.037      3.965      0.000       0.074       0.217\n",
            "condtn                0.0217      0.023      0.925      0.355      -0.024       0.068\n",
            "order                -0.1568      0.023     -6.903      0.000      -0.201      -0.112\n",
            "int_corr              0.0797      0.022      3.603      0.000       0.036       0.123\n",
            "samerace              0.0211      0.022      0.967      0.334      -0.022       0.064\n",
            "age_o                -0.0094      0.031     -0.306      0.759      -0.070       0.051\n",
            "mn_sat_o             -0.0665      0.028     -2.357      0.018      -0.122      -0.011\n",
            "tuition_o            -0.0628      0.028     -2.213      0.027      -0.118      -0.007\n",
            "income               -0.0009      0.031     -0.028      0.977      -0.061       0.059\n",
            "exphappy_o            0.1749      0.023      7.528      0.000       0.129       0.220\n",
            "met_o                -0.1248      0.022     -5.692      0.000      -0.168      -0.082\n",
            "world_rank_o         -0.1565      0.029     -5.343      0.000      -0.214      -0.099\n",
            "masters_o            -0.0949      0.028     -3.356      0.001      -0.150      -0.039\n",
            "sinc1_1              -0.1023      0.035     -2.952      0.003      -0.170      -0.034\n",
            "intel1_1             -0.0354      0.036     -0.971      0.331      -0.107       0.036\n",
            "fun1_1               -0.0733      0.032     -2.275      0.023      -0.137      -0.010\n",
            "amb1_1                0.0404      0.033      1.229      0.219      -0.024       0.105\n",
            "shar1_1              -0.0098      0.033     -0.295      0.768      -0.075       0.056\n",
            "age_diff             -0.0214      0.031     -0.692      0.489      -0.082       0.039\n",
            "income_diff          -0.0357      0.031     -1.152      0.249      -0.096       0.025\n",
            "date_diff            -0.0203      0.024     -0.849      0.396      -0.067       0.027\n",
            "go_out_diff           0.0224      0.024      0.930      0.352      -0.025       0.070\n",
            "sports_diff           0.0119      0.031      0.387      0.698      -0.048       0.072\n",
            "tvsport_diff          0.0015      0.028      0.055      0.956      -0.054       0.057\n",
            "exercise_diff        -0.0066      0.025     -0.265      0.791      -0.056       0.042\n",
            "dining_diff          -0.1117      0.026     -4.216      0.000      -0.164      -0.060\n",
            "museums_diff          0.0929      0.046      2.033      0.042       0.003       0.182\n",
            "art_diff             -0.0333      0.044     -0.759      0.448      -0.119       0.053\n",
            "hiking_diff          -0.0680      0.025     -2.703      0.007      -0.117      -0.019\n",
            "gaming_diff           0.0430      0.026      1.651      0.099      -0.008       0.094\n",
            "clubbing_diff        -0.0133      0.023     -0.572      0.567      -0.059       0.032\n",
            "reading_diff          0.1379      0.024      5.712      0.000       0.091       0.185\n",
            "tv_diff               0.0667      0.029      2.307      0.021       0.010       0.123\n",
            "theater_diff         -0.0438      0.032     -1.351      0.177      -0.107       0.020\n",
            "movies_diff           0.0922      0.028      3.266      0.001       0.037       0.148\n",
            "concerts_diff         0.0913      0.033      2.786      0.005       0.027       0.156\n",
            "music_diff            0.0224      0.030      0.756      0.450      -0.036       0.080\n",
            "shopping_diff         0.0322      0.031      1.050      0.294      -0.028       0.092\n",
            "yoga_diff            -0.0227      0.025     -0.909      0.363      -0.072       0.026\n",
            "worldrank_diff        0.0019      0.029      0.066      0.947      -0.054       0.058\n",
            "(3_1-pf_o)_att       -0.1146      0.040     -2.892      0.004      -0.192      -0.037\n",
            "(3_1-pf_o)_sinc      -0.0574      0.029     -1.957      0.050      -0.115       0.000\n",
            "(3_1-pf_o)_fun        0.0564      0.027      2.085      0.037       0.003       0.109\n",
            "(3_1-pf_o)_intel      0.0494      0.029      1.695      0.090      -0.008       0.107\n",
            "(3_1-pf_o)_amb       -0.1760      0.030     -5.935      0.000      -0.234      -0.118\n",
            "(1_1-2_1_o)_sinc      0.0128      0.036      0.357      0.721      -0.058       0.083\n",
            "(1_1-2_1_o)_fun       0.0350      0.033      1.069      0.285      -0.029       0.099\n",
            "(1_1-2_1_o)_intel    -0.0404      0.036     -1.131      0.258      -0.110       0.030\n",
            "(1_1-2_1_o)_amb      -0.0469      0.031     -1.512      0.131      -0.108       0.014\n",
            "(1_1-2_1_o)_shar     -0.0099      0.035     -0.285      0.776      -0.078       0.058\n",
            "from_m               -0.0204      0.022     -0.947      0.344      -0.063       0.022\n",
            "goal_m               -0.0114      0.022     -0.525      0.599      -0.054       0.031\n",
            "imprace_m            -0.0272      0.022     -1.230      0.219      -0.071       0.016\n",
            "imprelig_m            0.0382      0.022      1.724      0.085      -0.005       0.082\n",
            "career_c_m            0.0216      0.022      0.969      0.333      -0.022       0.065\n",
            "masters_m            -0.0200      0.028     -0.724      0.469      -0.074       0.034\n",
            "==============================================================================\n",
            "Omnibus:                      168.009   Durbin-Watson:                   1.988\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              184.101\n",
            "Skew:                          -0.382   Prob(JB):                     1.05e-40\n",
            "Kurtosis:                       3.315   Cond. No.                         6.28\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhSzjTEg8zjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530b9730-740e-4ae1-b2c4-0337a709290d"
      },
      "source": [
        "#Check VIF values again\n",
        "amb_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(X_train, amb_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "museums_diff         4.541766\n",
              "art_diff             4.197169\n",
              "(3_1-pf_o)_att       3.420823\n",
              "gender               2.930325\n",
              "intel1_1             2.889620\n",
              "(1_1-2_1_o)_sinc     2.811507\n",
              "(1_1-2_1_o)_intel    2.777644\n",
              "(1_1-2_1_o)_shar     2.645535\n",
              "sinc1_1              2.617768\n",
              "shar1_1              2.426319\n",
              "amb1_1               2.351199\n",
              "concerts_diff        2.337299\n",
              "(1_1-2_1_o)_fun      2.332280\n",
              "theater_diff         2.292683\n",
              "fun1_1               2.261990\n",
              "(1_1-2_1_o)_amb      2.098396\n",
              "income_diff          2.088078\n",
              "age_diff             2.086715\n",
              "age_o                2.067219\n",
              "shopping_diff        2.054380\n",
              "sports_diff          2.046133\n",
              "income               2.028244\n",
              "(3_1-pf_o)_amb       1.915229\n",
              "music_diff           1.905844\n",
              "(3_1-pf_o)_sinc      1.872735\n",
              "world_rank_o         1.869021\n",
              "(3_1-pf_o)_intel     1.849470\n",
              "tv_diff              1.821491\n",
              "worldrank_diff       1.791270\n",
              "tuition_o            1.751295\n",
              "tvsport_diff         1.750397\n",
              "masters_o            1.741015\n",
              "movies_diff          1.735546\n",
              "mn_sat_o             1.732489\n",
              "masters_m            1.663827\n",
              "(3_1-pf_o)_fun       1.593343\n",
              "dining_diff          1.526870\n",
              "gaming_diff          1.475922\n",
              "hiking_diff          1.378499\n",
              "exercise_diff        1.359534\n",
              "yoga_diff            1.355573\n",
              "reading_diff         1.268428\n",
              "go_out_diff          1.266613\n",
              "date_diff            1.244179\n",
              "condtn               1.199649\n",
              "clubbing_diff        1.177651\n",
              "exphappy_o           1.175452\n",
              "order                1.123707\n",
              "career_c_m           1.078700\n",
              "imprelig_m           1.067944\n",
              "int_corr             1.066581\n",
              "imprace_m            1.065706\n",
              "met_o                1.046389\n",
              "samerace             1.033158\n",
              "goal_m               1.019923\n",
              "from_m               1.010171\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0VmB9tq8-eX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d822db-98d3-48c5-a132-c6fe31d78037"
      },
      "source": [
        "#drop 'museums_diff' since it has the highest VIF value now\n",
        "X_train = X_train.drop(columns=['museums_diff'])\n",
        "\n",
        "amb_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(amb_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  amb_o   R-squared:                       0.086\n",
            "Model:                            OLS   Adj. R-squared:                  0.078\n",
            "Method:                 Least Squares   F-statistic:                     10.94\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           3.12e-88\n",
            "Time:                        23:12:59   Log-Likelihood:                -12689.\n",
            "No. Observations:                6475   AIC:                         2.549e+04\n",
            "Df Residuals:                    6419   BIC:                         2.587e+04\n",
            "Df Model:                          55                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.7514      0.021    314.964      0.000       6.709       6.793\n",
            "gender                0.1401      0.037      3.828      0.000       0.068       0.212\n",
            "condtn                0.0214      0.023      0.910      0.363      -0.025       0.067\n",
            "order                -0.1565      0.023     -6.889      0.000      -0.201      -0.112\n",
            "int_corr              0.0800      0.022      3.615      0.000       0.037       0.123\n",
            "samerace              0.0214      0.022      0.984      0.325      -0.021       0.064\n",
            "age_o                -0.0073      0.031     -0.238      0.812      -0.068       0.053\n",
            "mn_sat_o             -0.0675      0.028     -2.392      0.017      -0.123      -0.012\n",
            "tuition_o            -0.0617      0.028     -2.175      0.030      -0.117      -0.006\n",
            "income               -0.0021      0.031     -0.068      0.945      -0.062       0.058\n",
            "exphappy_o            0.1737      0.023      7.477      0.000       0.128       0.219\n",
            "met_o                -0.1242      0.022     -5.665      0.000      -0.167      -0.081\n",
            "world_rank_o         -0.1569      0.029     -5.353      0.000      -0.214      -0.099\n",
            "masters_o            -0.0951      0.028     -3.363      0.001      -0.151      -0.040\n",
            "sinc1_1              -0.1044      0.035     -3.010      0.003      -0.172      -0.036\n",
            "intel1_1             -0.0422      0.036     -1.162      0.245      -0.113       0.029\n",
            "fun1_1               -0.0708      0.032     -2.197      0.028      -0.134      -0.008\n",
            "amb1_1                0.0428      0.033      1.302      0.193      -0.022       0.107\n",
            "shar1_1              -0.0100      0.033     -0.300      0.764      -0.075       0.055\n",
            "age_diff             -0.0207      0.031     -0.669      0.504      -0.081       0.040\n",
            "income_diff          -0.0399      0.031     -1.291      0.197      -0.100       0.021\n",
            "date_diff            -0.0197      0.024     -0.823      0.410      -0.067       0.027\n",
            "go_out_diff           0.0234      0.024      0.969      0.333      -0.024       0.071\n",
            "sports_diff           0.0125      0.031      0.408      0.683      -0.048       0.073\n",
            "tvsport_diff          0.0057      0.028      0.202      0.840      -0.050       0.061\n",
            "exercise_diff        -0.0076      0.025     -0.304      0.761      -0.057       0.041\n",
            "dining_diff          -0.1021      0.026     -3.917      0.000      -0.153      -0.051\n",
            "art_diff              0.0345      0.029      1.206      0.228      -0.022       0.090\n",
            "hiking_diff          -0.0652      0.025     -2.594      0.009      -0.114      -0.016\n",
            "gaming_diff           0.0386      0.026      1.487      0.137      -0.012       0.089\n",
            "clubbing_diff        -0.0119      0.023     -0.513      0.608      -0.058       0.034\n",
            "reading_diff          0.1480      0.024      6.261      0.000       0.102       0.194\n",
            "tv_diff               0.0642      0.029      2.220      0.026       0.008       0.121\n",
            "theater_diff         -0.0325      0.032     -1.015      0.310      -0.095       0.030\n",
            "movies_diff           0.0900      0.028      3.188      0.001       0.035       0.145\n",
            "concerts_diff         0.0914      0.033      2.789      0.005       0.027       0.156\n",
            "music_diff            0.0233      0.030      0.786      0.432      -0.035       0.081\n",
            "shopping_diff         0.0385      0.031      1.259      0.208      -0.021       0.098\n",
            "yoga_diff            -0.0256      0.025     -1.027      0.304      -0.074       0.023\n",
            "worldrank_diff        0.0050      0.029      0.175      0.861      -0.051       0.061\n",
            "(3_1-pf_o)_att       -0.1137      0.040     -2.868      0.004      -0.191      -0.036\n",
            "(3_1-pf_o)_sinc      -0.0586      0.029     -1.999      0.046      -0.116      -0.001\n",
            "(3_1-pf_o)_fun        0.0572      0.027      2.113      0.035       0.004       0.110\n",
            "(3_1-pf_o)_intel      0.0459      0.029      1.578      0.115      -0.011       0.103\n",
            "(3_1-pf_o)_amb       -0.1741      0.030     -5.871      0.000      -0.232      -0.116\n",
            "(1_1-2_1_o)_sinc      0.0138      0.036      0.384      0.701      -0.057       0.084\n",
            "(1_1-2_1_o)_fun       0.0329      0.033      1.004      0.315      -0.031       0.097\n",
            "(1_1-2_1_o)_intel    -0.0362      0.036     -1.014      0.311      -0.106       0.034\n",
            "(1_1-2_1_o)_amb      -0.0514      0.031     -1.658      0.097      -0.112       0.009\n",
            "(1_1-2_1_o)_shar     -0.0101      0.035     -0.291      0.771      -0.078       0.058\n",
            "from_m               -0.0201      0.022     -0.934      0.351      -0.062       0.022\n",
            "goal_m               -0.0120      0.022     -0.554      0.580      -0.054       0.030\n",
            "imprace_m            -0.0277      0.022     -1.250      0.211      -0.071       0.016\n",
            "imprelig_m            0.0383      0.022      1.728      0.084      -0.005       0.082\n",
            "career_c_m            0.0218      0.022      0.977      0.329      -0.022       0.065\n",
            "masters_m            -0.0194      0.028     -0.703      0.482      -0.074       0.035\n",
            "==============================================================================\n",
            "Omnibus:                      171.638   Durbin-Watson:                   1.988\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              188.441\n",
            "Skew:                          -0.386   Prob(JB):                     1.20e-41\n",
            "Kurtosis:                       3.320   Cond. No.                         5.84\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sUk1e_S9KZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32353998-7cad-4594-bc76-d458678f26b5"
      },
      "source": [
        "#Check VIF values again\n",
        "amb_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(X_train, amb_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3_1-pf_o)_att       3.420383\n",
              "gender               2.915146\n",
              "intel1_1             2.865390\n",
              "(1_1-2_1_o)_sinc     2.810981\n",
              "(1_1-2_1_o)_intel    2.768172\n",
              "(1_1-2_1_o)_shar     2.645514\n",
              "sinc1_1              2.615632\n",
              "shar1_1              2.426298\n",
              "amb1_1               2.348208\n",
              "concerts_diff        2.337292\n",
              "(1_1-2_1_o)_fun      2.329898\n",
              "fun1_1               2.258588\n",
              "theater_diff         2.224635\n",
              "(1_1-2_1_o)_amb      2.088006\n",
              "age_diff             2.086451\n",
              "income_diff          2.078722\n",
              "age_o                2.064890\n",
              "sports_diff          2.045913\n",
              "shopping_diff        2.033868\n",
              "income               2.027454\n",
              "(3_1-pf_o)_amb       1.913267\n",
              "music_diff           1.905428\n",
              "(3_1-pf_o)_sinc      1.871904\n",
              "world_rank_o         1.868964\n",
              "(3_1-pf_o)_intel     1.843109\n",
              "tv_diff              1.818018\n",
              "worldrank_diff       1.786211\n",
              "art_diff             1.775966\n",
              "tuition_o            1.750688\n",
              "tvsport_diff         1.741261\n",
              "masters_o            1.740990\n",
              "movies_diff          1.732914\n",
              "mn_sat_o             1.731956\n",
              "masters_m            1.663644\n",
              "(3_1-pf_o)_fun       1.593029\n",
              "dining_diff          1.478948\n",
              "gaming_diff          1.465760\n",
              "hiking_diff          1.374335\n",
              "exercise_diff        1.359041\n",
              "yoga_diff            1.351142\n",
              "go_out_diff          1.266154\n",
              "date_diff            1.243983\n",
              "reading_diff         1.215025\n",
              "condtn               1.199591\n",
              "clubbing_diff        1.176677\n",
              "exphappy_o           1.174710\n",
              "order                1.123662\n",
              "career_c_m           1.078682\n",
              "imprelig_m           1.067938\n",
              "int_corr             1.066533\n",
              "imprace_m            1.065600\n",
              "met_o                1.046211\n",
              "samerace             1.033083\n",
              "goal_m               1.019722\n",
              "from_m               1.010128\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lixq6JD-9T6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc38188-e127-4abb-c662-386368329ed1"
      },
      "source": [
        "#drop '(3_1-pf_o)_att' since it has the highest VIF value now\n",
        "X_train = X_train.drop(columns=['(3_1-pf_o)_att'])\n",
        "\n",
        "amb_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(amb_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  amb_o   R-squared:                       0.085\n",
            "Model:                            OLS   Adj. R-squared:                  0.077\n",
            "Method:                 Least Squares   F-statistic:                     10.98\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           3.96e-87\n",
            "Time:                        23:13:01   Log-Likelihood:                -12693.\n",
            "No. Observations:                6475   AIC:                         2.550e+04\n",
            "Df Residuals:                    6420   BIC:                         2.587e+04\n",
            "Df Model:                          54                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.7514      0.021    314.787      0.000       6.709       6.793\n",
            "gender                0.1165      0.036      3.265      0.001       0.047       0.186\n",
            "condtn                0.0192      0.023      0.820      0.412      -0.027       0.065\n",
            "order                -0.1563      0.023     -6.874      0.000      -0.201      -0.112\n",
            "int_corr              0.0771      0.022      3.486      0.000       0.034       0.121\n",
            "samerace              0.0217      0.022      0.993      0.321      -0.021       0.064\n",
            "age_o                -0.0091      0.031     -0.296      0.767      -0.070       0.051\n",
            "mn_sat_o             -0.0680      0.028     -2.409      0.016      -0.123      -0.013\n",
            "tuition_o            -0.0600      0.028     -2.114      0.035      -0.116      -0.004\n",
            "income               -0.0070      0.030     -0.228      0.819      -0.067       0.053\n",
            "exphappy_o            0.1740      0.023      7.483      0.000       0.128       0.220\n",
            "met_o                -0.1247      0.022     -5.682      0.000      -0.168      -0.082\n",
            "world_rank_o         -0.1546      0.029     -5.275      0.000      -0.212      -0.097\n",
            "masters_o            -0.0985      0.028     -3.485      0.000      -0.154      -0.043\n",
            "sinc1_1              -0.1169      0.034     -3.396      0.001      -0.184      -0.049\n",
            "intel1_1             -0.0396      0.036     -1.092      0.275      -0.111       0.032\n",
            "fun1_1               -0.0751      0.032     -2.332      0.020      -0.138      -0.012\n",
            "amb1_1                0.0235      0.032      0.732      0.464      -0.040       0.087\n",
            "shar1_1              -0.0316      0.033     -0.971      0.332      -0.095       0.032\n",
            "age_diff             -0.0183      0.031     -0.590      0.555      -0.079       0.042\n",
            "income_diff          -0.0448      0.031     -1.450      0.147      -0.105       0.016\n",
            "date_diff            -0.0255      0.024     -1.069      0.285      -0.072       0.021\n",
            "go_out_diff           0.0210      0.024      0.871      0.384      -0.026       0.068\n",
            "sports_diff           0.0204      0.031      0.667      0.505      -0.040       0.080\n",
            "tvsport_diff          0.0064      0.028      0.224      0.822      -0.049       0.062\n",
            "exercise_diff        -0.0031      0.025     -0.124      0.901      -0.052       0.046\n",
            "dining_diff          -0.0991      0.026     -3.803      0.000      -0.150      -0.048\n",
            "art_diff              0.0355      0.029      1.242      0.214      -0.021       0.092\n",
            "hiking_diff          -0.0635      0.025     -2.524      0.012      -0.113      -0.014\n",
            "gaming_diff           0.0373      0.026      1.436      0.151      -0.014       0.088\n",
            "clubbing_diff        -0.0102      0.023     -0.438      0.661      -0.056       0.035\n",
            "reading_diff          0.1497      0.024      6.334      0.000       0.103       0.196\n",
            "tv_diff               0.0613      0.029      2.121      0.034       0.005       0.118\n",
            "theater_diff         -0.0340      0.032     -1.064      0.287      -0.097       0.029\n",
            "movies_diff           0.0924      0.028      3.275      0.001       0.037       0.148\n",
            "concerts_diff         0.0838      0.033      2.563      0.010       0.020       0.148\n",
            "music_diff            0.0273      0.030      0.924      0.355      -0.031       0.085\n",
            "shopping_diff         0.0487      0.030      1.605      0.109      -0.011       0.108\n",
            "yoga_diff            -0.0252      0.025     -1.011      0.312      -0.074       0.024\n",
            "worldrank_diff        0.0067      0.029      0.234      0.815      -0.049       0.063\n",
            "(3_1-pf_o)_sinc      -0.0124      0.025     -0.505      0.614      -0.060       0.036\n",
            "(3_1-pf_o)_fun        0.0908      0.024      3.725      0.000       0.043       0.139\n",
            "(3_1-pf_o)_intel      0.0937      0.024      3.924      0.000       0.047       0.141\n",
            "(3_1-pf_o)_amb       -0.1306      0.025     -5.122      0.000      -0.181      -0.081\n",
            "(1_1-2_1_o)_sinc      0.0353      0.035      1.003      0.316      -0.034       0.104\n",
            "(1_1-2_1_o)_fun       0.0427      0.033      1.310      0.190      -0.021       0.106\n",
            "(1_1-2_1_o)_intel    -0.0321      0.036     -0.899      0.369      -0.102       0.038\n",
            "(1_1-2_1_o)_amb      -0.0350      0.030     -1.148      0.251      -0.095       0.025\n",
            "(1_1-2_1_o)_shar      0.0244      0.033      0.745      0.456      -0.040       0.089\n",
            "from_m               -0.0191      0.022     -0.888      0.375      -0.061       0.023\n",
            "goal_m               -0.0132      0.022     -0.611      0.541      -0.056       0.029\n",
            "imprace_m            -0.0284      0.022     -1.281      0.200      -0.072       0.015\n",
            "imprelig_m            0.0405      0.022      1.829      0.067      -0.003       0.084\n",
            "career_c_m            0.0215      0.022      0.963      0.336      -0.022       0.065\n",
            "masters_m            -0.0198      0.028     -0.715      0.475      -0.074       0.034\n",
            "==============================================================================\n",
            "Omnibus:                      171.126   Durbin-Watson:                   1.987\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              188.103\n",
            "Skew:                          -0.384   Prob(JB):                     1.43e-41\n",
            "Kurtosis:                       3.326   Cond. No.                         5.23\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj0D6E4M9kJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a49920-8473-444a-9dd3-6faba5814f0a"
      },
      "source": [
        "#Check VIF values again\n",
        "amb_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(X_train, amb_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "intel1_1             2.863669\n",
              "gender               2.767929\n",
              "(1_1-2_1_o)_intel    2.763741\n",
              "(1_1-2_1_o)_sinc     2.689244\n",
              "sinc1_1              2.574314\n",
              "(1_1-2_1_o)_shar     2.330162\n",
              "concerts_diff        2.321894\n",
              "(1_1-2_1_o)_fun      2.304496\n",
              "shar1_1              2.303232\n",
              "fun1_1               2.253657\n",
              "amb1_1               2.250375\n",
              "theater_diff         2.223974\n",
              "age_diff             2.084868\n",
              "income_diff          2.072401\n",
              "age_o                2.064028\n",
              "sports_diff          2.029620\n",
              "income               2.021172\n",
              "(1_1-2_1_o)_amb      2.016894\n",
              "shopping_diff        2.006044\n",
              "music_diff           1.901041\n",
              "world_rank_o         1.867622\n",
              "tv_diff              1.815838\n",
              "worldrank_diff       1.785438\n",
              "art_diff             1.775679\n",
              "tuition_o            1.749917\n",
              "tvsport_diff         1.741152\n",
              "masters_o            1.737898\n",
              "mn_sat_o             1.731888\n",
              "movies_diff          1.731304\n",
              "masters_m            1.663612\n",
              "dining_diff          1.476581\n",
              "gaming_diff          1.465318\n",
              "(3_1-pf_o)_amb       1.412582\n",
              "hiking_diff          1.373530\n",
              "exercise_diff        1.353708\n",
              "yoga_diff            1.351102\n",
              "(3_1-pf_o)_sinc      1.305605\n",
              "(3_1-pf_o)_fun       1.293013\n",
              "go_out_diff          1.264678\n",
              "(3_1-pf_o)_intel     1.239389\n",
              "date_diff            1.235117\n",
              "reading_diff         1.214212\n",
              "condtn               1.198395\n",
              "clubbing_diff        1.175866\n",
              "exphappy_o           1.174697\n",
              "order                1.123647\n",
              "career_c_m           1.078659\n",
              "imprelig_m           1.066598\n",
              "imprace_m            1.065475\n",
              "int_corr             1.064308\n",
              "met_o                1.046156\n",
              "samerace             1.033071\n",
              "goal_m               1.019307\n",
              "from_m               1.009874\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoSJRyIP98TP"
      },
      "source": [
        "All VIF values look small, let remove the large p-value next"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg_7zwzZ90bx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6863626-a00b-4280-d4f3-44f890ce3afd"
      },
      "source": [
        "#from our newest model, let's remove 'exercise_diff' which has the largest p-value\n",
        "X_train = X_train.drop(columns=['exercise_diff'])\n",
        "\n",
        "amb_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(amb_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  amb_o   R-squared:                       0.085\n",
            "Model:                            OLS   Adj. R-squared:                  0.077\n",
            "Method:                 Least Squares   F-statistic:                     11.19\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           1.18e-87\n",
            "Time:                        23:13:03   Log-Likelihood:                -12693.\n",
            "No. Observations:                6475   AIC:                         2.549e+04\n",
            "Df Residuals:                    6421   BIC:                         2.586e+04\n",
            "Df Model:                          53                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.7514      0.021    314.811      0.000       6.709       6.793\n",
            "gender                0.1159      0.035      3.277      0.001       0.047       0.185\n",
            "condtn                0.0192      0.023      0.819      0.413      -0.027       0.065\n",
            "order                -0.1563      0.023     -6.876      0.000      -0.201      -0.112\n",
            "int_corr              0.0772      0.022      3.487      0.000       0.034       0.121\n",
            "samerace              0.0216      0.022      0.993      0.321      -0.021       0.064\n",
            "age_o                -0.0092      0.031     -0.298      0.766      -0.070       0.051\n",
            "mn_sat_o             -0.0679      0.028     -2.406      0.016      -0.123      -0.013\n",
            "tuition_o            -0.0600      0.028     -2.114      0.035      -0.116      -0.004\n",
            "income               -0.0069      0.030     -0.227      0.820      -0.067       0.053\n",
            "exphappy_o            0.1740      0.023      7.485      0.000       0.128       0.220\n",
            "met_o                -0.1246      0.022     -5.681      0.000      -0.168      -0.082\n",
            "world_rank_o         -0.1546      0.029     -5.275      0.000      -0.212      -0.097\n",
            "masters_o            -0.0985      0.028     -3.483      0.000      -0.154      -0.043\n",
            "sinc1_1              -0.1170      0.034     -3.404      0.001      -0.184      -0.050\n",
            "intel1_1             -0.0397      0.036     -1.094      0.274      -0.111       0.031\n",
            "fun1_1               -0.0753      0.032     -2.344      0.019      -0.138      -0.012\n",
            "amb1_1                0.0236      0.032      0.733      0.464      -0.039       0.087\n",
            "shar1_1              -0.0317      0.033     -0.973      0.331      -0.095       0.032\n",
            "age_diff             -0.0183      0.031     -0.590      0.555      -0.079       0.042\n",
            "income_diff          -0.0449      0.031     -1.457      0.145      -0.105       0.016\n",
            "date_diff            -0.0253      0.024     -1.064      0.287      -0.072       0.021\n",
            "go_out_diff           0.0210      0.024      0.870      0.385      -0.026       0.068\n",
            "sports_diff           0.0191      0.029      0.664      0.507      -0.037       0.075\n",
            "tvsport_diff          0.0061      0.028      0.216      0.829      -0.049       0.061\n",
            "dining_diff          -0.0993      0.026     -3.811      0.000      -0.150      -0.048\n",
            "art_diff              0.0356      0.029      1.247      0.212      -0.020       0.092\n",
            "hiking_diff          -0.0635      0.025     -2.527      0.012      -0.113      -0.014\n",
            "gaming_diff           0.0374      0.026      1.442      0.149      -0.013       0.088\n",
            "clubbing_diff        -0.0101      0.023     -0.435      0.663      -0.056       0.035\n",
            "reading_diff          0.1496      0.024      6.335      0.000       0.103       0.196\n",
            "tv_diff               0.0612      0.029      2.118      0.034       0.005       0.118\n",
            "theater_diff         -0.0343      0.032     -1.076      0.282      -0.097       0.028\n",
            "movies_diff           0.0925      0.028      3.280      0.001       0.037       0.148\n",
            "concerts_diff         0.0842      0.033      2.589      0.010       0.020       0.148\n",
            "music_diff            0.0273      0.030      0.924      0.356      -0.031       0.085\n",
            "shopping_diff         0.0486      0.030      1.601      0.109      -0.011       0.108\n",
            "yoga_diff            -0.0255      0.025     -1.027      0.304      -0.074       0.023\n",
            "worldrank_diff        0.0069      0.029      0.242      0.809      -0.049       0.063\n",
            "(3_1-pf_o)_sinc      -0.0126      0.024     -0.513      0.608      -0.061       0.035\n",
            "(3_1-pf_o)_fun        0.0907      0.024      3.723      0.000       0.043       0.138\n",
            "(3_1-pf_o)_intel      0.0937      0.024      3.923      0.000       0.047       0.140\n",
            "(3_1-pf_o)_amb       -0.1305      0.025     -5.122      0.000      -0.180      -0.081\n",
            "(1_1-2_1_o)_sinc      0.0351      0.035      0.999      0.318      -0.034       0.104\n",
            "(1_1-2_1_o)_fun       0.0427      0.033      1.313      0.189      -0.021       0.107\n",
            "(1_1-2_1_o)_intel    -0.0321      0.036     -0.899      0.369      -0.102       0.038\n",
            "(1_1-2_1_o)_amb      -0.0351      0.030     -1.153      0.249      -0.095       0.025\n",
            "(1_1-2_1_o)_shar      0.0242      0.033      0.740      0.459      -0.040       0.088\n",
            "from_m               -0.0192      0.022     -0.889      0.374      -0.061       0.023\n",
            "goal_m               -0.0133      0.022     -0.612      0.540      -0.056       0.029\n",
            "imprace_m            -0.0284      0.022     -1.281      0.200      -0.072       0.015\n",
            "imprelig_m            0.0405      0.022      1.829      0.067      -0.003       0.084\n",
            "career_c_m            0.0215      0.022      0.963      0.335      -0.022       0.065\n",
            "masters_m            -0.0198      0.028     -0.715      0.474      -0.074       0.034\n",
            "==============================================================================\n",
            "Omnibus:                      171.042   Durbin-Watson:                   1.987\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              188.006\n",
            "Skew:                          -0.384   Prob(JB):                     1.50e-41\n",
            "Kurtosis:                       3.326   Cond. No.                         5.22\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nofafVvO-zIq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44222f49-f2f0-4210-ca2c-cd68d01cd828"
      },
      "source": [
        "#from our newest model, let's remove 'tvsport_diff' which has the largest p-value\n",
        "X_train = X_train.drop(columns=['tvsport_diff'])\n",
        "\n",
        "amb_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(amb_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  amb_o   R-squared:                       0.085\n",
            "Model:                            OLS   Adj. R-squared:                  0.077\n",
            "Method:                 Least Squares   F-statistic:                     11.41\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           3.56e-88\n",
            "Time:                        23:13:03   Log-Likelihood:                -12693.\n",
            "No. Observations:                6475   AIC:                         2.549e+04\n",
            "Df Residuals:                    6422   BIC:                         2.585e+04\n",
            "Df Model:                          52                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.7514      0.021    314.834      0.000       6.709       6.793\n",
            "gender                0.1157      0.035      3.273      0.001       0.046       0.185\n",
            "condtn                0.0192      0.023      0.819      0.413      -0.027       0.065\n",
            "order                -0.1563      0.023     -6.875      0.000      -0.201      -0.112\n",
            "int_corr              0.0772      0.022      3.490      0.000       0.034       0.121\n",
            "samerace              0.0216      0.022      0.992      0.321      -0.021       0.064\n",
            "age_o                -0.0093      0.031     -0.301      0.764      -0.070       0.051\n",
            "mn_sat_o             -0.0681      0.028     -2.417      0.016      -0.123      -0.013\n",
            "tuition_o            -0.0597      0.028     -2.107      0.035      -0.115      -0.004\n",
            "income               -0.0070      0.030     -0.231      0.817      -0.067       0.053\n",
            "exphappy_o            0.1739      0.023      7.483      0.000       0.128       0.219\n",
            "met_o                -0.1245      0.022     -5.679      0.000      -0.168      -0.082\n",
            "world_rank_o         -0.1546      0.029     -5.276      0.000      -0.212      -0.097\n",
            "masters_o            -0.0981      0.028     -3.477      0.001      -0.153      -0.043\n",
            "sinc1_1              -0.1166      0.034     -3.397      0.001      -0.184      -0.049\n",
            "intel1_1             -0.0397      0.036     -1.095      0.274      -0.111       0.031\n",
            "fun1_1               -0.0758      0.032     -2.363      0.018      -0.139      -0.013\n",
            "amb1_1                0.0237      0.032      0.737      0.461      -0.039       0.087\n",
            "shar1_1              -0.0320      0.033     -0.984      0.325      -0.096       0.032\n",
            "age_diff             -0.0183      0.031     -0.591      0.554      -0.079       0.042\n",
            "income_diff          -0.0450      0.031     -1.458      0.145      -0.105       0.015\n",
            "date_diff            -0.0250      0.024     -1.051      0.293      -0.071       0.022\n",
            "go_out_diff           0.0212      0.024      0.880      0.379      -0.026       0.068\n",
            "sports_diff           0.0221      0.025      0.878      0.380      -0.027       0.071\n",
            "dining_diff          -0.0991      0.026     -3.807      0.000      -0.150      -0.048\n",
            "art_diff              0.0357      0.029      1.252      0.211      -0.020       0.092\n",
            "hiking_diff          -0.0640      0.025     -2.554      0.011      -0.113      -0.015\n",
            "gaming_diff           0.0382      0.026      1.487      0.137      -0.012       0.089\n",
            "clubbing_diff        -0.0100      0.023     -0.429      0.668      -0.056       0.036\n",
            "reading_diff          0.1491      0.024      6.342      0.000       0.103       0.195\n",
            "tv_diff               0.0629      0.028      2.274      0.023       0.009       0.117\n",
            "theater_diff         -0.0342      0.032     -1.072      0.284      -0.097       0.028\n",
            "movies_diff           0.0919      0.028      3.275      0.001       0.037       0.147\n",
            "concerts_diff         0.0845      0.032      2.603      0.009       0.021       0.148\n",
            "music_diff            0.0275      0.030      0.932      0.351      -0.030       0.085\n",
            "shopping_diff         0.0483      0.030      1.592      0.111      -0.011       0.108\n",
            "yoga_diff            -0.0257      0.025     -1.036      0.300      -0.074       0.023\n",
            "worldrank_diff        0.0070      0.029      0.246      0.806      -0.049       0.063\n",
            "(3_1-pf_o)_sinc      -0.0127      0.024     -0.518      0.605      -0.061       0.035\n",
            "(3_1-pf_o)_fun        0.0904      0.024      3.717      0.000       0.043       0.138\n",
            "(3_1-pf_o)_intel      0.0936      0.024      3.922      0.000       0.047       0.140\n",
            "(3_1-pf_o)_amb       -0.1305      0.025     -5.123      0.000      -0.181      -0.081\n",
            "(1_1-2_1_o)_sinc      0.0345      0.035      0.985      0.324      -0.034       0.103\n",
            "(1_1-2_1_o)_fun       0.0436      0.032      1.347      0.178      -0.020       0.107\n",
            "(1_1-2_1_o)_intel    -0.0319      0.036     -0.895      0.371      -0.102       0.038\n",
            "(1_1-2_1_o)_amb      -0.0353      0.030     -1.160      0.246      -0.095       0.024\n",
            "(1_1-2_1_o)_shar      0.0247      0.033      0.757      0.449      -0.039       0.089\n",
            "from_m               -0.0191      0.022     -0.888      0.375      -0.061       0.023\n",
            "goal_m               -0.0132      0.022     -0.609      0.543      -0.056       0.029\n",
            "imprace_m            -0.0283      0.022     -1.281      0.200      -0.072       0.015\n",
            "imprelig_m            0.0405      0.022      1.829      0.068      -0.003       0.084\n",
            "career_c_m            0.0214      0.022      0.961      0.337      -0.022       0.065\n",
            "masters_m            -0.0196      0.028     -0.711      0.477      -0.074       0.035\n",
            "==============================================================================\n",
            "Omnibus:                      170.598   Durbin-Watson:                   1.987\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              187.494\n",
            "Skew:                          -0.384   Prob(JB):                     1.93e-41\n",
            "Kurtosis:                       3.325   Cond. No.                         5.19\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n616fsvp_S38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ea07e7-67af-4495-c438-782259aa8a27"
      },
      "source": [
        "#from our newest model, let's remove 'income' which has the largest p-value\n",
        "X_train = X_train.drop(columns=['income'])\n",
        "\n",
        "amb_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(amb_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  amb_o   R-squared:                       0.085\n",
            "Model:                            OLS   Adj. R-squared:                  0.077\n",
            "Method:                 Least Squares   F-statistic:                     11.63\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           1.06e-88\n",
            "Time:                        23:13:03   Log-Likelihood:                -12693.\n",
            "No. Observations:                6475   AIC:                         2.549e+04\n",
            "Df Residuals:                    6423   BIC:                         2.584e+04\n",
            "Df Model:                          51                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.7514      0.021    314.858      0.000       6.709       6.793\n",
            "gender                0.1156      0.035      3.270      0.001       0.046       0.185\n",
            "condtn                0.0195      0.023      0.830      0.406      -0.026       0.065\n",
            "order                -0.1563      0.023     -6.878      0.000      -0.201      -0.112\n",
            "int_corr              0.0771      0.022      3.485      0.000       0.034       0.120\n",
            "samerace              0.0213      0.022      0.981      0.327      -0.021       0.064\n",
            "age_o                -0.0089      0.031     -0.288      0.773      -0.069       0.051\n",
            "mn_sat_o             -0.0680      0.028     -2.415      0.016      -0.123      -0.013\n",
            "tuition_o            -0.0600      0.028     -2.118      0.034      -0.116      -0.004\n",
            "exphappy_o            0.1739      0.023      7.487      0.000       0.128       0.219\n",
            "met_o                -0.1244      0.022     -5.675      0.000      -0.167      -0.081\n",
            "world_rank_o         -0.1546      0.029     -5.276      0.000      -0.212      -0.097\n",
            "masters_o            -0.0982      0.028     -3.479      0.001      -0.153      -0.043\n",
            "sinc1_1              -0.1166      0.034     -3.397      0.001      -0.184      -0.049\n",
            "intel1_1             -0.0399      0.036     -1.099      0.272      -0.111       0.031\n",
            "fun1_1               -0.0762      0.032     -2.378      0.017      -0.139      -0.013\n",
            "amb1_1                0.0236      0.032      0.734      0.463      -0.039       0.087\n",
            "shar1_1              -0.0325      0.032     -1.004      0.316      -0.096       0.031\n",
            "age_diff             -0.0186      0.031     -0.600      0.548      -0.079       0.042\n",
            "income_diff          -0.0400      0.022     -1.806      0.071      -0.083       0.003\n",
            "date_diff            -0.0249      0.024     -1.049      0.294      -0.071       0.022\n",
            "go_out_diff           0.0211      0.024      0.878      0.380      -0.026       0.068\n",
            "sports_diff           0.0220      0.025      0.875      0.381      -0.027       0.071\n",
            "dining_diff          -0.0991      0.026     -3.808      0.000      -0.150      -0.048\n",
            "art_diff              0.0356      0.029      1.248      0.212      -0.020       0.092\n",
            "hiking_diff          -0.0639      0.025     -2.552      0.011      -0.113      -0.015\n",
            "gaming_diff           0.0382      0.026      1.486      0.137      -0.012       0.089\n",
            "clubbing_diff        -0.0099      0.023     -0.425      0.671      -0.055       0.036\n",
            "reading_diff          0.1492      0.024      6.344      0.000       0.103       0.195\n",
            "tv_diff               0.0629      0.028      2.272      0.023       0.009       0.117\n",
            "theater_diff         -0.0341      0.032     -1.069      0.285      -0.097       0.028\n",
            "movies_diff           0.0919      0.028      3.276      0.001       0.037       0.147\n",
            "concerts_diff         0.0846      0.032      2.605      0.009       0.021       0.148\n",
            "music_diff            0.0274      0.030      0.928      0.354      -0.031       0.085\n",
            "shopping_diff         0.0482      0.030      1.589      0.112      -0.011       0.108\n",
            "yoga_diff            -0.0257      0.025     -1.035      0.301      -0.074       0.023\n",
            "worldrank_diff        0.0070      0.029      0.243      0.808      -0.049       0.063\n",
            "(3_1-pf_o)_sinc      -0.0127      0.024     -0.518      0.604      -0.061       0.035\n",
            "(3_1-pf_o)_fun        0.0905      0.024      3.724      0.000       0.043       0.138\n",
            "(3_1-pf_o)_intel      0.0937      0.024      3.923      0.000       0.047       0.140\n",
            "(3_1-pf_o)_amb       -0.1307      0.025     -5.132      0.000      -0.181      -0.081\n",
            "(1_1-2_1_o)_sinc      0.0345      0.035      0.984      0.325      -0.034       0.103\n",
            "(1_1-2_1_o)_fun       0.0438      0.032      1.355      0.175      -0.020       0.107\n",
            "(1_1-2_1_o)_intel    -0.0316      0.036     -0.888      0.374      -0.101       0.038\n",
            "(1_1-2_1_o)_amb      -0.0351      0.030     -1.153      0.249      -0.095       0.025\n",
            "(1_1-2_1_o)_shar      0.0251      0.033      0.770      0.441      -0.039       0.089\n",
            "from_m               -0.0191      0.022     -0.887      0.375      -0.061       0.023\n",
            "goal_m               -0.0132      0.022     -0.608      0.543      -0.056       0.029\n",
            "imprace_m            -0.0282      0.022     -1.275      0.203      -0.072       0.015\n",
            "imprelig_m            0.0405      0.022      1.827      0.068      -0.003       0.084\n",
            "career_c_m            0.0214      0.022      0.961      0.336      -0.022       0.065\n",
            "masters_m            -0.0196      0.028     -0.708      0.479      -0.074       0.035\n",
            "==============================================================================\n",
            "Omnibus:                      170.158   Durbin-Watson:                   1.987\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              186.927\n",
            "Skew:                          -0.383   Prob(JB):                     2.57e-41\n",
            "Kurtosis:                       3.324   Cond. No.                         5.19\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJi8AQMK_vXM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a32e4d-db5a-4981-d233-84974538a081"
      },
      "source": [
        "#from our newest model, let's remove 'worldrank_diff' which has the largest p-value\n",
        "X_train = X_train.drop(columns=['worldrank_diff'])\n",
        "\n",
        "amb_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(amb_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  amb_o   R-squared:                       0.085\n",
            "Model:                            OLS   Adj. R-squared:                  0.077\n",
            "Method:                 Least Squares   F-statistic:                     11.86\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           3.15e-89\n",
            "Time:                        23:13:03   Log-Likelihood:                -12693.\n",
            "No. Observations:                6475   AIC:                         2.549e+04\n",
            "Df Residuals:                    6424   BIC:                         2.583e+04\n",
            "Df Model:                          50                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.7514      0.021    314.881      0.000       6.709       6.793\n",
            "gender                0.1153      0.035      3.263      0.001       0.046       0.185\n",
            "condtn                0.0190      0.023      0.814      0.415      -0.027       0.065\n",
            "order                -0.1564      0.023     -6.880      0.000      -0.201      -0.112\n",
            "int_corr              0.0769      0.022      3.481      0.001       0.034       0.120\n",
            "samerace              0.0216      0.022      0.995      0.320      -0.021       0.064\n",
            "age_o                -0.0095      0.031     -0.312      0.755      -0.070       0.050\n",
            "mn_sat_o             -0.0681      0.028     -2.416      0.016      -0.123      -0.013\n",
            "tuition_o            -0.0600      0.028     -2.119      0.034      -0.116      -0.004\n",
            "exphappy_o            0.1739      0.023      7.486      0.000       0.128       0.219\n",
            "met_o                -0.1244      0.022     -5.676      0.000      -0.167      -0.081\n",
            "world_rank_o         -0.1502      0.023     -6.499      0.000      -0.196      -0.105\n",
            "masters_o            -0.0980      0.028     -3.474      0.001      -0.153      -0.043\n",
            "sinc1_1              -0.1160      0.034     -3.389      0.001      -0.183      -0.049\n",
            "intel1_1             -0.0395      0.036     -1.091      0.275      -0.111       0.032\n",
            "fun1_1               -0.0758      0.032     -2.369      0.018      -0.138      -0.013\n",
            "amb1_1                0.0241      0.032      0.751      0.453      -0.039       0.087\n",
            "shar1_1              -0.0320      0.032     -0.989      0.323      -0.095       0.031\n",
            "age_diff             -0.0177      0.031     -0.575      0.565      -0.078       0.043\n",
            "income_diff          -0.0401      0.022     -1.811      0.070      -0.084       0.003\n",
            "date_diff            -0.0250      0.024     -1.053      0.292      -0.072       0.022\n",
            "go_out_diff           0.0214      0.024      0.888      0.374      -0.026       0.069\n",
            "sports_diff           0.0216      0.025      0.860      0.390      -0.028       0.071\n",
            "dining_diff          -0.0994      0.026     -3.821      0.000      -0.150      -0.048\n",
            "art_diff              0.0353      0.029      1.237      0.216      -0.021       0.091\n",
            "hiking_diff          -0.0636      0.025     -2.544      0.011      -0.113      -0.015\n",
            "gaming_diff           0.0384      0.026      1.495      0.135      -0.012       0.089\n",
            "clubbing_diff        -0.0099      0.023     -0.425      0.671      -0.055       0.036\n",
            "reading_diff          0.1491      0.024      6.344      0.000       0.103       0.195\n",
            "tv_diff               0.0627      0.028      2.268      0.023       0.009       0.117\n",
            "theater_diff         -0.0336      0.032     -1.056      0.291      -0.096       0.029\n",
            "movies_diff           0.0917      0.028      3.270      0.001       0.037       0.147\n",
            "concerts_diff         0.0849      0.032      2.620      0.009       0.021       0.149\n",
            "music_diff            0.0272      0.030      0.920      0.357      -0.031       0.085\n",
            "shopping_diff         0.0484      0.030      1.599      0.110      -0.011       0.108\n",
            "yoga_diff            -0.0253      0.025     -1.022      0.307      -0.074       0.023\n",
            "(3_1-pf_o)_sinc      -0.0126      0.024     -0.514      0.607      -0.060       0.035\n",
            "(3_1-pf_o)_fun        0.0905      0.024      3.723      0.000       0.043       0.138\n",
            "(3_1-pf_o)_intel      0.0936      0.024      3.923      0.000       0.047       0.140\n",
            "(3_1-pf_o)_amb       -0.1309      0.025     -5.139      0.000      -0.181      -0.081\n",
            "(1_1-2_1_o)_sinc      0.0343      0.035      0.979      0.328      -0.034       0.103\n",
            "(1_1-2_1_o)_fun       0.0435      0.032      1.348      0.178      -0.020       0.107\n",
            "(1_1-2_1_o)_intel    -0.0316      0.036     -0.888      0.375      -0.101       0.038\n",
            "(1_1-2_1_o)_amb      -0.0352      0.030     -1.157      0.247      -0.095       0.024\n",
            "(1_1-2_1_o)_shar      0.0247      0.033      0.760      0.447      -0.039       0.088\n",
            "from_m               -0.0190      0.022     -0.883      0.377      -0.061       0.023\n",
            "goal_m               -0.0132      0.022     -0.610      0.542      -0.056       0.029\n",
            "imprace_m            -0.0283      0.022     -1.281      0.200      -0.072       0.015\n",
            "imprelig_m            0.0402      0.022      1.819      0.069      -0.003       0.084\n",
            "career_c_m            0.0216      0.022      0.968      0.333      -0.022       0.065\n",
            "masters_m            -0.0194      0.028     -0.702      0.483      -0.074       0.035\n",
            "==============================================================================\n",
            "Omnibus:                      169.964   Durbin-Watson:                   1.987\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              186.663\n",
            "Skew:                          -0.383   Prob(JB):                     2.93e-41\n",
            "Kurtosis:                       3.323   Cond. No.                         5.18\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH2KBiMBAISC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5483c655-bd24-4224-ba33-894d826951ea"
      },
      "source": [
        "#from our newest model, let's remove 'age_o' which has the largest p-value\n",
        "X_train = X_train.drop(columns=['age_o'])\n",
        "\n",
        "amb_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(amb_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  amb_o   R-squared:                       0.085\n",
            "Model:                            OLS   Adj. R-squared:                  0.078\n",
            "Method:                 Least Squares   F-statistic:                     12.10\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           9.39e-90\n",
            "Time:                        23:13:03   Log-Likelihood:                -12694.\n",
            "No. Observations:                6475   AIC:                         2.549e+04\n",
            "Df Residuals:                    6425   BIC:                         2.583e+04\n",
            "Df Model:                          49                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.7514      0.021    314.903      0.000       6.709       6.793\n",
            "gender                0.1151      0.035      3.259      0.001       0.046       0.184\n",
            "condtn                0.0185      0.023      0.792      0.428      -0.027       0.064\n",
            "order                -0.1560      0.023     -6.873      0.000      -0.201      -0.112\n",
            "int_corr              0.0763      0.022      3.467      0.001       0.033       0.119\n",
            "samerace              0.0215      0.022      0.990      0.322      -0.021       0.064\n",
            "mn_sat_o             -0.0679      0.028     -2.410      0.016      -0.123      -0.013\n",
            "tuition_o            -0.0600      0.028     -2.120      0.034      -0.116      -0.005\n",
            "exphappy_o            0.1736      0.023      7.480      0.000       0.128       0.219\n",
            "met_o                -0.1248      0.022     -5.707      0.000      -0.168      -0.082\n",
            "world_rank_o         -0.1511      0.023     -6.586      0.000      -0.196      -0.106\n",
            "masters_o            -0.0983      0.028     -3.486      0.000      -0.154      -0.043\n",
            "sinc1_1              -0.1163      0.034     -3.396      0.001      -0.183      -0.049\n",
            "intel1_1             -0.0397      0.036     -1.096      0.273      -0.111       0.031\n",
            "fun1_1               -0.0754      0.032     -2.359      0.018      -0.138      -0.013\n",
            "amb1_1                0.0239      0.032      0.745      0.456      -0.039       0.087\n",
            "shar1_1              -0.0322      0.032     -0.996      0.319      -0.096       0.031\n",
            "age_diff             -0.0240      0.023     -1.039      0.299      -0.069       0.021\n",
            "income_diff          -0.0401      0.022     -1.810      0.070      -0.084       0.003\n",
            "date_diff            -0.0251      0.024     -1.057      0.290      -0.072       0.021\n",
            "go_out_diff           0.0214      0.024      0.889      0.374      -0.026       0.069\n",
            "sports_diff           0.0215      0.025      0.859      0.391      -0.028       0.071\n",
            "dining_diff          -0.0994      0.026     -3.823      0.000      -0.150      -0.048\n",
            "art_diff              0.0351      0.029      1.232      0.218      -0.021       0.091\n",
            "hiking_diff          -0.0636      0.025     -2.546      0.011      -0.113      -0.015\n",
            "gaming_diff           0.0384      0.026      1.496      0.135      -0.012       0.089\n",
            "clubbing_diff        -0.0097      0.023     -0.419      0.676      -0.055       0.036\n",
            "reading_diff          0.1492      0.024      6.345      0.000       0.103       0.195\n",
            "tv_diff               0.0627      0.028      2.266      0.023       0.008       0.117\n",
            "theater_diff         -0.0336      0.032     -1.057      0.291      -0.096       0.029\n",
            "movies_diff           0.0917      0.028      3.270      0.001       0.037       0.147\n",
            "concerts_diff         0.0852      0.032      2.627      0.009       0.022       0.149\n",
            "music_diff            0.0271      0.030      0.918      0.358      -0.031       0.085\n",
            "shopping_diff         0.0485      0.030      1.603      0.109      -0.011       0.108\n",
            "yoga_diff            -0.0253      0.025     -1.021      0.307      -0.074       0.023\n",
            "(3_1-pf_o)_sinc      -0.0124      0.024     -0.509      0.611      -0.060       0.035\n",
            "(3_1-pf_o)_fun        0.0901      0.024      3.713      0.000       0.043       0.138\n",
            "(3_1-pf_o)_intel      0.0937      0.024      3.928      0.000       0.047       0.141\n",
            "(3_1-pf_o)_amb       -0.1308      0.025     -5.136      0.000      -0.181      -0.081\n",
            "(1_1-2_1_o)_sinc      0.0342      0.035      0.976      0.329      -0.034       0.103\n",
            "(1_1-2_1_o)_fun       0.0435      0.032      1.348      0.178      -0.020       0.107\n",
            "(1_1-2_1_o)_intel    -0.0317      0.036     -0.890      0.374      -0.102       0.038\n",
            "(1_1-2_1_o)_amb      -0.0350      0.030     -1.153      0.249      -0.095       0.025\n",
            "(1_1-2_1_o)_shar      0.0245      0.033      0.754      0.451      -0.039       0.088\n",
            "from_m               -0.0190      0.022     -0.884      0.377      -0.061       0.023\n",
            "goal_m               -0.0129      0.022     -0.598      0.550      -0.055       0.029\n",
            "imprace_m            -0.0287      0.022     -1.297      0.195      -0.072       0.015\n",
            "imprelig_m            0.0400      0.022      1.810      0.070      -0.003       0.083\n",
            "career_c_m            0.0209      0.022      0.942      0.346      -0.023       0.064\n",
            "masters_m            -0.0184      0.027     -0.671      0.502      -0.072       0.035\n",
            "==============================================================================\n",
            "Omnibus:                      169.663   Durbin-Watson:                   1.987\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              186.325\n",
            "Skew:                          -0.383   Prob(JB):                     3.47e-41\n",
            "Kurtosis:                       3.323   Cond. No.                         5.17\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5PKgcO_BHza"
      },
      "source": [
        "I have removed some variables with super large p-values, and I will use the current model as my final linear model for amb_o attribute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anh839iQu4Vd"
      },
      "source": [
        "## Ambition (Linear Regression - Bootstrap OSR2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcvcEMMxBDeM"
      },
      "source": [
        "#define the current model as the final model \"amb_lr\"\n",
        "amb_lr = amb_model\n",
        "\n",
        "#remove the variables from test set\n",
        "X_test = X_test.drop(columns = ['(1_1-2_1_o)_att', 'attr1_1', 'museums_diff', '(3_1-pf_o)_att', 'exercise_diff', 'tvsport_diff', 'income', 'worldrank_diff', 'age_o'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbQuAgq6Bl0Z"
      },
      "source": [
        "# compute out-of-sample R-squared using the test set\n",
        "def amb_OSR2(predictions, y_test,y_train):\n",
        "    SSE = np.sum((y_test-predictions)**2)\n",
        "    SST = np.sum((y_test-np.mean(y_train))**2)\n",
        "    r2 = 1-SSE/SST\n",
        "    return r2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk7v2C44BrLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48d09ce-bd9b-4a3f-882a-fd8bff753cab"
      },
      "source": [
        "# Check OSR^2 with test set\n",
        "amb_test_OSR2 = amb_OSR2(amb_lr.predict(sm.add_constant(X_test)), y_test, y_train)\n",
        "amb_test_OSR2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05870217689690993"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GERzYFMB-PQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3fc3ec2-73eb-4252-badf-3f6ce26c4807"
      },
      "source": [
        "amb_output = bootstrap_validation(sm.add_constant(X_test),y_test,y_train,amb_lr,\n",
        "                                 metrics_list=[amb_OSR2],\n",
        "                                 sample = 5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean of bootstrapped estimate\n",
        "print('The bootstrapped mean of linear regression OSR2 is:', np.mean(amb_output)[0])"
      ],
      "metadata": {
        "id": "Z_Wurv0B3K7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba5b859-4810-4ada-b9af-39c63e1119b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The bootstrapped mean of linear regression OSR2 is: 0.05848478492951455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnZBYHW6CEmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "746d5fb8-b4a1-4a92-b5d0-c9c0dc81fc06"
      },
      "source": [
        "#bootstrap plots\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap OSR2 Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot OSR2 - Test Set OSR2', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].hist(amb_output.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[0].set_xlim([0.0,0.1])\n",
        "axs[1].hist(amb_output.iloc[:,0]-amb_test_OSR2, bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[1].set_xlim([-0.05,0.05])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.05, 0.05)"
            ]
          },
          "metadata": {},
          "execution_count": 193
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuiynXAZCc60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "255c00fd-dde2-497e-eaac-b79c9d0352b5"
      },
      "source": [
        "# The 95% confidence interval\n",
        "CI_0 = np.quantile(amb_output.iloc[:,0]-amb_test_OSR2,np.array([0.025,0.975]))\n",
        "left = amb_test_OSR2 - CI_0[1]\n",
        "right = amb_test_OSR2 - CI_0[0]\n",
        "print(\"The 95-percent confidence interval of the test set OSR2 for the linear regression ambition model is:\",[left, right])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 95-percent confidence interval of the test set OSR2 for the linear regression ambition model is: [0.03207518707393858, 0.0870139373173629]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNThc4vowArZ"
      },
      "source": [
        "## Ambition (LDA - Train Model + Bootstrap Accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSRyLQCiCwTJ"
      },
      "source": [
        "Let's do LDA on amb_o next"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzy6roLMCyN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472b6411-7304-4820-9c7b-420b45e34d60"
      },
      "source": [
        "# Run LDA on ambition trait.\n",
        "import sklearn\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# redefine y_train/y_test type to int\n",
        "y_train = amb_y_train.astype(int)\n",
        "y_test = amb_y_test.astype(int)\n",
        "\n",
        "# train\n",
        "amb_lda = LinearDiscriminantAnalysis()\n",
        "amb_lda.fit(sm.add_constant(amb_X_train), y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearDiscriminantAnalysis()"
            ]
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUA6kFAEDXON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd1886c-36ed-4d28-cd60-b5fcad630727"
      },
      "source": [
        "# compute ambition accuracy of LDA\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = amb_lda.predict(sm.add_constant(amb_X_test))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix : \\n\", cm)\n",
        "amb_lda_acc = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nTest Set Accuracy:\", amb_lda_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix : \n",
            " [[  0   0   0   0   0   0   1   0   0   0   0]\n",
            " [  0   3   0   0   0   2   2   2   1   0   0]\n",
            " [  0   3   2   0   0   3   2   5   4   0   0]\n",
            " [  0   3   0   1   0   7   6  18   4   1   1]\n",
            " [  0   0   1   1   0   9  15  28  20   0   1]\n",
            " [  1   1   2   2   0  45  51  89  34   3   5]\n",
            " [  0   0   0   3   0  48  67 127  63   3   8]\n",
            " [  0   1   1   1   0  42  56 140  78   7   4]\n",
            " [  0   0   7   2   0  22  44 110 108  13   7]\n",
            " [  1   0   0   1   0  13  28  72  52  13   8]\n",
            " [  0   0   2   0   0   3  17  20  36   3   9]]\n",
            "\n",
            "Test Set Accuracy: 0.23965410747374924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vnnXplQFCGm"
      },
      "source": [
        "Although the accuracy is not very high, we can see that the most predicted is very closed to the true values from the confusion matrix which is noticeable and good enought under our scenario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUEpqtN3DiqT"
      },
      "source": [
        "#define the accuracy function\n",
        "def amb_accuracy(y_pred, y_test, y_train):\n",
        "  return accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHTjXI2JDk9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d18653d-331a-4720-83b1-2117682bfe95"
      },
      "source": [
        "amb_lda_output = bootstrap_validation(sm.add_constant(amb_X_test),y_test,y_train,amb_lda,\n",
        "                                 metrics_list=[amb_accuracy],\n",
        "                                 sample = 5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnY-TLEuDr-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c3c19ae7-3229-4401-e8d4-3d842d1a57db"
      },
      "source": [
        "#bootstrap accuracy plots for ambition\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap Accuracy Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot Acc - Test Set Acc', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].hist(amb_lda_output.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[0].set_xlim([0.2,0.3])\n",
        "axs[1].hist(amb_lda_output.iloc[:,0]-amb_lda_acc, bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[1].set_xlim([-0.05,0.05])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.05, 0.05)"
            ]
          },
          "metadata": {},
          "execution_count": 199
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtgAAAFCCAYAAADLxqq9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wkVX3//9cbEIwgAroiAXS9IEbjN2hWgxoVRSNqFDRGMRoXJV9yISbG3DT5JSyJ+QXNL2JMDIZoAmq+IvESiOIFEFTyFeOCBC+ArFxkkct6AbkICn5+f9QZ6G1mZqdnarpndl7Px6MeXV11qvqcqu5Tnz51qipVhSRJkqR+bDPpDEiSJElbEwNsSZIkqUcG2JIkSVKPDLAlSZKkHhlgS5IkST0ywJYkSZJ6tN2kM9C3BzzgAbV69epJZ0OSRnbeeed9u6pWTTof42SdLWk5m6ne3uoC7NWrV7N+/fpJZ0OSRpbkyknnYdyssyUtZzPV23YRkSRJknpkgC1JkiT1yABbkiRJ6pEBtiRJktQjA2xJkiSpRwbYkiRJUo8MsCVJkqQeGWBLkiRJPTLAliRJknpkgC1JkiT1aKt7VLpWnhydeS1XR1XPOZEkLXUeMzQOtmBLkiRJPbIFW1uNdazrNZ0kaevlMUOLyRZsSZIkqUcG2JIkSVKPDLAlSZKkHhlgS5IkST0ywJYkSZJ6ZIAtSZIk9cjb9GlJmu+DACRJK4/HDC01tmBLkiRJPbIFW0vaXG7w70MAJEngMUNLx1hbsJPsm+SCgeH7SV6XZLckpye5tL3u2tInyduTbEhyYZLHjzO/krSSWWdL0vyMNcCuqkuqar+q2g/4WeBW4CPAG4Azq2of4Mz2HuC5wD5tOAI4bpz5laSVzDpbkuZnkn2wDwS+UVVXAgcDJ7bpJwKHtPGDgfdU51xglyR7jD+rkrTiWWdL0hxNMsA+FHh/G9+9qq5p49cCu7fxPYGrBpbZ2KZJksbLOluS5mgiAXaS7YEXAv8+PK+qCqgR13dEkvVJ1m/atKmnXEqSwDpbkkY1qRbs5wLnV9V17f11U6cR2+v1bfrVwN4Dy+3Vpm2mqo6vqjVVtWbVqlWLmG1JWpGssyVpBJMKsF/O3acaAU4F1rbxtcApA9Nf1a5M3x+4ceC0pCRpPKyzJWkEY78PdpIdgWcDvz4w+Rjg5CSHA1cCL23TTwOeB2ygu3r91WPMqiSteNbZkjS6sQfYVXULcP+had+hu0J9OG0BR44pa5KkIdbZkjQ6H5UuSZIk9cgAW5IkSeqRAbYkSZLUIwNsSZIkqUcG2JIkSVKPDLAlSZKkHhlgS5IkST0ywJYkSZJ6ZIAtSZIk9cgAW5IkSeqRAbYkSZLUIwNsSZIkqUfbTToDWhlydCadBUnSMuExQ8udLdiSJElSj2zB1litY12v6SRJWy+PGVqubMGWJEmSemSALUmSJPXIAFuSJEnqkQG2JEmS1CMDbEmSJKlHBtiSJElSjwywJUmSpB4ZYEuSJEk9MsCWJEmSemSALUmSJPXIAFuSJEnq0dgD7CS7JPlgkouTXJTkSUl2S3J6kkvb664tbZK8PcmGJBcmefy48ytJK5l1tiSNbhIt2H8HfKKqHgX8DHAR8AbgzKraBzizvQd4LrBPG44Ajht/diVpRbPOlqQRjTXATnI/4GnAuwGq6odVdQNwMHBiS3YicEgbPxh4T3XOBXZJssc48yxJK5V1tiTNz7hbsB8KbAL+NcmXkrwryY7A7lV1TUtzLbB7G98TuGpg+Y1t2maSHJFkfZL1mzZtWsTsS9KKYp0tSfMw7gB7O+DxwHFV9TjgFu4+tQhAVRVQo6y0qo6vqjVVtWbVqlW9ZVaSVjjrbEmah3EH2BuBjVX1hfb+g3SV93VTpxHb6/Vt/tXA3gPL79WmSZIWn3W2JM3DWAPsqroWuCrJvm3SgcDXgFOBtW3aWuCUNn4q8Kp2Zfr+wI0DpyUlSYvIOluS5me7CXzma4F/S7I9cBnwarpA/+QkhwNXAi9taU8DngdsAG5taSVJ42OdLUkjGnuAXVUXAGummXXgNGkLOHLRMyVJmpZ1tiSNzic5SpIkST0ywJYkSZJ6ZIAtSZIk9cgAW5IkSeqRAbYkSZLUo0ncpk9biRydSWdBkrRMeMzQSmILtiRJktQjW7C1YOtY10saSdLWz2OGVgJbsCVJkqQeGWBLkiRJPTLAliRJknpkgC1JkiT1yABbkiRJ6pEBtiRJktQjA2xJkiSpRwbYkiRJUo8MsCVJkqQeGWBLkiRJPTLAliRJknpkgC1JkiT1yABbkiRJ6pEBtiRJktQjA2xJkiSpRwbYkiRJUo8MsCVJkqQeGWBLkiRJPRp7gJ3kiiRfTnJBkvVt2m5JTk9yaXvdtU1Pkrcn2ZDkwiSPH3d+JWkls86WpNFNqgX7GVW1X1Wtae/fAJxZVfsAZ7b3AM8F9mnDEcBxY8+pJMk6W5JGsFS6iBwMnNjGTwQOGZj+nuqcC+ySZI9JZFCSdBfrbEmaxSQC7AI+leS8JEe0abtX1TVt/Fpg9za+J3DVwLIb27TNJDkiyfok6zdt2rRY+Zaklcg6W5JGtN0EPvPnq+rqJA8ETk9y8eDMqqokNcoKq+p44HiANWvWjLSsJGlW1tmSNKKxt2BX1dXt9XrgI8ATgeumTiO21+tb8quBvQcW36tNkySNgXW2JI1urC3YSXYEtqmqm9r4LwB/AZwKrAWOaa+ntEVOBX47yUnAzwE3DpyWlBYkR2fOaesoG9m08lhnS3fzmKFRjLuLyO7AR5JMffb/qapPJPkicHKSw4ErgZe29KcBzwM2ALcCrx5zfiVpJbPOlqR5GGuAXVWXAT8zzfTvAAdOM72AI8eQNa1A61jXSxppa2WdLd3NY4ZGsVRu0ydJkiRtFQywJUmSpB4ZYEuSJEk9MsCWJEmSemSALUmSJPXIAFuSJEnqkQG2JEmS1CMDbEmSJKlHBtiSJElSjwywJUmSpB4ZYEuSJEk9mnOAneRpSXaaYd5OSZ7WX7YkSZKk5WmUFuyzgEfPMG/fNl+SJEla0UYJsDPLvB2AOxeYF0mSJGnZ2262mUlWAw8bmLRmmm4iPwG8BvhmrzmTJEmSlqFZA2xgLXAUUG34ezZvya72/g7gyMXIoCRJkrScbCnAPgE4my6I/jRdEP21oTS3A1+vqu/2nTlJkiRpuZk1wK6qK4ErAZI8Azi/qm4aR8YkSZKk5WhLLdh3qarPLGZGJEmSpK3BKPfB3j7JUUkuTnJrkjuHhjsWM6OSJEnScjDnFmzgb+j6YH8c+DBd32tJkiRJA0YJsF8CHFVVf7VYmZEkSZKWu1EC7J2Azy9WRrQ05OjZnickSVLH44U0s1Ge5PifwNMWKyOSJEnS1mCUFuy/B96T5MfAacA97ntdVZf1lTFN1jrW9ZJGkrR1m+uxwGOGVpJRWrA/D+wDrAO+AFw6zTAnSbZN8qUkH23vH5rkC0k2JPlAku3b9B3a+w1t/uoR8itJ6oF1tiSNZpQW7NfQPRq9D78LXATs3N6/GTi2qk5K8k7gcOC49vq9qnpEkkNbupf1lAdJ0txYZ0vSCEZ50MwJfXxgkr2A5wN/Bbw+SYBnAr/SkpxI10p+HHBwGwf4IPAPSVJVfQX6kqRZWGdL0uhG6SLSl7cBfwT8uL2/P3BDVU09qGYjsGcb3xO4CqDNv7GllySNh3W2JI1ozi3YSf5lC0mqqg7fwjp+Ebi+qs5LcsBcP3sOeTsCOALgwQ9+cF+rlaQVzTpbkuZnlD7Yz+SefbB3A+4L3NCGLXkK8MIkzwPuTdef7++AXZJs11o89gKubumvBvYGNibZDrgf8J3hlVbV8cDxAGvWrPFUpHo36v1e6yi/htoqWGdL8+AxQ3PuIlJVq6vqoUPD/YADgGuBX5rDOt5YVXtV1WrgUODTVfUK4Cy6J0UCrAVOaeOntve0+Z+2L58kjYd1tiTNzygt2NOqqs8mOZbuPtk/P8/V/DFwUpI3AV8C3t2mvxt4b5INdPfdPnSh+ZXmw/u8SpuxzpZm4TFDCw6wm8uAx42yQFWdDZzdxi8DnjhNmtuAX1549iRJC2GdLUlzt+C7iLR+dofRXUkuSZIkrWij3EXk09NM3h54JN1tmH6jr0xJkiRJy9UoXUS24Z53EbkJ+DBwUjt9KEmSJK1oozzJ8YBFzIckSZK0VZjEkxwlSZKkrdZIAXaSxyb5YJJNSe5orycneexiZVCSJElaTka5yPEJwGeAH9A9TOBa4EHAC4DnJ3laVZ23KLmUJEmSlolRLnL8a+ArwIFVddPUxCT3Bc5o83+h3+xJkiRJy8soXUT2B/56MLgGaO/fDDypz4xJkiRJy9EoAfbwLfpGnS9JkiRt9UYJsL8A/EnrEnKXJDsCfwyc22fGJEmSpOVolD7YfwKcDVyZ5KPANXQXOT4P2BF4eu+5kyRJkpaZUR40899J9gf+HHgOsBvwXeAs4C+r6suLk0VJkiRp+Zg1wE6yDfB84PKq+kpVXQi8ZCjNY4HVgAG2JEmSVrwt9cF+JfB+4JZZ0twEvD/Jy3vLlSRJkrRMzSXA/tequnymBFV1BfBuYG2P+ZIkSZKWpS0F2I8HPjWH9ZwBrFl4diRJkqTlbUsB9n2B781hPd9raSVJkqQVbUsB9reBh8xhPQ9uaSVJkqQVbUsB9jnMrW/1YS2tJEmStKJtKcB+G3BgkmOTbD88M8m9krwNeCZw7GJkUJIkSVpOZr0PdlV9PsnvA38LvCLJp4Ar2+yHAM8G7g/8flX5qHRJkiSteFt8kmNVvS3J+cAfAy8CfqLN+gHdo9OPqarPLVoOJUmSpGVkTo9Kr6rPAp9tT3Z8QJv8naq6c9FyJkmSJC1Dcwqwp1TVj4HrFykvkiRJ0rK3pYscJUmSJI1grAF2knsn+e8k/5Pkq0mObtMfmuQLSTYk+cDUHUuS7NDeb2jzV48zv5K0kllnS9L8jLsF+3bgmVX1M8B+wEFJ9gfeDBxbVY+geyrk4S394cD32vRjWzpJ0nhYZ0vSPIw1wK7Oze3tvdpQdPfR/mCbfiJwSBs/uL2nzT8wScaUXUla0ayzJWl+xt4HO8m2SS6gu1jydOAbwA1VdUdLshHYs43vCVwF0ObfSHffbUnSGFhnS9Loxh5gV9WdVbUfsBfwROBRC11nkiOSrE+yftOmTQvOoySpY50tSaOb2F1EquoG4CzgScAuSaZuGbgXcHUbvxrYG6DNvx/wnWnWdXxVramqNatWrVr0vEvSSmOdLUlzN+67iKxKsksb/wm6R61fRFdpv6QlWwuc0sZPbe9p8z9dVTW+HEvSymWdLUnzM9KDZnqwB3Bikm3pgvuTq+qjSb4GnJTkTcCXgHe39O8G3ptkA/Bd4NAx51eSVjLrbEmah7EG2FV1IfC4aaZfRte3b3j6bcAvjyFrkqQh1tmSND8+yVGSJEnqkQG2JEmS1CMDbEmSJKlHBtiSJElSjwywJUmSpB4ZYEuSJEk9MsCWJEmSemSALUmSJPXIAFuSJEnq0bgfla4xy9GZdBYkScuExwypH7ZgS5IkST2yBXuFWMe6XtNJkrZeczkWeLyQZmaALS2CUU6z1lG1iDmRJC11HjO2PnYRkSRJknpkC7a0CDy9KkmaK48ZWx9bsCVJkqQeGWBLkiRJPTLAliRJknpkgC1JkiT1yABbkiRJ6pEBtiRJktQjA2xJkiSpRwbYkiRJUo8MsCVJkqQeGWBLkiRJPTLAliRJkno01gA7yd5JzkrytSRfTfK7bfpuSU5Pcml73bVNT5K3J9mQ5MIkjx9nfiVpJbPOlqT5GXcL9h3A71fVo4H9gSOTPBp4A3BmVe0DnNneAzwX2KcNRwDHjTm/krSSWWdL0jyMNcCuqmuq6vw2fhNwEbAncDBwYkt2InBIGz8YeE91zgV2SbLHOPMsSSuVdbYkzc/E+mAnWQ08DvgCsHtVXdNmXQvs3sb3BK4aWGxjmyZJGiPrbEmau4kE2El2Aj4EvK6qvj84r6oKqBHXd0SS9UnWb9q0qcecSpKssyVpNGMPsJPci66i/req+nCbfN3UacT2en2bfjWw98Die7Vpm6mq46tqTVWtWbVq1eJlXpJWGOtsSRrduO8iEuDdwEVV9daBWacCa9v4WuCUgemvalem7w/cOHBaUpK0iKyzJWl+thvz5z0F+FXgy0kuaNP+BDgGODnJ4cCVwEvbvNOA5wEbgFuBV483u5K0ollnS9I8jDXArqpzgMww+8Bp0hdw5KJmSpI0LetsSZofn+QoSZIk9cgAW5IkSeqRAbYkSZLUIwNsSZIkqUcG2JIkSVKPDLAlSZKkHhlgS5IkST0ywJYkSZJ6NO4nOaoHOXqm5z5IknQ3jxfSZNiCLUmSJPXIFuxlbB3rekkjSdq6zfVY4DFD6oct2JIkSVKPDLAlSZKkHhlgS5IkST0ywJYkSZJ6ZIAtSZIk9cgAW5IkSeqRAbYkSZLUIwNsSZIkqUcG2JIkSVKPfJKjNGE5OiOlr6NqkXIiSVrqRjlmeLyYHFuwJUmSpB7Zgi1N2DrW9ZpOkrT1msuxwOPF5NmCLUmSJPXIAFuSJEnqkQG2JEmS1KOxBthJ/iXJ9Um+MjBttySnJ7m0ve7apifJ25NsSHJhksePM6+SJOttSZqPcbdgnwAcNDTtDcCZVbUPcGZ7D/BcYJ82HAEcN6Y8SpLudgLW25I0krEG2FX1WeC7Q5MPBk5s4ycChwxMf091zgV2SbLHeHIqSQLrbUmaj6XQB3v3qrqmjV8L7N7G9wSuGki3sU2TJE2W9bYkzWIpBNh3qaoCRn7sUJIjkqxPsn7Tpk2LkDNJ0nTmU29bZ0va2i2FAPu6qVOI7fX6Nv1qYO+BdHu1afdQVcdX1ZqqWrNq1apFzawkaWH1tnW2pK3dUgiwTwXWtvG1wCkD01/VrkrfH7hx4JSkJGlyrLclaRZjfVR6kvcDBwAPSLIROAo4Bjg5yeHAlcBLW/LTgOcBG4BbgVePM6+SJOttSZqPsQbYVfXyGWYdOE3aAo5c3BxJkmZjvS1Jo1sKXUQkSZKkrYYBtiRJktQjA2xJkiSpR2Ptgy1p4XJ05py2jhr5tvKSpK3EKMcL8JjRJ1uwJUmSpB7Zgi0tM+tY10saSdLWba7HAo8Z/TPAXgJGPYUjSVq5PGZIS59dRCRJkqQe2YK9hHgqR5I0V3YXk5YuW7AlSZKkHhlgS5IkST0ywJYkSZJ6ZIAtSZIk9cgAW5IkSeqRAbYkSZLUIwNsSZIkqUcG2JIkSVKPDLAlSZKkHvkkx0WSozPpLEgjfw/rqFqknEiaiccLLRWjfBc9XszOFmxJkiSpR7ZgL7J1rOsljTQfc/1u+R2UJs/fqybNmKU/tmBLkiRJPTLAliRJknpkgC1JkiT1yABbkiRJ6pEXOc6Rt1HSSuAtmqR+eMzQ1s7bwM5uyQfYSQ4C/g7YFnhXVR0z4SxJwspVM7PeljRspTXgLOkAO8m2wDuAZwMbgS8mObWqvtbL+ufRwuBtlLQ18xZNWqjFrrcnZb4t0v6mtLUyHprdkg6wgScCG6rqMoAkJwEHA8u6opa2BqNWriut9WIFs96WdA+j/NncGs6QpmrpZWpKkpcAB1XVr7X3vwr8XFX99kzLrFmzptavXz+39dtHTlqSlmJlOQ5JzquqNZPOx0KMWm+PUmdPkscLaema5DFjpnp7qbdgz0mSI4Aj2tvbk3xlkvmZgAcA3550JsbMMm/Fsu6uYGbFlLnZd9IZGIehOvvmJJdMKCsr7fsFK6/MK628sALLnHWZZJkfMt3EpR5gXw3sPfB+rzZtM1V1PHA8QJL1y70FaFSWeWWwzFu/JEu/KXfLtlhvD9bZk7TSvl+w8sq80soLlnmpWOr3wf4isE+ShybZHjgUOHXCeZIkzcx6W9KKt6RbsKvqjiS/DXyS7nZP/1JVX51wtiRJM7DelqQlHmADVNVpwGkjLDLx044TYJlXBsu89dsqyjuPentStortPaKVVuaVVl6wzEvCkr6LiCRJkrTcLPU+2JIkSdKysqwC7CQHJbkkyYYkb5hm/uuTfC3JhUnOTPKQgXlrk1zahrXjzfn8zLe8SfZL8vkkX23zXjb+3M/PQvZxm79zko1J/mF8uV6YBX6vH5zkU0kuamlWjzPv87XAMr+lfbcvSvL2JMviBsVzKPNvJPlykguSnJPk0QPz3tiWuyTJc8ab8+UtyW5JTm91/+lJdp0h3azHiCSnLodbwC6kvEnuk+RjSS5uv7El/Yj7OfymdkjygTb/C4P143L9Tc23zEmeneS8Vsecl+SZ4877fC1kP7f5D05yc5I/GFeeAaiqZTHQXSzzDeBhwPbA/wCPHkrzDOA+bfw3gQ+08d2Ay9rrrm1810mXaRHL+0hgnzb+k8A1wC6TLtNilnlg/t8B/wf4h0mXZxxlBs4Gnt3Gd5pKt5SHBX63nwz8V1vHtsDngQMmXaaeyrzzwPgLgU+08Ue39DsAD23r2XbSZVouA/AW4A1t/A3Am6dJM+sxAnhxq1e+MunyLGZ5gfsAz2hptgc+Bzx30mWaoZxz+U39FvDONn7oQD2yLH9TCyzz44CfbOM/DVw96fIsdpkH5n8Q+HfgD8aZ9+XUgn3X43er6ofA1ON371JVZ1XVre3tuXT3XwV4DnB6VX23qr4HnA4cNKZ8z9e8y1tVX6+qS9v4t4DrgVVjy/n8LWQfk+Rngd2BT40pv32Yd5lbC+d2VXV6S3fzQLqlbCH7uYB701W0OwD3Aq4bS64XZi5l/v7A2x3pykpLd1JV3V5VlwMb2vo0NwcDJ7bxE4FDpkkz4zEiyU7A64E3jSGvfZh3eavq1qo6C6B9T89noI5dYrb4m2LzbfFB4MB2xmu5/qbmXeaq+lKLBwC+CvxEkh3GkuuFWch+JskhwOV0ZR6r5RRg7wlcNfB+Y5s2k8OBj89z2aVgIeW9S5In0gUj3+g1d4tj3mVOsg3wt8B4TwEt3EL28yOBG5J8OMmXkvxNkm0XKZ99mneZq+rzwFl0Z2WuAT5ZVRctUj77NKcyJzkyyTfoWiF/Z5RlNaPdq+qaNn4t3Z/wYbNt47+kq1uWw59XWHh5AUiyC/AC4MzFyGQP5vK7uCtNVd0B3Ajcf47LLkULKfOgXwLOr6rbFymffZp3mduf4z8Gjh5DPu9hyd+mbz6SvBJYAzx90nkZh5nKm2QP4L3A2qr68STytlimKfNvAadV1cZl0iV3ZNOUeTvgqXSn/r4JfAA4DHj3JPK3GIbLnOQRwE9xd6va6UmeWlWfm1AWe1VV7wDekeRXgP8HWBbXi0xakjOAB00z608H31RVJZnzrbOS7Ac8vKp+byld37BY5R1Y/3bA+4G3V9Vl88ullqIkjwHeDPzCpPMyBuuAY6vq5knEBcspwJ7TY9OTPIuuknn6wL+zq4EDhpY9e1Fy2Z+FlJckOwMfA/60qs5d5Lz2ZSFlfhLw1CS/RdcXefskN1fVPS6IWGIWUuaNwAVTB8Ak/wHsz9IPsBdS5hcB51bVzS3Nx+n2/VIPsOdU5gEnAcfNc9kVp6qeNdO8JNcl2aOqrmmNDtdPk2ymY8STgDVJrqA7Xj4wydlVdQATtIjlnXI8cGlVva2H7C6WufwuptJsbH8a7gd8Z47LLkULKTNJ9gI+AryqqpbDWW1YWJl/DnhJkrcAuwA/TnJbVY3nJgjj7PC9kIGucruM7oKEqY7ujxlK8zi6rhD7DE3fja4Pzq5tuBzYbdJlWsTybk93Wu91ky7HuMo8lOYwls9FjgvZz9u29Kva+38Fjpx0mRa5zC8DzmjruFf7nr9g0mXqqcz7DIy/AFjfxh/D5hdkXcYyuCBrqQzA37D5RX9vmSbNFo8RwGqWx0WOCyovXV/zDwHbTLosWyjnXH5TR7L5xW8nt/Fl+ZtaYJl3aelfPOlyjKvMQ2nWMeaLHCe+8Ubc0M8Dvt4OvH/apv0F8MI2fgbdBU8XtOHUgWVfQ3chwwbg1ZMuy2KWF3gl8KOB6RcA+026PIu9jwfWcRjLJMBeaJmBZwMXAl8GTgC2n3R5FrPMdH8q/gm4CPga8NZJl6XHMv8d3YU4F9D1M3/MwLJ/2pa7hCV6V4elOtD1Pz0TuLR9r6YCyTXAuwbSzXqMYPkE2PMuL13rYLXf19Rv79cmXaZZyrql39S96e4esQH4b+BhA8suy9/UfMtM1+XsFjaPCx446fIs9n4eWMc6xhxg+yRHSZIkqUfL6S4ikiRJ0pJngC1JkiT1yABbkiRJ6pEBtiRJktQjA2xJkiSpRwbYY5DksCQ1MNyZ5OokJyfZdxE/d5ck65I8fh7LHpLk9YuRr74keUXbnl+adF6WgyQnDH0PB4f/GGE9B7Tv1TZD01e3dR3We+Znz8/qlp+HjfNzpYVYpseF01tef3cx8rZQSa6YpY67a+jhc0aqc5LskOT3kvxPkpuSfD/JxUlOTLLPPD7/dUlePI/l/rRtg4+MuqxGt5ye5Lg1+GW6p+9tCzwc+DPgzCSPqaobF+HzdgGOap95/ojLHgI8C3hr35nq0dRjpPdL8tiq+vJEc7M8bAJeOM30746wjgPovldvAn48MP0auiffjfsJYatbfs6heyCBtJwsi+NCewrgM9vbV9Hdt32peRHdw2Om/CPddv31nj9nNaPVOe+nezT5W4BzW55+im7fP5runuWjeF377A+PuNyr2uvzkty/qr4z4vIagQH2eF1QVRva+H8l+RZwOvBk4OOTy9bCJNmhBh7TPqbP3BM4kG67PZcu2P6DceZhLiaxbbbgh1V17mKsuJVzUdYtbcWWy3HhV+nOep9GF6D9dFV9ZcJ52kxVbXY2M8n3ge0Wq86bi9bK/SK6JysP/in5OPDW4TOBi5iPJwGPpO0/4OXAeB4ZvkLZRWSyvt9e7zU4MclBST6f5AdJbkzyH8OnDNP5vSSXJPlhkmuS/EOSndv81XSPvwX454HTY4e1+c9J8n/b+m9u6/nzNu8EuoB1z4HlrmjzDmjvX5zkn5NsonviHkkekeS9SS5veb8syXFJdh3K+wlJNiZ5cpIvJrmtndp77QjbbqqyPwr4L7qxuwQAAA6OSURBVOAVSbYdTpTkZ5J8JMl3Wp4uSfLGoTQvSvJfbTt8P8l/J3nh1HacrtvDwHY4YGDa2UnOSfKCJF9KcjvwW23eb7d9+t0kNyQ5N8nzp8nvjkmOSfKNJLcnuTbJh5LsnuRn22cePM1yU9v0HttgVEmekO5U8NQ2uyzJP7Z56+i2OcCPMnDKdbptNZCvNe37NrUPnt/mv77t++8nOSXJqqG8zLrd2vY/q72dOn09vF+OSHdq9rYk307y7iS7LXQ7SYtkYseFLVhL96TR1w28v4ck/zvJ+S2f30vymSRPHpg/Yx03hzwsWJJVSd6ZrjvO7em6ahwxlOZB6bpvfKuluSbJR5M8cC51zpCpuuba6WZW1eBZQJI8PcmZ6bqS3JLkk0l+emD+FcBD6I55U599whyKvha4E/jfwFXMvP+e3ur/G9vn/0+Sw4fSzLqP1THAHq9tk2yXrj/WTwH/L3A9cPZUgiQHAR8DbgZeBvwm8NPAOelabaf8FV33jdOBF9CdejoM+Fi6f8TXAFN9tP6a7tT9k9r8hwGn0lW0L6PrMvBWYMeW/i/p/uVuGljuRUNl+XsgdIHuYW3aT9L9cF8HPIfuUaYHtnUN2xn4AHAiXXeUs4G3z7Gih65yuKiqvgi8B3gQ3Sm4uyR5IvB5utOuvwc8v5Vzr4E0r6U7zXZ9W+cvAx+hOwU4H48E3k63fZ5D98hi2vre1db/MmA98NG2v6fysj3d/nwt3WPPfxH4bbruG7tW1XnAFxk63ZlkF+CldI9BvnNLGWzfweEhbd5OwCfpKuLD6M4O/AV3n+16F/DuNv7z3P39mM3OdPvoXXTfo+uBDyX5W+AZwJF035lnAO8YWnY1s2+389vyAL8zkJ/zW3mOaes8g+57/ofAQcDH08OfEakHS+K4MFsGk/wcsC/w3qq6lK5evUejRpL/Dzie7vf3UuCVwGeBB7f5s9ZxW9pQC9X+aJxD14K7ju6Y8J/Acdm8gee9dNvlD4Fn09UtG4H7sIU6ZxoX0/1pOibJK2f7I9EaD86k28+vBH4FuC/wuSR7t2QvogvWPznw2X+5hXLvQPe9Ob2qvgW8D1jTvm+D6Q5un7893XHmYOBf6AL6qTSz7mMNmPQz5lfCQFfB1TTD1cAThtKup+uPtd3AtIcCPwLe2t7vBtwOnDC07Cvbel/Y3q9u739tKN1L2vSdZ8nzCcDGaaYf0Jb9yBzKvR1dEFbA44bWXcChQ+lPB64EsoX1PrEt/8b2fhfgB8BJQ+k+Sxfw32eG9ewM3AR8eJbPmtqGh82wHQ4YmHY2XZ/k/baQ/23atvkUcMrA9NcM7r9Zvkt3Ag8ZmPY7wB3AXlv43KntPt3wBy3Nmvb+f82ynnUtzXZD0++xrQY+82kD0/5Xm3YJsO3A9Le27/m2M3zuTNttal88a5r83An8+dD0p7T0h4zyO3Zw6HNgiR0XtpDXf2y/pT3b+19v6zhoIM0jWpq3zrKeLdZxPW/js4FzBt7/GXAbsM9Qun8Gvj21fekC3N+ZZb3T1jmzpH8BXYPV1D7+Bl33jEcNpdsAnDk0beeWt7cNTLsCeN8I2+Gl7XNf3t7v294fM5Ambb3rgW1mWM8W97HD3YMt2OP1IuAJdAHiIcDXgNOm/kUm2RF4PPCBqrpjaqGqupyuG8TT26T96f5hvm9o/SfRBVpPZ3YX0FXMJyV5SZIHzqMs97gKOcn2Sf6knXL7QfuMz7XZw1fF3wl8aGjaSXT/gvdkdmvpAtn3AVTVDcApwMFJ7tfych+6QOrfqurWGdbzZGAnun/jfbmiqi4Ynpiue8dHk1xHt49+RNcyMrhdfgG4tqpOnWX9JwE30J3mm/LrwMeqauMc8nc93XdweHhvm39pW/8/tdaWvaddy2huqarPDry/uL2eUZu3uF9MF0DvMTVhjtttJs+mC8r/bbC1HvgC3R+rp827RFJ/lspxYVqt9fNQ4NNVdXWb/AG6YH6wm8Gz6H5vs9Wnc6njpsvDtGfc5uEgut//5UN1wieB+9NdcAjdmcI/TPK7SR67gM8DoKr+k+6PzYvpzm7eQNd98EtJngWQ7m4iD+ee9dWtdGcMFlJfraVrRf+Plp9L6LbDK3N3H/B96Vqq31VD3VYGzGUfqzHAHq+vVNX6qvpiVZ1Cd8o6dC2C0J0iC91pvGHXcndfrqnXzdK1yvc7A/OnVd0FNc+h2//vBa5N17d1lAp4ujz+NV1Z3kd36u2J3H068t5Dab9XVT8amnZde50xwG6nGA+lq3BuSnfLqV3oAv570/1Th25bbkN3Wm8m92+vcwlM5+oe26UFqWfS7ZfX0gX2TwA+webb5f50rVczqqrbgH8FXtMq4KfSHRTeOcf8/ah9B4eH69r6b6TrqvEtularbyb5SpJfmuP6p3PDUBl+2Ea/N5Ruavq9YaTtNpOpP44b6ALzweG+3L3/pUlaEseFWbyg5eEjA/UtdEHpwa3bBcytPt1iHTeD4d/vvP4s0NUJT5tmff8+kD/oulOcCvwRcCFwdZI/zwIuSKyqW6rqI1X1O1X1s3T12Z3AMQN5g64L3nD+fpF51ldJHkR3vP8YsMPAPvwQ3bH2wJZ0rvtvS2nUeBeRCaqqHyS5jO6UOXQBR9H1Jx72IO6+ldp3B6Z9dSpB+7d7f+Zwy7WqOgs4q7VOPIWun+3Hkqyuqm/PJfvTTDsUeE9VvWkgTzvNsPyuSe41FGRP9U2brQJ+Ad2B4incM0CD7p/6P7d5P2b21vCpcu4JzHQ1/G3tdfuh6TNVdtNtl4OA+wEvHWxlbq3sw/n5abbsOOD1dP3jXkR3Wu+Tc1huTloL/C+179Ma4I3AyUl+psZ714C5breZTN2C6heY/rviLaq05EzyuDCDqVbqd3DPaySgXf/B5vXpJTOsa6513LAnDL2faf1b8h26s3gz3cf7EoCqup6un/WR6S4kXQscTdfN47h5fvZmqurcJJ+iq+em8gZdfXvGNIv8cJppc/EKutsCvrwNw9bSdc8c3H8zmcs+VmML9gS1QOHhdD9aquoW4DzglwcvHknyELp/u2e3SefS/dgOHVrly+j+NE2lm7o93E/MlIequr2qPk13McyOdP36ppadcbkZ3Ifu3/agV8+QdltguFX0UOCbzB5grwVuoTtV9Yyh4QTgKUke3rqFnEN3Cmymcvxfur52R8wwH7pW9du550HhHncAmcVUQHjXtknySLo/CYM+BTwoyQtmW1lVfaOl/UO6/vT/PMspvXmrqjuqu73Vn9HVFVMXxGzxe9WTuW63mfJzOt2frAfP0Gp/+aLkWlqApXBcGPiMB9IFgKdwz/r2GXQt6FMB+Bl0v7fZ6tM51XHDpvnt3jTK8gM+ATwK+OYMdcI91ltVl1TVn9D90Zk6DoyyDe/buvkMT98W2Ie7zzhcQtdY8pgZ8nbhwOKjHJ/X0l3bNN3++wTwoiT3Bb7ePv/XZukSM5d9rMYW7PHaL8kD6E737UF39fRudH2ypvwZ3amcj6a7NdpOdP+cbwT+FqCqvtvuwPDGJLfQ3aXjp+ge/HEOd18Rfh3dv+JDk1xIF5heTndHhqe15a4CHkD3r/lb3N2S+zVgtyS/SXfRw2215Qe5fAJYm+TLdKflX0x3AJjOTcBb2va4lO6f9bPoLpCbrhV4qrJ/Lt3FHWdOM/9auguHXkV3K7k/AD4DfL5tr43Aw+guQnxtVd2U7pZ9f5/kQ8C/tXzt18r791VVST4AHJ7k63SV4PPpLnKZqzPo+kC+p+VjD7p9+k02/5P7Prq+1e9P8td0feTuS3d6721VdfFA2n+kO+j9iLvv6jEX2yfZf5rpt1bVhUl+ka7y/A+678qOdBdR3kTXLQe67wbA7yf5OHBnVa0fIQ9zNdft9vWW7jVJvkt38Lmkqr6R5M3AP7RWqM/QnZHYm65/9rvamRxpkpbEcaGmf+jIK+jihGOr6jPDM5OcCPxRkoe139uxwOtbwHYqXReIJwIXV9UHGK2OWwzH0v3h+FzL6yV0ddyjgKdW1dR1PGfQHQ8upqtjD6brJvOptp6Z6pzpAv99gU8keT/dn5zr6fbzr9EF7L8F0I41RwKntK6QJ9O1GO9Odxz9ZlVNPfjta8BTW319LfDtqrpi+IOTPA54LLCuqs6eZv696f5AvaSq/jXJ6+juqvXpJO+k+5P3U8ADq+qoOe5jTZnLlZAOCxuY/mrx64FPA8+ZJv1BdMHMD+gq0FOAfYfShO7Wc5fQtVpcQ3f6buehdFMXzfyofe5hdLf1OYUuuL69Lfvvg59BV+m8n7tPT17Rph/ADFdP0wXqJ7VlvkdXQT1h6nMH0p1AF+w+me5iktvo/mHPeNV2W+51bV1PnSXNf9EFhmnvH0d3G6Yb2va8GPjjoWVeQlfR/4DuQpAvAL84MH8Xur7q36Y7zfpOuiB7uruInDNDvl7aPvs2utO3h7btcMVQup2Av2nbY2q/fpCughtMty3dxS//PsL38IRpvodTw1damn3pLmC6vOV1E92B+ueGPvsddN/hH9MdG2Dmu4hMdzeaAt40w+/kEfPYbr9O90S1O6bZL79K17p3C90Zi4voruCf9a4rDg6LObDEjgsz5PECusaSae/sRHdb0qIL4Kam/QZdv+Xb6erLs4EnDcyfUx3X0zY+m6E6mS5QPrbVcT9s2/xzdA+Cge5JkP/U6pub6Y4JXwR+ZWg9M9Y5Q+l2Af6c7q5W17Rt/j26e2m/ZJr0TwI+2tLcRteqfNLQNnxUy/Ot7bNPmOGz38bQXaeG5m9D12Bx9sC0Z7a83dyG/wFePbTcrPvYoRumghBpbNLdFP9ZVbXXltJqekmeTdea8qyapjVfkiRNjl1EpGUkycPpurkcC5xvcC1J0tLjRY7S8vJnwMfpTs29asJ5kSRJ07CLiCRJktQjW7AlSZKkHhlgS5IkST0ywJYkSZJ6ZIAtSZIk9cgAW5IkSeqRAbYkSZLUo/8fozqIWgZ8mXoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The bootstrapped mean of LDA accuracy is:', np.mean(amb_lda_output)[0])"
      ],
      "metadata": {
        "id": "Ap8DxHkN6GJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a4e55d-db42-42d3-ecb9-b712ed962875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The bootstrapped mean of LDA accuracy is: 0.2393724521309446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbt2rY4IGBFn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b78763c-7706-4353-fc85-3cd5d3dadcda"
      },
      "source": [
        "# The 95% confidence interval\n",
        "CI_0 = np.quantile(amb_lda_output.iloc[:,0]-amb_lda_acc,np.array([0.025,0.975]))\n",
        "left =amb_lda_acc - CI_0[1]\n",
        "right = amb_lda_acc - CI_0[0]\n",
        "print(\"The 95-percent confidence interval of the test set accuuracy for the LDA ambition model is :\",[left, right])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 95-percent confidence interval of the test set accuuracy for the LDA ambition model is : [0.2192711550339716, 0.2600370599135269]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ambition (Rescaled OSR^2 & Baseline Comparisons)"
      ],
      "metadata": {
        "id": "1OacTaNFLiU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_amb = 1 - (np.sum((amb_y_test - amb_y_train.value_counts().index[0])**2)/np.sum((amb_y_test - np.mean(amb_y_train))**2))\n",
        "rm1_lr_amb = rescaled_OSR2_1(amb_lr, amb_y_train, amb_test.drop(columns = ['(1_1-2_1_o)_att', 'attr1_1', 'museums_diff', '(3_1-pf_o)_att', 'exercise_diff', 'tvsport_diff', 'income', 'worldrank_diff', 'age_o']), 'amb_o', 'lr')\n",
        "rm2_lr_amb = rescaled_OSR2_2(amb_lr, amb_y_train, amb_test.drop(columns = ['(1_1-2_1_o)_att', 'attr1_1', 'museums_diff', '(3_1-pf_o)_att', 'exercise_diff', 'tvsport_diff', 'income', 'worldrank_diff', 'age_o']), 'amb_o', 'lr')\n",
        "rm1_lda_amb = rescaled_OSR2_1(amb_lda, amb_y_train, amb_test, 'amb_o', 'lda')\n",
        "rm2_lda_amb = rescaled_OSR2_1(amb_lda, amb_y_train, amb_test, 'amb_o', 'lda')\n",
        "\n",
        "print('Models for AMBITION:')\n",
        "print('No Rescale Bootstrapped Mean, Linear Regression OSR2 for AMBITION:', np.mean(amb_output)[0])\n",
        "print('No Rescale Bootstrapped Mean, LDA Accuracy for AMBITION:', np.mean(amb_lda_output)[0])\n",
        "print()\n",
        "print('Rescaling Linear Regression Model before OSR2 Calculation:')\n",
        "print('Rescale Method 1, Linear Regression AMBITION OSR2:', rm1_lr_amb)\n",
        "print('Rescale Method 2, Linear Regression AMBITION OSR2:', rm2_lr_amb)\n",
        "print()\n",
        "print('BASELINE:')\n",
        "print('Baseline OSR2:', base_amb)\n",
        "print()\n",
        "print('AMBITION MODEL COMPARISONS:')\n",
        "print('OSR2 difference between LR model and baseline:', np.amax(np.array([rm1_lr_amb, rm2_lr_amb, rm1_lda_amb, rm2_lda_amb, np.mean(amb_output)[0]])) - base_amb)\n",
        "print('Accuracy difference for LDA and baseline:', np.mean(amb_lda_output)[0] - amb_y_train.value_counts().values[0]/np.sum(amb_y_train.value_counts()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci3G_G4fLkZr",
        "outputId": "ad1e85d9-1c29-4109-db0a-29a61edeb8ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models for AMBITION:\n",
            "No Rescale Bootstrapped Mean, Linear Regression OSR2 for AMBITION: 0.05848478492951455\n",
            "No Rescale Bootstrapped Mean, LDA Accuracy for AMBITION: 0.2393724521309446\n",
            "\n",
            "Rescaling Linear Regression Model before OSR2 Calculation:\n",
            "Rescale Method 1, Linear Regression AMBITION OSR2: -6.86217208168576\n",
            "Rescale Method 2, Linear Regression AMBITION OSR2: -5.426562239790915\n",
            "\n",
            "BASELINE:\n",
            "Baseline OSR2: -0.016633220338508004\n",
            "\n",
            "AMBITION MODEL COMPARISONS:\n",
            "OSR2 difference between LR model and baseline: 0.07511800526802255\n",
            "Accuracy difference for LDA and baseline: 0.01342650618499866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPvex0NpsRtJ"
      },
      "source": [
        "## Shared Interests (Linear Regression + LDA)\n",
        "\n",
        "Make linear model for this trait, perform feature selection.\n",
        "Bootstrap your model and report the mean and 95% CI for OSR^2.\n",
        "\n",
        "Run LDA on this trait. Bootstrap this model and report the mean and 95% CI for accuracy. Calculate OSR^2 as well.\n",
        "\n",
        "Calculate rescaled OSR^2 using the two methods above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlXZCghYvfX0"
      },
      "source": [
        "## Shared Interests (Linear Regression - Feature Selection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3Vj_GRRPe8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdd6d7d1-33b3-446b-bb7c-0b435e05f227"
      },
      "source": [
        "# Fit linear model\n",
        "X_train = sha_X_train.copy()\n",
        "y_train = sha_y_train.copy()\n",
        "X_test = sha_X_test.copy()\n",
        "y_test = sha_y_test.copy()\n",
        "# We must add an intercept as the standard model doesn't automatically fit one\n",
        "X_train = sm.add_constant(X_train)\n",
        "\n",
        "# Fit the data to the model\n",
        "sha_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(amb_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  amb_o   R-squared:                       0.085\n",
            "Model:                            OLS   Adj. R-squared:                  0.078\n",
            "Method:                 Least Squares   F-statistic:                     12.10\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           9.39e-90\n",
            "Time:                        23:13:56   Log-Likelihood:                -12694.\n",
            "No. Observations:                6475   AIC:                         2.549e+04\n",
            "Df Residuals:                    6425   BIC:                         2.583e+04\n",
            "Df Model:                          49                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 6.7514      0.021    314.903      0.000       6.709       6.793\n",
            "gender                0.1151      0.035      3.259      0.001       0.046       0.184\n",
            "condtn                0.0185      0.023      0.792      0.428      -0.027       0.064\n",
            "order                -0.1560      0.023     -6.873      0.000      -0.201      -0.112\n",
            "int_corr              0.0763      0.022      3.467      0.001       0.033       0.119\n",
            "samerace              0.0215      0.022      0.990      0.322      -0.021       0.064\n",
            "mn_sat_o             -0.0679      0.028     -2.410      0.016      -0.123      -0.013\n",
            "tuition_o            -0.0600      0.028     -2.120      0.034      -0.116      -0.005\n",
            "exphappy_o            0.1736      0.023      7.480      0.000       0.128       0.219\n",
            "met_o                -0.1248      0.022     -5.707      0.000      -0.168      -0.082\n",
            "world_rank_o         -0.1511      0.023     -6.586      0.000      -0.196      -0.106\n",
            "masters_o            -0.0983      0.028     -3.486      0.000      -0.154      -0.043\n",
            "sinc1_1              -0.1163      0.034     -3.396      0.001      -0.183      -0.049\n",
            "intel1_1             -0.0397      0.036     -1.096      0.273      -0.111       0.031\n",
            "fun1_1               -0.0754      0.032     -2.359      0.018      -0.138      -0.013\n",
            "amb1_1                0.0239      0.032      0.745      0.456      -0.039       0.087\n",
            "shar1_1              -0.0322      0.032     -0.996      0.319      -0.096       0.031\n",
            "age_diff             -0.0240      0.023     -1.039      0.299      -0.069       0.021\n",
            "income_diff          -0.0401      0.022     -1.810      0.070      -0.084       0.003\n",
            "date_diff            -0.0251      0.024     -1.057      0.290      -0.072       0.021\n",
            "go_out_diff           0.0214      0.024      0.889      0.374      -0.026       0.069\n",
            "sports_diff           0.0215      0.025      0.859      0.391      -0.028       0.071\n",
            "dining_diff          -0.0994      0.026     -3.823      0.000      -0.150      -0.048\n",
            "art_diff              0.0351      0.029      1.232      0.218      -0.021       0.091\n",
            "hiking_diff          -0.0636      0.025     -2.546      0.011      -0.113      -0.015\n",
            "gaming_diff           0.0384      0.026      1.496      0.135      -0.012       0.089\n",
            "clubbing_diff        -0.0097      0.023     -0.419      0.676      -0.055       0.036\n",
            "reading_diff          0.1492      0.024      6.345      0.000       0.103       0.195\n",
            "tv_diff               0.0627      0.028      2.266      0.023       0.008       0.117\n",
            "theater_diff         -0.0336      0.032     -1.057      0.291      -0.096       0.029\n",
            "movies_diff           0.0917      0.028      3.270      0.001       0.037       0.147\n",
            "concerts_diff         0.0852      0.032      2.627      0.009       0.022       0.149\n",
            "music_diff            0.0271      0.030      0.918      0.358      -0.031       0.085\n",
            "shopping_diff         0.0485      0.030      1.603      0.109      -0.011       0.108\n",
            "yoga_diff            -0.0253      0.025     -1.021      0.307      -0.074       0.023\n",
            "(3_1-pf_o)_sinc      -0.0124      0.024     -0.509      0.611      -0.060       0.035\n",
            "(3_1-pf_o)_fun        0.0901      0.024      3.713      0.000       0.043       0.138\n",
            "(3_1-pf_o)_intel      0.0937      0.024      3.928      0.000       0.047       0.141\n",
            "(3_1-pf_o)_amb       -0.1308      0.025     -5.136      0.000      -0.181      -0.081\n",
            "(1_1-2_1_o)_sinc      0.0342      0.035      0.976      0.329      -0.034       0.103\n",
            "(1_1-2_1_o)_fun       0.0435      0.032      1.348      0.178      -0.020       0.107\n",
            "(1_1-2_1_o)_intel    -0.0317      0.036     -0.890      0.374      -0.102       0.038\n",
            "(1_1-2_1_o)_amb      -0.0350      0.030     -1.153      0.249      -0.095       0.025\n",
            "(1_1-2_1_o)_shar      0.0245      0.033      0.754      0.451      -0.039       0.088\n",
            "from_m               -0.0190      0.022     -0.884      0.377      -0.061       0.023\n",
            "goal_m               -0.0129      0.022     -0.598      0.550      -0.055       0.029\n",
            "imprace_m            -0.0287      0.022     -1.297      0.195      -0.072       0.015\n",
            "imprelig_m            0.0400      0.022      1.810      0.070      -0.003       0.083\n",
            "career_c_m            0.0209      0.022      0.942      0.346      -0.023       0.064\n",
            "masters_m            -0.0184      0.027     -0.671      0.502      -0.072       0.035\n",
            "==============================================================================\n",
            "Omnibus:                      169.663   Durbin-Watson:                   1.987\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              186.325\n",
            "Skew:                          -0.383   Prob(JB):                     3.47e-41\n",
            "Kurtosis:                       3.323   Cond. No.                         5.17\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTAd5Eeer5Q3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b18ae8e-c593-4751-e1b0-5e0816a1e37e"
      },
      "source": [
        "# Check VIF values\n",
        "#Check VIF values\n",
        "sha_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(X_train, sha_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_att      115.180474\n",
              "attr1_1               92.029895\n",
              "(1_1-2_1_o)_sinc      38.050239\n",
              "(1_1-2_1_o)_intel     34.868451\n",
              "intel1_1              34.420952\n",
              "sinc1_1               33.695288\n",
              "(1_1-2_1_o)_fun       32.347815\n",
              "(1_1-2_1_o)_shar      29.808634\n",
              "shar1_1               28.235763\n",
              "(1_1-2_1_o)_amb       25.543280\n",
              "fun1_1                24.940522\n",
              "amb1_1                23.562514\n",
              "museums_diff           4.630002\n",
              "art_diff               4.236244\n",
              "(3_1-pf_o)_att         3.502786\n",
              "gender                 3.044223\n",
              "concerts_diff          2.371739\n",
              "theater_diff           2.336949\n",
              "sports_diff            2.112711\n",
              "income_diff            2.109407\n",
              "age_diff               2.079827\n",
              "shopping_diff          2.057689\n",
              "income                 2.038006\n",
              "age_o                  2.031456\n",
              "music_diff             1.943634\n",
              "world_rank_o           1.926133\n",
              "(3_1-pf_o)_amb         1.897611\n",
              "(3_1-pf_o)_sinc        1.891250\n",
              "(3_1-pf_o)_intel       1.854387\n",
              "worldrank_diff         1.826370\n",
              "tv_diff                1.787047\n",
              "movies_diff            1.749388\n",
              "tvsport_diff           1.746762\n",
              "tuition_o              1.744687\n",
              "mn_sat_o               1.718266\n",
              "masters_o              1.687016\n",
              "masters_m              1.621111\n",
              "(3_1-pf_o)_fun         1.560054\n",
              "dining_diff            1.545413\n",
              "gaming_diff            1.497160\n",
              "hiking_diff            1.402230\n",
              "exercise_diff          1.401330\n",
              "yoga_diff              1.342748\n",
              "reading_diff           1.270275\n",
              "go_out_diff            1.265859\n",
              "date_diff              1.250868\n",
              "condtn                 1.207447\n",
              "clubbing_diff          1.178400\n",
              "exphappy_o             1.173412\n",
              "order                  1.122826\n",
              "career_c_m             1.078191\n",
              "int_corr               1.070303\n",
              "imprelig_m             1.067537\n",
              "imprace_m              1.065021\n",
              "met_o                  1.036877\n",
              "samerace               1.035867\n",
              "goal_m                 1.017922\n",
              "from_m                 1.007581\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GkH1HtdsRhK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d7f6c7-9fb1-4e28-f164-169aeb3622c0"
      },
      "source": [
        "# Remove highest VIF column\n",
        "X_train = X_train.drop(columns=['museums_diff'])\n",
        "\n",
        "sha_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sha_model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 shar_o   R-squared:                       0.082\n",
            "Model:                            OLS   Adj. R-squared:                  0.074\n",
            "Method:                 Least Squares   F-statistic:                     9.644\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           1.18e-77\n",
            "Time:                        23:13:58   Log-Likelihood:                -13253.\n",
            "No. Observations:                6196   AIC:                         2.662e+04\n",
            "Df Residuals:                    6138   BIC:                         2.701e+04\n",
            "Df Model:                          57                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 5.4946      0.026    209.515      0.000       5.443       5.546\n",
            "gender               -0.0942      0.046     -2.066      0.039      -0.184      -0.005\n",
            "condtn               -0.0784      0.029     -2.721      0.007      -0.135      -0.022\n",
            "order                 0.0068      0.028      0.243      0.808      -0.048       0.061\n",
            "int_corr              0.0796      0.027      2.934      0.003       0.026       0.133\n",
            "samerace              0.1460      0.027      5.471      0.000       0.094       0.198\n",
            "age_o                 0.1056      0.037      2.825      0.005       0.032       0.179\n",
            "mn_sat_o             -0.0186      0.034     -0.541      0.588      -0.086       0.049\n",
            "tuition_o            -0.0346      0.035     -0.999      0.318      -0.103       0.033\n",
            "income                0.1053      0.037      2.813      0.005       0.032       0.179\n",
            "exphappy_o            0.2698      0.028      9.500      0.000       0.214       0.325\n",
            "met_o                -0.1677      0.027     -6.281      0.000      -0.220      -0.115\n",
            "world_rank_o         -0.1158      0.036     -3.182      0.001      -0.187      -0.044\n",
            "masters_o            -0.0946      0.034     -2.778      0.005      -0.161      -0.028\n",
            "attr1_1               0.3177      0.251      1.263      0.206      -0.175       0.811\n",
            "sinc1_1               0.0438      0.152      0.288      0.774      -0.255       0.342\n",
            "intel1_1              0.3428      0.154      2.228      0.026       0.041       0.644\n",
            "fun1_1                0.1719      0.131      1.314      0.189      -0.085       0.428\n",
            "amb1_1                0.1676      0.127      1.317      0.188      -0.082       0.417\n",
            "shar1_1               0.0952      0.139      0.684      0.494      -0.178       0.368\n",
            "age_diff             -0.0358      0.038     -0.946      0.344      -0.110       0.038\n",
            "income_diff           0.0668      0.038      1.757      0.079      -0.008       0.141\n",
            "date_diff             0.0060      0.029      0.205      0.838      -0.051       0.064\n",
            "go_out_diff           0.0596      0.030      2.021      0.043       0.002       0.117\n",
            "sports_diff          -0.0650      0.038     -1.705      0.088      -0.140       0.010\n",
            "tvsport_diff          0.0192      0.035      0.554      0.579      -0.049       0.087\n",
            "exercise_diff         0.0279      0.031      0.898      0.369      -0.033       0.089\n",
            "dining_diff          -0.0632      0.032     -1.973      0.049      -0.126      -0.000\n",
            "art_diff             -0.0264      0.035     -0.752      0.452      -0.095       0.042\n",
            "hiking_diff          -0.0335      0.031     -1.079      0.281      -0.094       0.027\n",
            "gaming_diff           0.0906      0.032      2.835      0.005       0.028       0.153\n",
            "clubbing_diff         0.0008      0.028      0.028      0.978      -0.055       0.057\n",
            "reading_diff          0.0558      0.029      1.933      0.053      -0.001       0.112\n",
            "tv_diff               0.0328      0.035      0.935      0.350      -0.036       0.101\n",
            "theater_diff          0.1153      0.040      2.911      0.004       0.038       0.193\n",
            "movies_diff          -0.0194      0.035     -0.559      0.576      -0.087       0.049\n",
            "concerts_diff        -0.0444      0.040     -1.100      0.271      -0.124       0.035\n",
            "music_diff            0.0627      0.037      1.714      0.087      -0.009       0.134\n",
            "shopping_diff         0.1214      0.037      3.241      0.001       0.048       0.195\n",
            "yoga_diff             0.0687      0.030      2.262      0.024       0.009       0.128\n",
            "worldrank_diff        0.0425      0.035      1.202      0.229      -0.027       0.112\n",
            "(3_1-pf_o)_att       -0.0398      0.049     -0.810      0.418      -0.136       0.056\n",
            "(3_1-pf_o)_sinc      -0.0010      0.036     -0.028      0.978      -0.072       0.070\n",
            "(3_1-pf_o)_fun        0.1470      0.033      4.489      0.000       0.083       0.211\n",
            "(3_1-pf_o)_intel      0.1144      0.036      3.209      0.001       0.044       0.184\n",
            "(3_1-pf_o)_amb       -0.0622      0.036     -1.724      0.085      -0.133       0.009\n",
            "(1_1-2_1_o)_att      -0.8330      0.281     -2.961      0.003      -1.385      -0.282\n",
            "(1_1-2_1_o)_sinc     -0.4350      0.162     -2.690      0.007      -0.752      -0.118\n",
            "(1_1-2_1_o)_fun      -0.4005      0.149     -2.688      0.007      -0.693      -0.108\n",
            "(1_1-2_1_o)_intel    -0.7137      0.155     -4.609      0.000      -1.017      -0.410\n",
            "(1_1-2_1_o)_amb      -0.2906      0.132     -2.195      0.028      -0.550      -0.031\n",
            "(1_1-2_1_o)_shar     -0.4671      0.143     -3.264      0.001      -0.748      -0.187\n",
            "from_m               -0.0312      0.026     -1.184      0.236      -0.083       0.020\n",
            "goal_m               -0.0032      0.026     -0.122      0.903      -0.055       0.049\n",
            "imprace_m            -0.0435      0.027     -1.608      0.108      -0.097       0.010\n",
            "imprelig_m            0.0355      0.027      1.311      0.190      -0.018       0.089\n",
            "career_c_m            0.1289      0.027      4.735      0.000       0.076       0.182\n",
            "masters_m             0.0072      0.033      0.215      0.830      -0.058       0.073\n",
            "==============================================================================\n",
            "Omnibus:                       64.340   Durbin-Watson:                   2.004\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               59.718\n",
            "Skew:                          -0.202   Prob(JB):                     1.08e-13\n",
            "Kurtosis:                       2.740   Cond. No.                         45.1\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check VIF values\n",
        "sha_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "\n",
        "VIF(X_train, sha_cols).sort_values(ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtNYE9HzEzpZ",
        "outputId": "7301c290-6f6a-4bd0-8ba8-22e574c63545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_att      115.047567\n",
              "attr1_1               91.937500\n",
              "(1_1-2_1_o)_sinc      38.016977\n",
              "(1_1-2_1_o)_intel     34.859497\n",
              "intel1_1              34.420422\n",
              "sinc1_1               33.675609\n",
              "(1_1-2_1_o)_fun       32.287092\n",
              "(1_1-2_1_o)_shar      29.772718\n",
              "shar1_1               28.202526\n",
              "(1_1-2_1_o)_amb       25.478789\n",
              "fun1_1                24.891708\n",
              "amb1_1                23.529462\n",
              "(3_1-pf_o)_att         3.502111\n",
              "gender                 3.023843\n",
              "concerts_diff          2.371544\n",
              "theater_diff           2.278620\n",
              "sports_diff            2.112345\n",
              "income_diff            2.103556\n",
              "age_diff               2.079687\n",
              "shopping_diff          2.039543\n",
              "income                 2.037995\n",
              "age_o                  2.029634\n",
              "music_diff             1.942680\n",
              "world_rank_o           1.925285\n",
              "(3_1-pf_o)_amb         1.895421\n",
              "(3_1-pf_o)_sinc        1.889477\n",
              "(3_1-pf_o)_intel       1.846422\n",
              "worldrank_diff         1.820399\n",
              "art_diff               1.794882\n",
              "tv_diff                1.785115\n",
              "movies_diff            1.746261\n",
              "tuition_o              1.743917\n",
              "tvsport_diff           1.738621\n",
              "mn_sat_o               1.717582\n",
              "masters_o              1.687007\n",
              "masters_m              1.620571\n",
              "(3_1-pf_o)_fun         1.559229\n",
              "dining_diff            1.489970\n",
              "gaming_diff            1.485998\n",
              "exercise_diff          1.400424\n",
              "hiking_diff            1.398232\n",
              "yoga_diff              1.341213\n",
              "go_out_diff            1.265550\n",
              "date_diff              1.250606\n",
              "reading_diff           1.209291\n",
              "condtn                 1.207209\n",
              "clubbing_diff          1.177802\n",
              "exphappy_o             1.172535\n",
              "order                  1.122745\n",
              "career_c_m             1.078037\n",
              "int_corr               1.070295\n",
              "imprelig_m             1.067527\n",
              "imprace_m              1.064671\n",
              "met_o                  1.036405\n",
              "samerace               1.035857\n",
              "goal_m                 1.017894\n",
              "from_m                 1.007420\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rit9TWJvTyD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f3d9d8-146c-44f4-a3fa-7e0df532fde4"
      },
      "source": [
        "# since VIF values are all small - lets remove high p-valued features\n",
        "X_train = X_train.drop(columns=['order'])\n",
        "sha_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sha_model.summary())\n",
        "\n",
        "# Check VIF values\n",
        "sha_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, sha_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 shar_o   R-squared:                       0.082\n",
            "Model:                            OLS   Adj. R-squared:                  0.074\n",
            "Method:                 Least Squares   F-statistic:                     9.817\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           3.84e-78\n",
            "Time:                        23:14:00   Log-Likelihood:                -13253.\n",
            "No. Observations:                6196   AIC:                         2.662e+04\n",
            "Df Residuals:                    6139   BIC:                         2.700e+04\n",
            "Df Model:                          56                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 5.4946      0.026    209.531      0.000       5.443       5.546\n",
            "gender               -0.0941      0.046     -2.064      0.039      -0.184      -0.005\n",
            "condtn               -0.0762      0.027     -2.785      0.005      -0.130      -0.023\n",
            "int_corr              0.0797      0.027      2.937      0.003       0.027       0.133\n",
            "samerace              0.1459      0.027      5.468      0.000       0.094       0.198\n",
            "age_o                 0.1053      0.037      2.819      0.005       0.032       0.178\n",
            "mn_sat_o             -0.0185      0.034     -0.538      0.590      -0.086       0.049\n",
            "tuition_o            -0.0347      0.035     -1.001      0.317      -0.103       0.033\n",
            "income                0.1054      0.037      2.814      0.005       0.032       0.179\n",
            "exphappy_o            0.2698      0.028      9.500      0.000       0.214       0.325\n",
            "met_o                -0.1677      0.027     -6.281      0.000      -0.220      -0.115\n",
            "world_rank_o         -0.1158      0.036     -3.184      0.001      -0.187      -0.045\n",
            "masters_o            -0.0946      0.034     -2.778      0.005      -0.161      -0.028\n",
            "attr1_1               0.3183      0.251      1.266      0.206      -0.175       0.811\n",
            "sinc1_1               0.0441      0.152      0.290      0.772      -0.254       0.342\n",
            "intel1_1              0.3433      0.154      2.231      0.026       0.042       0.645\n",
            "fun1_1                0.1723      0.131      1.317      0.188      -0.084       0.429\n",
            "amb1_1                0.1676      0.127      1.318      0.188      -0.082       0.417\n",
            "shar1_1               0.0957      0.139      0.687      0.492      -0.177       0.369\n",
            "age_diff             -0.0355      0.038     -0.939      0.348      -0.110       0.039\n",
            "income_diff           0.0668      0.038      1.756      0.079      -0.008       0.141\n",
            "date_diff             0.0060      0.029      0.203      0.839      -0.052       0.063\n",
            "go_out_diff           0.0597      0.030      2.025      0.043       0.002       0.118\n",
            "sports_diff          -0.0649      0.038     -1.702      0.089      -0.140       0.010\n",
            "tvsport_diff          0.0191      0.035      0.551      0.581      -0.049       0.087\n",
            "exercise_diff         0.0279      0.031      0.898      0.369      -0.033       0.089\n",
            "dining_diff          -0.0632      0.032     -1.975      0.048      -0.126      -0.000\n",
            "art_diff             -0.0264      0.035     -0.752      0.452      -0.095       0.042\n",
            "hiking_diff          -0.0336      0.031     -1.082      0.279      -0.094       0.027\n",
            "gaming_diff           0.0908      0.032      2.841      0.005       0.028       0.153\n",
            "clubbing_diff         0.0008      0.028      0.029      0.977      -0.055       0.057\n",
            "reading_diff          0.0558      0.029      1.933      0.053      -0.001       0.112\n",
            "tv_diff               0.0327      0.035      0.934      0.350      -0.036       0.101\n",
            "theater_diff          0.1152      0.040      2.909      0.004       0.038       0.193\n",
            "movies_diff          -0.0193      0.035     -0.556      0.578      -0.087       0.049\n",
            "concerts_diff        -0.0445      0.040     -1.101      0.271      -0.124       0.035\n",
            "music_diff            0.0626      0.037      1.713      0.087      -0.009       0.134\n",
            "shopping_diff         0.1214      0.037      3.241      0.001       0.048       0.195\n",
            "yoga_diff             0.0688      0.030      2.265      0.024       0.009       0.128\n",
            "worldrank_diff        0.0426      0.035      1.204      0.229      -0.027       0.112\n",
            "(3_1-pf_o)_att       -0.0399      0.049     -0.812      0.417      -0.136       0.056\n",
            "(3_1-pf_o)_sinc      -0.0010      0.036     -0.027      0.978      -0.072       0.070\n",
            "(3_1-pf_o)_fun        0.1470      0.033      4.490      0.000       0.083       0.211\n",
            "(3_1-pf_o)_intel      0.1143      0.036      3.208      0.001       0.044       0.184\n",
            "(3_1-pf_o)_amb       -0.0622      0.036     -1.722      0.085      -0.133       0.009\n",
            "(1_1-2_1_o)_att      -0.8338      0.281     -2.965      0.003      -1.385      -0.282\n",
            "(1_1-2_1_o)_sinc     -0.4355      0.162     -2.693      0.007      -0.752      -0.119\n",
            "(1_1-2_1_o)_fun      -0.4009      0.149     -2.690      0.007      -0.693      -0.109\n",
            "(1_1-2_1_o)_intel    -0.7142      0.155     -4.613      0.000      -1.018      -0.411\n",
            "(1_1-2_1_o)_amb      -0.2909      0.132     -2.197      0.028      -0.550      -0.031\n",
            "(1_1-2_1_o)_shar     -0.4676      0.143     -3.268      0.001      -0.748      -0.187\n",
            "from_m               -0.0311      0.026     -1.183      0.237      -0.083       0.020\n",
            "goal_m               -0.0033      0.026     -0.125      0.900      -0.055       0.049\n",
            "imprace_m            -0.0437      0.027     -1.613      0.107      -0.097       0.009\n",
            "imprelig_m            0.0356      0.027      1.315      0.189      -0.017       0.089\n",
            "career_c_m            0.1290      0.027      4.737      0.000       0.076       0.182\n",
            "masters_m             0.0071      0.033      0.211      0.833      -0.058       0.072\n",
            "==============================================================================\n",
            "Omnibus:                       64.385   Durbin-Watson:                   2.004\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               59.801\n",
            "Skew:                          -0.203   Prob(JB):                     1.03e-13\n",
            "Kurtosis:                       2.741   Cond. No.                         45.1\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_att      115.030877\n",
              "attr1_1               91.927998\n",
              "(1_1-2_1_o)_sinc      38.010675\n",
              "(1_1-2_1_o)_intel     34.851726\n",
              "intel1_1              34.415375\n",
              "sinc1_1               33.672288\n",
              "(1_1-2_1_o)_fun       32.284058\n",
              "(1_1-2_1_o)_shar      29.768464\n",
              "shar1_1               28.197482\n",
              "(1_1-2_1_o)_amb       25.477352\n",
              "fun1_1                24.888474\n",
              "amb1_1                23.529434\n",
              "(3_1-pf_o)_att         3.501876\n",
              "gender                 3.023573\n",
              "concerts_diff          2.371519\n",
              "theater_diff           2.278403\n",
              "sports_diff            2.112061\n",
              "income_diff            2.103539\n",
              "age_diff               2.077707\n",
              "shopping_diff          2.039532\n",
              "income                 2.037977\n",
              "age_o                  2.027158\n",
              "music_diff             1.942641\n",
              "world_rank_o           1.925238\n",
              "(3_1-pf_o)_amb         1.895294\n",
              "(3_1-pf_o)_sinc        1.889446\n",
              "(3_1-pf_o)_intel       1.846393\n",
              "worldrank_diff         1.820320\n",
              "art_diff               1.794877\n",
              "tv_diff                1.785088\n",
              "movies_diff            1.745864\n",
              "tuition_o              1.743863\n",
              "tvsport_diff           1.738369\n",
              "mn_sat_o               1.717262\n",
              "masters_o              1.687007\n",
              "masters_m              1.620252\n",
              "(3_1-pf_o)_fun         1.559229\n",
              "dining_diff            1.489904\n",
              "gaming_diff            1.485318\n",
              "exercise_diff          1.400414\n",
              "hiking_diff            1.398041\n",
              "yoga_diff              1.341003\n",
              "go_out_diff            1.265343\n",
              "date_diff              1.250569\n",
              "reading_diff           1.209290\n",
              "clubbing_diff          1.177789\n",
              "exphappy_o             1.172535\n",
              "condtn                 1.088720\n",
              "career_c_m             1.077988\n",
              "int_corr               1.070141\n",
              "imprelig_m             1.067337\n",
              "imprace_m              1.064271\n",
              "met_o                  1.036390\n",
              "samerace               1.035649\n",
              "goal_m                 1.017733\n",
              "from_m                 1.007376\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4NFqhPBUCD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6f75cd-741d-4c58-e1e8-8e6a6d6a81d7"
      },
      "source": [
        "# since VIF values are still all small - lets continue to remove high p-valued features\n",
        "X_train = X_train.drop(columns=['movies_diff'])\n",
        "sha_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sha_model.summary())\n",
        "\n",
        "# Check VIF values\n",
        "sha_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, sha_cols).sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 shar_o   R-squared:                       0.082\n",
            "Model:                            OLS   Adj. R-squared:                  0.074\n",
            "Method:                 Least Squares   F-statistic:                     9.991\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           1.39e-78\n",
            "Time:                        23:14:02   Log-Likelihood:                -13254.\n",
            "No. Observations:                6196   AIC:                         2.662e+04\n",
            "Df Residuals:                    6140   BIC:                         2.700e+04\n",
            "Df Model:                          55                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 5.4946      0.026    209.543      0.000       5.443       5.546\n",
            "gender               -0.0921      0.045     -2.027      0.043      -0.181      -0.003\n",
            "condtn               -0.0763      0.027     -2.789      0.005      -0.130      -0.023\n",
            "int_corr              0.0795      0.027      2.932      0.003       0.026       0.133\n",
            "samerace              0.1462      0.027      5.480      0.000       0.094       0.199\n",
            "age_o                 0.1053      0.037      2.819      0.005       0.032       0.178\n",
            "mn_sat_o             -0.0180      0.034     -0.525      0.600      -0.085       0.049\n",
            "tuition_o            -0.0362      0.035     -1.047      0.295      -0.104       0.032\n",
            "income                0.1057      0.037      2.822      0.005       0.032       0.179\n",
            "exphappy_o            0.2698      0.028      9.503      0.000       0.214       0.326\n",
            "met_o                -0.1673      0.027     -6.269      0.000      -0.220      -0.115\n",
            "world_rank_o         -0.1160      0.036     -3.189      0.001      -0.187      -0.045\n",
            "masters_o            -0.0952      0.034     -2.795      0.005      -0.162      -0.028\n",
            "attr1_1               0.3058      0.250      1.221      0.222      -0.185       0.797\n",
            "sinc1_1               0.0373      0.152      0.246      0.806      -0.260       0.335\n",
            "intel1_1              0.3374      0.153      2.198      0.028       0.037       0.638\n",
            "fun1_1                0.1661      0.130      1.274      0.203      -0.089       0.422\n",
            "amb1_1                0.1608      0.127      1.270      0.204      -0.087       0.409\n",
            "shar1_1               0.0880      0.139      0.635      0.526      -0.184       0.360\n",
            "age_diff             -0.0357      0.038     -0.946      0.344      -0.110       0.038\n",
            "income_diff           0.0668      0.038      1.756      0.079      -0.008       0.141\n",
            "date_diff             0.0056      0.029      0.192      0.848      -0.052       0.063\n",
            "go_out_diff           0.0585      0.029      1.988      0.047       0.001       0.116\n",
            "sports_diff          -0.0660      0.038     -1.733      0.083      -0.141       0.009\n",
            "tvsport_diff          0.0212      0.034      0.616      0.538      -0.046       0.089\n",
            "exercise_diff         0.0282      0.031      0.910      0.363      -0.033       0.089\n",
            "dining_diff          -0.0644      0.032     -2.018      0.044      -0.127      -0.002\n",
            "art_diff             -0.0262      0.035     -0.746      0.455      -0.095       0.043\n",
            "hiking_diff          -0.0333      0.031     -1.074      0.283      -0.094       0.027\n",
            "gaming_diff           0.0914      0.032      2.860      0.004       0.029       0.154\n",
            "clubbing_diff         0.0002      0.028      0.008      0.993      -0.056       0.056\n",
            "reading_diff          0.0558      0.029      1.936      0.053      -0.001       0.112\n",
            "tv_diff               0.0267      0.033      0.801      0.423      -0.039       0.092\n",
            "theater_diff          0.1080      0.037      2.886      0.004       0.035       0.181\n",
            "concerts_diff        -0.0473      0.040     -1.182      0.237      -0.126       0.031\n",
            "music_diff            0.0608      0.036      1.669      0.095      -0.011       0.132\n",
            "shopping_diff         0.1215      0.037      3.245      0.001       0.048       0.195\n",
            "yoga_diff             0.0690      0.030      2.271      0.023       0.009       0.129\n",
            "worldrank_diff        0.0433      0.035      1.226      0.220      -0.026       0.113\n",
            "(3_1-pf_o)_att       -0.0394      0.049     -0.803      0.422      -0.136       0.057\n",
            "(3_1-pf_o)_sinc    7.704e-06      0.036      0.000      1.000      -0.071       0.071\n",
            "(3_1-pf_o)_fun        0.1478      0.033      4.519      0.000       0.084       0.212\n",
            "(3_1-pf_o)_intel      0.1162      0.035      3.275      0.001       0.047       0.186\n",
            "(3_1-pf_o)_amb       -0.0618      0.036     -1.713      0.087      -0.133       0.009\n",
            "(1_1-2_1_o)_att      -0.8215      0.280     -2.930      0.003      -1.371      -0.272\n",
            "(1_1-2_1_o)_sinc     -0.4287      0.161     -2.659      0.008      -0.745      -0.113\n",
            "(1_1-2_1_o)_fun      -0.3947      0.149     -2.657      0.008      -0.686      -0.103\n",
            "(1_1-2_1_o)_intel    -0.7083      0.154     -4.586      0.000      -1.011      -0.405\n",
            "(1_1-2_1_o)_amb      -0.2846      0.132     -2.158      0.031      -0.543      -0.026\n",
            "(1_1-2_1_o)_shar     -0.4609      0.143     -3.233      0.001      -0.740      -0.181\n",
            "from_m               -0.0312      0.026     -1.186      0.235      -0.083       0.020\n",
            "goal_m               -0.0031      0.026     -0.116      0.908      -0.055       0.049\n",
            "imprace_m            -0.0435      0.027     -1.609      0.108      -0.097       0.010\n",
            "imprelig_m            0.0356      0.027      1.312      0.189      -0.018       0.089\n",
            "career_c_m            0.1290      0.027      4.737      0.000       0.076       0.182\n",
            "masters_m             0.0065      0.033      0.195      0.846      -0.059       0.072\n",
            "==============================================================================\n",
            "Omnibus:                       63.874   Durbin-Watson:                   2.004\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               59.242\n",
            "Skew:                          -0.201   Prob(JB):                     1.37e-13\n",
            "Kurtosis:                       2.740   Cond. No.                         43.7\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_att      114.309405\n",
              "attr1_1               91.184600\n",
              "(1_1-2_1_o)_sinc      37.792660\n",
              "(1_1-2_1_o)_intel     34.683958\n",
              "intel1_1              34.251918\n",
              "sinc1_1               33.453521\n",
              "(1_1-2_1_o)_fun       32.105458\n",
              "(1_1-2_1_o)_shar      29.558715\n",
              "shar1_1               27.915818\n",
              "(1_1-2_1_o)_amb       25.290043\n",
              "fun1_1                24.710859\n",
              "amb1_1                23.311674\n",
              "(3_1-pf_o)_att         3.500870\n",
              "gender                 3.004928\n",
              "concerts_diff          2.332115\n",
              "sports_diff            2.106498\n",
              "income_diff            2.103538\n",
              "age_diff               2.077406\n",
              "shopping_diff          2.039450\n",
              "income                 2.037587\n",
              "theater_diff           2.035389\n",
              "age_o                  2.027158\n",
              "music_diff             1.926463\n",
              "world_rank_o           1.925051\n",
              "(3_1-pf_o)_amb         1.894760\n",
              "(3_1-pf_o)_sinc        1.884888\n",
              "(3_1-pf_o)_intel       1.829922\n",
              "worldrank_diff         1.817786\n",
              "art_diff               1.794678\n",
              "tuition_o              1.733329\n",
              "tvsport_diff           1.717514\n",
              "mn_sat_o               1.716292\n",
              "masters_o              1.685664\n",
              "masters_m              1.618787\n",
              "tv_diff                1.614069\n",
              "(3_1-pf_o)_fun         1.556191\n",
              "gaming_diff            1.483827\n",
              "dining_diff            1.482882\n",
              "exercise_diff          1.399771\n",
              "hiking_diff            1.397725\n",
              "yoga_diff              1.340849\n",
              "go_out_diff            1.257833\n",
              "date_diff              1.250005\n",
              "reading_diff           1.209271\n",
              "clubbing_diff          1.176207\n",
              "exphappy_o             1.172514\n",
              "condtn                 1.088656\n",
              "career_c_m             1.077988\n",
              "int_corr               1.070043\n",
              "imprelig_m             1.067316\n",
              "imprace_m              1.064192\n",
              "met_o                  1.035775\n",
              "samerace               1.035260\n",
              "goal_m                 1.017474\n",
              "from_m                 1.007329\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since VIF values are still all small but R squared has not changed - lets continue to remove high p-valued features\n",
        "X_train = X_train.drop(columns=['mn_sat_o'])\n",
        "sha_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sha_model.summary())\n",
        "\n",
        "# Check VIF values\n",
        "sha_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, sha_cols).sort_values(ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb5vnh2CFbE2",
        "outputId": "d044c490-3417-4410-9ef5-87b6ce0489d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 shar_o   R-squared:                       0.082\n",
            "Model:                            OLS   Adj. R-squared:                  0.074\n",
            "Method:                 Least Squares   F-statistic:                     10.17\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           4.89e-79\n",
            "Time:                        23:14:04   Log-Likelihood:                -13254.\n",
            "No. Observations:                6196   AIC:                         2.662e+04\n",
            "Df Residuals:                    6141   BIC:                         2.699e+04\n",
            "Df Model:                          54                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 5.4946      0.026    209.555      0.000       5.443       5.546\n",
            "gender               -0.0913      0.045     -2.009      0.045      -0.180      -0.002\n",
            "condtn               -0.0768      0.027     -2.810      0.005      -0.130      -0.023\n",
            "int_corr              0.0793      0.027      2.925      0.003       0.026       0.132\n",
            "samerace              0.1464      0.027      5.486      0.000       0.094       0.199\n",
            "age_o                 0.1060      0.037      2.842      0.005       0.033       0.179\n",
            "tuition_o            -0.0469      0.028     -1.686      0.092      -0.101       0.008\n",
            "income                0.1056      0.037      2.820      0.005       0.032       0.179\n",
            "exphappy_o            0.2707      0.028      9.549      0.000       0.215       0.326\n",
            "met_o                -0.1672      0.027     -6.266      0.000      -0.220      -0.115\n",
            "world_rank_o         -0.1156      0.036     -3.178      0.001      -0.187      -0.044\n",
            "masters_o            -0.0947      0.034     -2.782      0.005      -0.161      -0.028\n",
            "attr1_1               0.3080      0.250      1.230      0.219      -0.183       0.799\n",
            "sinc1_1               0.0369      0.152      0.243      0.808      -0.260       0.334\n",
            "intel1_1              0.3378      0.153      2.201      0.028       0.037       0.639\n",
            "fun1_1                0.1673      0.130      1.283      0.199      -0.088       0.423\n",
            "amb1_1                0.1636      0.127      1.293      0.196      -0.084       0.412\n",
            "shar1_1               0.0886      0.139      0.640      0.522      -0.183       0.360\n",
            "age_diff             -0.0362      0.038     -0.959      0.337      -0.110       0.038\n",
            "income_diff           0.0665      0.038      1.748      0.081      -0.008       0.141\n",
            "date_diff             0.0056      0.029      0.191      0.849      -0.052       0.063\n",
            "go_out_diff           0.0583      0.029      1.982      0.048       0.001       0.116\n",
            "sports_diff          -0.0665      0.038     -1.747      0.081      -0.141       0.008\n",
            "tvsport_diff          0.0219      0.034      0.639      0.523      -0.045       0.089\n",
            "exercise_diff         0.0291      0.031      0.938      0.348      -0.032       0.090\n",
            "dining_diff          -0.0646      0.032     -2.024      0.043      -0.127      -0.002\n",
            "art_diff             -0.0271      0.035     -0.772      0.440      -0.096       0.042\n",
            "hiking_diff          -0.0336      0.031     -1.083      0.279      -0.094       0.027\n",
            "gaming_diff           0.0915      0.032      2.866      0.004       0.029       0.154\n",
            "clubbing_diff         0.0008      0.028      0.028      0.978      -0.055       0.057\n",
            "reading_diff          0.0556      0.029      1.929      0.054      -0.001       0.112\n",
            "tv_diff               0.0261      0.033      0.783      0.433      -0.039       0.091\n",
            "theater_diff          0.1088      0.037      2.911      0.004       0.036       0.182\n",
            "concerts_diff        -0.0468      0.040     -1.169      0.243      -0.125       0.032\n",
            "music_diff            0.0604      0.036      1.660      0.097      -0.011       0.132\n",
            "shopping_diff         0.1218      0.037      3.252      0.001       0.048       0.195\n",
            "yoga_diff             0.0702      0.030      2.319      0.020       0.011       0.130\n",
            "worldrank_diff        0.0433      0.035      1.224      0.221      -0.026       0.113\n",
            "(3_1-pf_o)_att       -0.0391      0.049     -0.798      0.425      -0.135       0.057\n",
            "(3_1-pf_o)_sinc      -0.0014      0.036     -0.039      0.969      -0.072       0.069\n",
            "(3_1-pf_o)_fun        0.1477      0.033      4.514      0.000       0.084       0.212\n",
            "(3_1-pf_o)_intel      0.1164      0.035      3.282      0.001       0.047       0.186\n",
            "(3_1-pf_o)_amb       -0.0608      0.036     -1.688      0.091      -0.132       0.010\n",
            "(1_1-2_1_o)_att      -0.8252      0.280     -2.944      0.003      -1.375      -0.276\n",
            "(1_1-2_1_o)_sinc     -0.4290      0.161     -2.661      0.008      -0.745      -0.113\n",
            "(1_1-2_1_o)_fun      -0.3967      0.149     -2.671      0.008      -0.688      -0.105\n",
            "(1_1-2_1_o)_intel    -0.7096      0.154     -4.595      0.000      -1.012      -0.407\n",
            "(1_1-2_1_o)_amb      -0.2875      0.132     -2.182      0.029      -0.546      -0.029\n",
            "(1_1-2_1_o)_shar     -0.4626      0.143     -3.246      0.001      -0.742      -0.183\n",
            "from_m               -0.0310      0.026     -1.180      0.238      -0.083       0.021\n",
            "goal_m               -0.0031      0.026     -0.116      0.908      -0.055       0.049\n",
            "imprace_m            -0.0434      0.027     -1.606      0.108      -0.096       0.010\n",
            "imprelig_m            0.0350      0.027      1.293      0.196      -0.018       0.088\n",
            "career_c_m            0.1282      0.027      4.716      0.000       0.075       0.182\n",
            "masters_m             0.0061      0.033      0.183      0.855      -0.059       0.071\n",
            "==============================================================================\n",
            "Omnibus:                       63.636   Durbin-Watson:                   2.003\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               58.932\n",
            "Skew:                          -0.200   Prob(JB):                     1.60e-13\n",
            "Kurtosis:                       2.740   Cond. No.                         43.6\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_att      114.236827\n",
              "attr1_1               91.158662\n",
              "(1_1-2_1_o)_sinc      37.792062\n",
              "(1_1-2_1_o)_intel     34.674905\n",
              "intel1_1              34.250941\n",
              "sinc1_1               33.452392\n",
              "(1_1-2_1_o)_fun       32.085599\n",
              "(1_1-2_1_o)_shar      29.543634\n",
              "shar1_1               27.913550\n",
              "(1_1-2_1_o)_amb       25.245655\n",
              "fun1_1                24.703859\n",
              "amb1_1                23.272212\n",
              "(3_1-pf_o)_att         3.500442\n",
              "gender                 3.000923\n",
              "concerts_diff          2.330430\n",
              "sports_diff            2.105143\n",
              "income_diff            2.102935\n",
              "age_diff               2.076074\n",
              "shopping_diff          2.039137\n",
              "income                 2.037534\n",
              "theater_diff           2.031846\n",
              "age_o                  2.024112\n",
              "music_diff             1.925766\n",
              "world_rank_o           1.924034\n",
              "(3_1-pf_o)_amb         1.889576\n",
              "(3_1-pf_o)_sinc        1.874687\n",
              "(3_1-pf_o)_intel       1.829687\n",
              "worldrank_diff         1.817768\n",
              "art_diff               1.790826\n",
              "tvsport_diff           1.714450\n",
              "masters_o              1.684509\n",
              "masters_m              1.617946\n",
              "tv_diff                1.612088\n",
              "(3_1-pf_o)_fun         1.556036\n",
              "gaming_diff            1.483691\n",
              "dining_diff            1.482706\n",
              "hiking_diff            1.397365\n",
              "exercise_diff          1.395990\n",
              "yoga_diff              1.332858\n",
              "go_out_diff            1.257649\n",
              "date_diff              1.249999\n",
              "reading_diff           1.209094\n",
              "clubbing_diff          1.174575\n",
              "exphappy_o             1.168655\n",
              "tuition_o              1.125235\n",
              "condtn                 1.087277\n",
              "career_c_m             1.074853\n",
              "int_corr               1.069772\n",
              "imprelig_m             1.065763\n",
              "imprace_m              1.064159\n",
              "met_o                  1.035700\n",
              "samerace               1.035177\n",
              "goal_m                 1.017473\n",
              "from_m                 1.007162\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since VIF values are still all small but R squared has not changed - lets continue to remove high p-valued features\n",
        "X_train = X_train.drop(columns=['art_diff'])\n",
        "sha_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sha_model.summary())\n",
        "\n",
        "# Check VIF values\n",
        "sha_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, sha_cols).sort_values(ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuE-GU7tFmui",
        "outputId": "e64fb6c5-4c02-41e3-c0b7-9780a613d4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 shar_o   R-squared:                       0.082\n",
            "Model:                            OLS   Adj. R-squared:                  0.074\n",
            "Method:                 Least Squares   F-statistic:                     10.35\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           1.98e-79\n",
            "Time:                        23:14:06   Log-Likelihood:                -13254.\n",
            "No. Observations:                6196   AIC:                         2.662e+04\n",
            "Df Residuals:                    6142   BIC:                         2.698e+04\n",
            "Df Model:                          53                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 5.4946      0.026    209.562      0.000       5.443       5.546\n",
            "gender               -0.0927      0.045     -2.043      0.041      -0.182      -0.004\n",
            "condtn               -0.0768      0.027     -2.807      0.005      -0.130      -0.023\n",
            "int_corr              0.0794      0.027      2.928      0.003       0.026       0.133\n",
            "samerace              0.1463      0.027      5.484      0.000       0.094       0.199\n",
            "age_o                 0.1058      0.037      2.835      0.005       0.033       0.179\n",
            "tuition_o            -0.0472      0.028     -1.697      0.090      -0.102       0.007\n",
            "income                0.1047      0.037      2.799      0.005       0.031       0.178\n",
            "exphappy_o            0.2700      0.028      9.529      0.000       0.214       0.325\n",
            "met_o                -0.1672      0.027     -6.266      0.000      -0.220      -0.115\n",
            "world_rank_o         -0.1155      0.036     -3.176      0.002      -0.187      -0.044\n",
            "masters_o            -0.0935      0.034     -2.751      0.006      -0.160      -0.027\n",
            "attr1_1               0.3052      0.250      1.219      0.223      -0.186       0.796\n",
            "sinc1_1               0.0330      0.152      0.218      0.828      -0.264       0.330\n",
            "intel1_1              0.3375      0.153      2.199      0.028       0.037       0.638\n",
            "fun1_1                0.1671      0.130      1.282      0.200      -0.088       0.423\n",
            "amb1_1                0.1613      0.126      1.275      0.202      -0.087       0.409\n",
            "shar1_1               0.0876      0.139      0.632      0.527      -0.184       0.359\n",
            "age_diff             -0.0368      0.038     -0.975      0.330      -0.111       0.037\n",
            "income_diff           0.0659      0.038      1.735      0.083      -0.009       0.140\n",
            "date_diff             0.0062      0.029      0.210      0.834      -0.051       0.064\n",
            "go_out_diff           0.0569      0.029      1.938      0.053      -0.001       0.114\n",
            "sports_diff          -0.0662      0.038     -1.740      0.082      -0.141       0.008\n",
            "tvsport_diff          0.0214      0.034      0.623      0.533      -0.046       0.089\n",
            "exercise_diff         0.0298      0.031      0.962      0.336      -0.031       0.090\n",
            "dining_diff          -0.0678      0.032     -2.142      0.032      -0.130      -0.006\n",
            "hiking_diff          -0.0364      0.031     -1.184      0.236      -0.097       0.024\n",
            "gaming_diff           0.0943      0.032      2.973      0.003       0.032       0.157\n",
            "clubbing_diff        -0.0014      0.028     -0.050      0.960      -0.057       0.054\n",
            "reading_diff          0.0533      0.029      1.859      0.063      -0.003       0.110\n",
            "tv_diff               0.0278      0.033      0.836      0.403      -0.037       0.093\n",
            "theater_diff          0.0990      0.035      2.816      0.005       0.030       0.168\n",
            "concerts_diff        -0.0504      0.040     -1.269      0.205      -0.128       0.027\n",
            "music_diff            0.0590      0.036      1.623      0.105      -0.012       0.130\n",
            "shopping_diff         0.1209      0.037      3.230      0.001       0.048       0.194\n",
            "yoga_diff             0.0674      0.030      2.243      0.025       0.009       0.126\n",
            "worldrank_diff        0.0447      0.035      1.265      0.206      -0.025       0.114\n",
            "(3_1-pf_o)_att       -0.0387      0.049     -0.789      0.430      -0.135       0.057\n",
            "(3_1-pf_o)_sinc      -0.0024      0.036     -0.066      0.948      -0.073       0.068\n",
            "(3_1-pf_o)_fun        0.1486      0.033      4.547      0.000       0.085       0.213\n",
            "(3_1-pf_o)_intel      0.1180      0.035      3.332      0.001       0.049       0.187\n",
            "(3_1-pf_o)_amb       -0.0613      0.036     -1.701      0.089      -0.132       0.009\n",
            "(1_1-2_1_o)_att      -0.8167      0.280     -2.916      0.004      -1.366      -0.268\n",
            "(1_1-2_1_o)_sinc     -0.4230      0.161     -2.627      0.009      -0.739      -0.107\n",
            "(1_1-2_1_o)_fun      -0.3938      0.148     -2.652      0.008      -0.685      -0.103\n",
            "(1_1-2_1_o)_intel    -0.7056      0.154     -4.572      0.000      -1.008      -0.403\n",
            "(1_1-2_1_o)_amb      -0.2842      0.132     -2.158      0.031      -0.542      -0.026\n",
            "(1_1-2_1_o)_shar     -0.4585      0.142     -3.219      0.001      -0.738      -0.179\n",
            "from_m               -0.0312      0.026     -1.185      0.236      -0.083       0.020\n",
            "goal_m               -0.0025      0.026     -0.094      0.925      -0.054       0.049\n",
            "imprace_m            -0.0435      0.027     -1.610      0.107      -0.097       0.009\n",
            "imprelig_m            0.0346      0.027      1.280      0.201      -0.018       0.088\n",
            "career_c_m            0.1280      0.027      4.707      0.000       0.075       0.181\n",
            "masters_m             0.0069      0.033      0.208      0.835      -0.058       0.072\n",
            "==============================================================================\n",
            "Omnibus:                       63.118   Durbin-Watson:                   2.003\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               58.450\n",
            "Skew:                          -0.199   Prob(JB):                     2.03e-13\n",
            "Kurtosis:                       2.740   Cond. No.                         42.2\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_att      114.062888\n",
              "attr1_1               91.139202\n",
              "(1_1-2_1_o)_sinc      37.701819\n",
              "(1_1-2_1_o)_intel     34.636300\n",
              "intel1_1              34.250679\n",
              "sinc1_1               33.416081\n",
              "(1_1-2_1_o)_fun       32.066115\n",
              "(1_1-2_1_o)_shar      29.502100\n",
              "shar1_1               27.911123\n",
              "(1_1-2_1_o)_amb       25.218771\n",
              "fun1_1                24.703804\n",
              "amb1_1                23.259564\n",
              "(3_1-pf_o)_att         3.499986\n",
              "gender                 2.995817\n",
              "concerts_diff          2.297915\n",
              "sports_diff            2.104975\n",
              "income_diff            2.102297\n",
              "age_diff               2.075291\n",
              "shopping_diff          2.037297\n",
              "income                 2.035914\n",
              "age_o                  2.023965\n",
              "world_rank_o           1.924006\n",
              "music_diff             1.920908\n",
              "(3_1-pf_o)_amb         1.889044\n",
              "(3_1-pf_o)_sinc        1.872375\n",
              "(3_1-pf_o)_intel       1.823621\n",
              "worldrank_diff         1.813184\n",
              "theater_diff           1.796733\n",
              "tvsport_diff           1.713717\n",
              "masters_o              1.681369\n",
              "masters_m              1.616227\n",
              "tv_diff                1.605076\n",
              "(3_1-pf_o)_fun         1.553735\n",
              "gaming_diff            1.464445\n",
              "dining_diff            1.457882\n",
              "exercise_diff          1.394764\n",
              "hiking_diff            1.377051\n",
              "yoga_diff              1.314137\n",
              "go_out_diff            1.252834\n",
              "date_diff              1.249208\n",
              "reading_diff           1.195990\n",
              "exphappy_o             1.167272\n",
              "clubbing_diff          1.162667\n",
              "tuition_o              1.125007\n",
              "condtn                 1.087261\n",
              "career_c_m             1.074684\n",
              "int_corr               1.069749\n",
              "imprelig_m             1.065420\n",
              "imprace_m              1.064134\n",
              "met_o                  1.035700\n",
              "samerace               1.035170\n",
              "goal_m                 1.016662\n",
              "from_m                 1.007116\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# since VIF values are still all small but R squared has not changed - lets continue to remove high p-valued features\n",
        "X_train = X_train.drop(columns=['tvsport_diff'])\n",
        "sha_model = sm.OLS(y_train, X_train).fit() #ordinary least square\n",
        "print(sha_model.summary())\n",
        "\n",
        "# Check VIF values\n",
        "sha_cols = X_train.drop(columns = ['const']).columns.to_list()\n",
        "VIF(X_train, sha_cols).sort_values(ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffys5TVcFxbt",
        "outputId": "82326a15-467d-4d38-93e2-bbed7d770793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                 shar_o   R-squared:                       0.082\n",
            "Model:                            OLS   Adj. R-squared:                  0.074\n",
            "Method:                 Least Squares   F-statistic:                     10.55\n",
            "Date:                Thu, 16 Dec 2021   Prob (F-statistic):           7.21e-80\n",
            "Time:                        23:14:08   Log-Likelihood:                -13254.\n",
            "No. Observations:                6196   AIC:                         2.661e+04\n",
            "Df Residuals:                    6143   BIC:                         2.697e+04\n",
            "Df Model:                          52                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=====================================================================================\n",
            "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------------\n",
            "const                 5.4946      0.026    209.572      0.000       5.443       5.546\n",
            "gender               -0.0935      0.045     -2.061      0.039      -0.182      -0.005\n",
            "condtn               -0.0770      0.027     -2.815      0.005      -0.131      -0.023\n",
            "int_corr              0.0793      0.027      2.925      0.003       0.026       0.132\n",
            "samerace              0.1464      0.027      5.487      0.000       0.094       0.199\n",
            "age_o                 0.1056      0.037      2.831      0.005       0.032       0.179\n",
            "tuition_o            -0.0471      0.028     -1.695      0.090      -0.102       0.007\n",
            "income                0.1047      0.037      2.798      0.005       0.031       0.178\n",
            "exphappy_o            0.2695      0.028      9.517      0.000       0.214       0.325\n",
            "met_o                -0.1669      0.027     -6.257      0.000      -0.219      -0.115\n",
            "world_rank_o         -0.1153      0.036     -3.170      0.002      -0.187      -0.044\n",
            "masters_o            -0.0925      0.034     -2.724      0.006      -0.159      -0.026\n",
            "attr1_1               0.3157      0.250      1.264      0.206      -0.174       0.805\n",
            "sinc1_1               0.0411      0.151      0.272      0.785      -0.255       0.337\n",
            "intel1_1              0.3440      0.153      2.247      0.025       0.044       0.644\n",
            "fun1_1                0.1709      0.130      1.313      0.189      -0.084       0.426\n",
            "amb1_1                0.1667      0.126      1.321      0.187      -0.081       0.414\n",
            "shar1_1               0.0924      0.138      0.668      0.504      -0.179       0.364\n",
            "age_diff             -0.0371      0.038     -0.982      0.326      -0.111       0.037\n",
            "income_diff           0.0660      0.038      1.736      0.083      -0.009       0.141\n",
            "date_diff             0.0075      0.029      0.256      0.798      -0.050       0.065\n",
            "go_out_diff           0.0575      0.029      1.959      0.050   -2.99e-05       0.115\n",
            "sports_diff          -0.0561      0.034     -1.630      0.103      -0.124       0.011\n",
            "exercise_diff         0.0310      0.031      1.003      0.316      -0.030       0.092\n",
            "dining_diff          -0.0679      0.032     -2.143      0.032      -0.130      -0.006\n",
            "hiking_diff          -0.0378      0.031     -1.232      0.218      -0.098       0.022\n",
            "gaming_diff           0.0973      0.031      3.101      0.002       0.036       0.159\n",
            "clubbing_diff        -0.0011      0.028     -0.039      0.969      -0.057       0.054\n",
            "reading_diff          0.0519      0.029      1.817      0.069      -0.004       0.108\n",
            "tv_diff               0.0328      0.032      1.016      0.310      -0.030       0.096\n",
            "theater_diff          0.0989      0.035      2.815      0.005       0.030       0.168\n",
            "concerts_diff        -0.0493      0.040     -1.242      0.214      -0.127       0.029\n",
            "music_diff            0.0597      0.036      1.645      0.100      -0.011       0.131\n",
            "shopping_diff         0.1202      0.037      3.214      0.001       0.047       0.194\n",
            "yoga_diff             0.0665      0.030      2.216      0.027       0.008       0.125\n",
            "worldrank_diff        0.0447      0.035      1.267      0.205      -0.024       0.114\n",
            "(3_1-pf_o)_att       -0.0389      0.049     -0.793      0.428      -0.135       0.057\n",
            "(3_1-pf_o)_sinc      -0.0029      0.036     -0.081      0.935      -0.073       0.067\n",
            "(3_1-pf_o)_fun        0.1477      0.033      4.524      0.000       0.084       0.212\n",
            "(3_1-pf_o)_intel      0.1178      0.035      3.326      0.001       0.048       0.187\n",
            "(3_1-pf_o)_amb       -0.0611      0.036     -1.695      0.090      -0.132       0.010\n",
            "(1_1-2_1_o)_att      -0.8230      0.280     -2.941      0.003      -1.372      -0.274\n",
            "(1_1-2_1_o)_sinc     -0.4290      0.161     -2.670      0.008      -0.744      -0.114\n",
            "(1_1-2_1_o)_fun      -0.3945      0.148     -2.657      0.008      -0.686      -0.103\n",
            "(1_1-2_1_o)_intel    -0.7085      0.154     -4.594      0.000      -1.011      -0.406\n",
            "(1_1-2_1_o)_amb      -0.2876      0.132     -2.186      0.029      -0.545      -0.030\n",
            "(1_1-2_1_o)_shar     -0.4602      0.142     -3.232      0.001      -0.739      -0.181\n",
            "from_m               -0.0312      0.026     -1.184      0.236      -0.083       0.020\n",
            "goal_m               -0.0024      0.026     -0.089      0.929      -0.054       0.049\n",
            "imprace_m            -0.0436      0.027     -1.611      0.107      -0.097       0.009\n",
            "imprelig_m            0.0344      0.027      1.272      0.204      -0.019       0.087\n",
            "career_c_m            0.1278      0.027      4.702      0.000       0.075       0.181\n",
            "masters_m             0.0075      0.033      0.225      0.822      -0.058       0.073\n",
            "==============================================================================\n",
            "Omnibus:                       62.843   Durbin-Watson:                   2.003\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               58.093\n",
            "Skew:                          -0.198   Prob(JB):                     2.43e-13\n",
            "Kurtosis:                       2.739   Cond. No.                         41.9\n",
            "==============================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1_1-2_1_o)_att      113.915593\n",
              "attr1_1               90.723704\n",
              "(1_1-2_1_o)_sinc      37.562837\n",
              "(1_1-2_1_o)_intel     34.603977\n",
              "intel1_1              34.090512\n",
              "sinc1_1               33.168993\n",
              "(1_1-2_1_o)_fun       32.064291\n",
              "(1_1-2_1_o)_shar      29.490446\n",
              "shar1_1               27.826197\n",
              "(1_1-2_1_o)_amb       25.174210\n",
              "fun1_1                24.650466\n",
              "amb1_1                23.151774\n",
              "(3_1-pf_o)_att         3.499800\n",
              "gender                 2.993534\n",
              "concerts_diff          2.293350\n",
              "income_diff            2.102286\n",
              "age_diff               2.074991\n",
              "income                 2.035903\n",
              "shopping_diff          2.035515\n",
              "age_o                  2.023836\n",
              "world_rank_o           1.923843\n",
              "music_diff             1.918773\n",
              "(3_1-pf_o)_amb         1.888840\n",
              "(3_1-pf_o)_sinc        1.871227\n",
              "(3_1-pf_o)_intel       1.823441\n",
              "worldrank_diff         1.813159\n",
              "theater_diff           1.796729\n",
              "sports_diff            1.724349\n",
              "masters_o              1.677123\n",
              "masters_m              1.615070\n",
              "(3_1-pf_o)_fun         1.550664\n",
              "tv_diff                1.512336\n",
              "dining_diff            1.457875\n",
              "gaming_diff            1.431690\n",
              "exercise_diff          1.389167\n",
              "hiking_diff            1.370117\n",
              "yoga_diff              1.311142\n",
              "go_out_diff            1.251519\n",
              "date_diff              1.242700\n",
              "reading_diff           1.188976\n",
              "exphappy_o             1.166554\n",
              "clubbing_diff          1.162276\n",
              "tuition_o              1.124992\n",
              "condtn                 1.087097\n",
              "career_c_m             1.074604\n",
              "int_corr               1.069716\n",
              "imprelig_m             1.065239\n",
              "imprace_m              1.064130\n",
              "met_o                  1.035407\n",
              "samerace               1.035154\n",
              "goal_m                 1.016588\n",
              "from_m                 1.007115\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaq1fd65UCD9"
      },
      "source": [
        "# Since R squared and Adj R squared still has not changed/improved, \n",
        "# and since VIF values are all small and p-values are also all smaller,\n",
        "# We are satisfied with this model for shared interests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R74lnzOvoDW"
      },
      "source": [
        "## Shared Interests (Linear Regression - Bootstrap OSR2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define the current model as the final model \"sha_lr\"\n",
        "sha_lr = sha_model\n",
        "\n",
        "#remove the variables from test set\n",
        "X_test = X_test.drop(columns = ['museums_diff','order','movies_diff', 'mn_sat_o', 'art_diff', 'tvsport_diff'])"
      ],
      "metadata": {
        "id": "d8pwXo49GfVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check OSR^2 with test set\n",
        "sha_test_OSR2 = OSR2(sha_lr.predict(sm.add_constant(X_test)), y_test, y_train)\n",
        "sha_test_OSR2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKRH-pCT_MWa",
        "outputId": "2a0a813c-0ce7-407c-b7c2-f89a1d86e4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09273767401285915"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3rDZlDVNJo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca88708f-dcf8-4de0-aedc-1969ce0fc748"
      },
      "source": [
        "# Bootstrap linear regression model\n",
        "sha_output = bootstrap_validation(sm.add_constant(X_test),y_test,y_train,sha_lr,\n",
        "                                 metrics_list=[OSR2],\n",
        "                                 sample = 5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as4Qwz4yaOId",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "175942de-f999-4004-a954-8747062e6026"
      },
      "source": [
        "#bootstrap plots\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap OSR2 Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot OSR2 - Test Set OSR2', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].hist(sha_output.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[1].hist(sha_output.iloc[:,0]-sha_test_OSR2, bins=20,edgecolor='green', linewidth=2,color = \"grey\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  5.,   9.,  25.,  49.,  92., 189., 315., 444., 604., 744., 701.,\n",
              "        633., 492., 332., 204., 104.,  41.,  11.,   4.,   2.]),\n",
              " array([-0.0500772 , -0.04509013, -0.04010306, -0.03511599, -0.03012892,\n",
              "        -0.02514185, -0.02015478, -0.01516771, -0.01018064, -0.00519357,\n",
              "        -0.0002065 ,  0.00478057,  0.00976764,  0.01475471,  0.01974178,\n",
              "         0.02472885,  0.02971592,  0.034703  ,  0.03969007,  0.04467714,\n",
              "         0.04966421]),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 216
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtgAAAFCCAYAAADLxqq9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgkVZmo8feDlkUQWWyRYZl24aK4IbYILsiIDogK6DCKa6N4mRnQUWdUQOfahcsIOm7oXL1cGYHRERFlQEXGtgGROyA0iIgs0rJIt+yb7Ah+949zik6ys6oyq6IyK6ve3/Pkk5kRJyLPyaj64ssTJyIiM5EkSZLUjDUGXQFJkiRpNjHBliRJkhpkgi1JkiQ1yARbkiRJapAJtiRJktQgE2xJkiSpQfMGXYGmPeEJT8gFCxYMuhqS1LMLLrjglsycP+h69JMxW9IwGytuz7oEe8GCBSxbtmzQ1ZCknkXEtYOuQ78ZsyUNs7HitkNEJEmSpAaZYEuSJEkNMsGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1CATbEmSJKlBJtiSJElSg0ywJUmSpAaZYEuSJEkNmnW3SpcmEofFlJbPxdlQTSRJ3ZhK3DZmaxDswZYkSZIaZA+25qwRRqa1vCSpWb3EYWO2BskebEmSJKlBJtiSJElSg0ywJUmSpAaZYEuSJEkNMsGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1CATbEmSJKlBfU2wI2KbiLio5fGHiHhfRGwcEUsi4sr6vFEtHxFxZEQsj4iLI2L7ftZXkuYyY7YkTU5fE+zMvCIzt8vM7YDnA/cCJwGHAEszc2tgaX0P8Cpg6/o4APhKP+srSXOZMVuSJmeQQ0R2BX6bmdcCewHH1unHAnvX13sBx2VxLrBhRGzW/6pK0pxnzJakLs0b4GfvC3yrvt40M6+vr28ANq2vNweua1lmRZ12PdKAxGExqeVycTZcE6mvjNkaSsZsDcJAerAjYi1gT+A77fMyM4Ge/qoj4oCIWBYRy26++eaGailJAmO2JPVqUD3YrwIuzMwb6/sbI2KzzLy+Hk68qU5fCWzZstwWddqjZOZRwFEACxcu9CenptUII9NaXpqBjNkaWsZsDcKgxmC/iVWHGgFOARbV14uAk1umv72emb4jcGfLYUlJUn8YsyWpB33vwY6I9YBXAn/TMvlw4ISI2B+4FnhDnX4qsAewnHL2+jv6WFVJmvOM2ZLUu74n2Jl5D7BJ27RbKWeot5dN4KA+VU2S1MaYLUm9806OkiRJUoNMsCVJkqQGmWBLkiRJDTLBliRJkhpkgi1JkiQ1yARbkiRJapAJtiRJktQgE2xJkiSpQSbYkiRJUoNMsCVJkqQGmWBLkiRJDTLBliRJkhpkgi1JkiQ1yARbkiRJapAJtiRJktQgE2xJkiSpQSbYkiRJUoNMsCVJkqQGzRt0BaS5Ig6LSS2Xi7PhmkiSJmLM1lTYgy1JkiQ1yB5sqU9GGJnW8pKk5hizNRX2YEuSJEkNMsGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1KC+J9gRsWFEnBgRl0fEZRGxU0RsHBFLIuLK+rxRLRsRcWRELI+IiyNi+37XV5LmMmO2JPVuED3YXwROy8ynA88FLgMOAZZm5tbA0voe4FXA1vVxAPCV/ldXkuY0Y7Yk9aiv18GOiMcDOwP7AWTmg8CDEbEXsEstdixwJnAwsBdwXGYmcG7tSdksM6/vZ701c032TluSJmbMVtOM2Zor+t2D/WTgZuDrEfGLiPhaRKwHbNoSgG8ANq2vNweua1l+RZ0mSZp+xmxJmoR+38lxHrA98J7M/HlEfJFVhxYByMyMiOxlpRFxAOVwJFtttVVTddUQ6eUOWt5tS+qaMVvTwrskarbrdw/2CmBFZv68vj+RErxvjIjNAOrzTXX+SmDLluW3qNMeJTOPysyFmblw/vz501Z5SZpjjNmSNAl9TbAz8wbguojYpk7aFbgUOAVYVKctAk6ur08B3l7PTN8RuNOxfJLUH8ZsSZqcfg8RAXgP8M2IWAu4CngHJdE/ISL2B64F3lDLngrsASwH7q1lJUn9Y8yWpB71PcHOzIuAhR1m7dqhbAIHTXulJEkdGbMlqXfeyVGSJElqkAm2JEmS1CATbEmSJKlBJtiSJElSg0ywJUmSpAaZYEuSJEkNMsGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1CATbEmSJKlBJtiSJElSg0ywJUmSpAaZYEuSJEkNMsGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1CATbEmSJKlBJtiSJElSg0ywJUmSpAaZYEuSJEkNMsGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1KC+J9gRcU1E/CoiLoqIZXXaxhGxJCKurM8b1ekREUdGxPKIuDgitu93fSVpLjNmS1LvBtWD/ReZuV1mLqzvDwGWZubWwNL6HuBVwNb1cQDwlb7XVJJkzJakHsyUISJ7AcfW18cCe7dMPy6Lc4ENI2KzQVRQkvQIY7YkjWMQCXYCP46ICyLigDpt08y8vr6+Adi0vt4cuK5l2RV1miSpP4zZktSjeQP4zJdk5sqIeCKwJCIub52ZmRkR2csKa9A/AGCrrbZqrqaSJGO2JPWo7z3YmbmyPt8EnATsANw4ehixPt9Ui68EtmxZfIs6rX2dR2XmwsxcOH/+/OmsviTNKcZsSepdX3uwI2I9YI3MvKu+/kvgY8ApwCLg8Pp8cl3kFODdEXE88ELgzpbDktKcEIfFpJbLxT11KkqrMWZLvTNmC/o/RGRT4KSIGP3s/8jM0yLifOCEiNgfuBZ4Qy1/KrAHsBy4F3hHn+srSXOZMVuSJqGvCXZmXgU8t8P0W4FdO0xP4KA+VE2asUYYmdby0liM2VLvjNmCmXOZPkmSJGlWMMGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1CATbEmSJKlBJtiSJElSg0ywJUmSpAaZYEuSJEkNMsGWJEmSGmSCLUmSJDWo6wQ7InaOiPXHmLd+ROzcXLUkSZKk4dRLD/YZwLZjzNumzpckSZLmtHk9lI1x5q0NPDzFumgOi8PG+/OSJM0kxmxpfOMm2BGxAHhKy6SFHYaJrAu8E/hdozWTJEmShtBEPdiLgMVA1seXeHRPdtb3DwEHTUcFNbeMMDKt5SVJzTFmS51NlGAfA5xJSaJPpyTRl7aVeQD4TWbe1nTlJEmSpGEzboKdmdcC1wJExF8AF2bmXf2omCRJkjSMuj7JMTN/Op0VkSRJkmaDXq6DvVZELI6IyyPi3oh4uO3x0HRWVJIkSRoGvVym7zOUMdg/Ar5HGXstSZIkqUUvCfY+wOLM/OR0VUaSJEkadr3cyXF94JzpqogkSZI0G/SSYH8f2Hm6KiJJkiTNBr0MEfkScFxE/Ak4FVjtuteZeVVTFZMkSZKGUS8J9ujwkBHK3R07WXNKtZEkSZKGXC8J9jspt0afsohYE1gGrMzM10TEk4HjgU2AC4C3ZeaDEbE2cBzwfOBW4I2ZeU0TdZAkdceYLUm96eVGM8c0+LnvBS4DNqjvjwA+n5nHR8RXgf2Br9Tn2zPzaRGxby33xgbrIUmamDFbknrQy0mOjYiILYBXA1+r7wN4OXBiLXIssHd9vVd9T52/ay0vSeoDY7Yk9a7rHuyI+LcJimRm7t/Fqr4AfAh4XH2/CXBHZo7eCXIFsHl9vTlwXV35QxFxZy1/S7f1liRNiTFbknrUyxjsl7P6GOyNKUH3jvoYV0S8BrgpMy+IiF16+OyJ1nsAcADAVltt1dRqJWlOM2ZL0uT0MgZ7QafpEbEz8FXgLV2s5sXAnhGxB7AOZTzfF4ENI2Je7RHZAlhZy68EtgRWRMQ84PGUE2fa63YUcBTAwoULGzkRU5JkzJakyZjyGOzMPAv4POU62ROVPTQzt6jJ+r7A6Zn5FuAMyq3YARYBJ9fXp9T31PmnZ6bBWJL6wJgtSZPT1EmOVwHPm8LyBwP/EBHLKeP1jq7TjwY2qdP/AThkSrWUJDXBmC1J4+hlDHZH9TDgfpQTXbqWmWcCZ9bXVwE7dChzP/DXU62jJGlqjNmS1L1eriJyeofJawH/g9KD8bdNVUqSJEkaVr30YK/B6lcRuQv4HnB87d2QJEmS5rReriKyyzTWQ5IkSZoV+n4nR0mSJGk26ynBjohnR8SJEXFzRDxUn0+IiGdPVwUlSZKkYdLLSY4vAH4K3Ee51ukNwJOA1wKvjoidM/OCaamlJEmSNCR6OcnxU8AlwK6ZedfoxIh4HPCTOv8vm62eJEmSNFx6GSKyI/Cp1uQaoL4/AtipyYpJkiRJw6iXBHui2916O1xJkiTNeb0k2D8HPlyHhDwiItaj3Db33CYrJkmSJA2jXsZgf5hym9xrI+IHwPWUkxz3ANYDXtZ47SRJkqQh08uNZs6LiB2BjwK7ARsDtwFnAB/PzF9NTxUlSZKk4TFugh0RawCvBq7OzEsy82Jgn7YyzwYWACbYkiRJmvMmGoP9VuBbwD3jlLkL+FZEvKmxWkmSJElDqpsE++uZefVYBTLzGuBoYFGD9ZIkSZKG0kQJ9vbAj7tYz0+AhVOvjiRJkjTcJkqwHwfc3sV6bq9lJUmSpDltoquI3AL8OXD2BOW2qmUlzRBxWExquVzsPaMkqd+M2bPLRD3YZ9Pd2Or9mDgJlyRJkma9iXqwvwCcHRGfBw7OzAdbZ0bEY4DPAC8HXjI9VZQ0GSOMTGt5SVJzjNmzy7gJdmaeExH/CHwWeEtE/Bi4ts7+c+CVwCbAP2amt0qXJEnSnDfhnRwz8wsRcSFwMPA6YN066z7KrdMPz8yfTVsNJUmSpCHS1a3SM/Ms4Kx6Z8cn1Mm3ZubD01YzSZIkaQh1lWCPysw/ATdNU100C0z2LGhJUv8Zs6XpMdFVRCRJkiT1oKcebKlbng0tScPDmC01q6892BGxTkScFxG/jIhfR8RhdfqTI+LnEbE8Ir4dEWvV6WvX98vr/AX9rK8kzWXGbEmanH4PEXkAeHlmPhfYDtg9InYEjgA+n5lPo9x2ff9afn/g9jr987WcJKk/jNmSNAl9TbCzuLu+fUx9JOVGNSfW6ccCe9fXe9X31Pm7RoRnZEhSHxizJWly+n6SY0SsGREXUa5GsgT4LXBHZj5Ui6wANq+vNweuA6jz76Tc2KZ9nQdExLKIWHbzzTdPdxMkac4wZktS7/qeYGfmw5m5HbAFsAPw9AbWeVRmLszMhfPnz59yHSVJhTFbkno3sMv0ZeYdwBnATsCGETF6RZMtgJX19UpgS4A6//HArX2uqiTNecZsSepev68iMj8iNqyv1wVeCVxGCdr71GKLgJPr61Pqe+r80zMz+1djSZq7jNmSNDn9vg72ZsCxEbEmJbk/ITN/EBGXAsdHxCeAXwBH1/JHA/8eEcuB24B9+1xfSZrLjNmSNAl9TbAz82LgeR2mX0UZ29c+/X7gr/tQNUlSG2O2JE2Ot0qXJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1CATbEmSJKlBJtiSJElSg0ywJUmSpAaZYEuSJEkNMsGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1CATbEmSJKlBJtiSJElSg0ywJUmSpAaZYEuSJEkNMsGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1CATbEmSJKlBJtiSJElSg0ywJUmSpAaZYEuSJEkNMsGWJEmSGtTXBDsitoyIMyLi0oj4dUS8t07fOCKWRMSV9XmjOj0i4siIWB4RF0fE9v2sryTNZcZsSZqcfvdgPwT8Y2ZuC+wIHBQR2wKHAEszc2tgaX0P8Cpg6/o4APhKn+srSXOZMVuSJmFePz8sM68Hrq+v74qIy4DNgb2AXWqxY4EzgYPr9OMyM4FzI2LDiNisrkfSNIjDoudlcnFOQ000aMZsaeabTMwG4/Z0G9gY7IhYADwP+DmwaUsAvgHYtL7eHLiuZbEVdZokqY+M2ZLUvb72YI+KiPWB7wLvy8w/RKz69ZWZGRE9/ayKiAMohyPZaqutmqyqNOeMMDItZTW8jNnSzNVrHDZu90ffe7Aj4jGUQP3NzPxenXxjRGxW528G3FSnrwS2bFl8izrtUTLzqMxcmJkL58+fP32Vl6Q5xpgtSb3r91VEAjgauCwzP9cy6xRgUX29CDi5Zfrb65npOwJ3OpZPkvrDmC1Jk9PvISIvBt4G/CoiLqrTPgwcDpwQEfsD1wJvqPNOBfYAlgP3Au/ob3XnrsmeNCFpVjFmDwljtjSz9PsqImcDY0WBXTuUT+Cgaa2UJKkjY7YkTc5ATnLU8PDkCUkaHsZsaWbwVumSJElSg0ywJUmSpAaZYEuSJEkNMsGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1CATbEmSJKlBJtiSJElSg0ywJUmSpAaZYEuSJEkNMsGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1CATbEmSJKlBJtiSJElSg0ywJUmSpAaZYEuSJEkNMsGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1CATbEmSJKlBJtiSJElSg/qaYEfEv0XETRFxScu0jSNiSURcWZ83qtMjIo6MiOURcXFEbN/PukqSjNuSNBn97sE+Bti9bdohwNLM3BpYWt8DvArYuj4OAL7SpzpKklY5BuO2JPVkXj8/LDPPiogFbZP3Anapr48FzgQOrtOPy8wEzo2IDSNis8y8vj+1ldStOCwmtVwuzoZroqYZt6XZybg9vWbCGOxNW4LvDcCm9fXmwHUt5VbUaZKkwTJuS9I4+tqDPZHMzIjo+adRRBxAORzJVltt1Xi9JI1vhJFpLa+ZazJx25gtDZ5xe3rNhB7sGyNiM4D6fFOdvhLYsqXcFnXaajLzqMxcmJkL58+fP62VlSRNLW4bsyXNdjOhB/sUYBFweH0+uWX6uyPieOCFwJ2O4+vdZMdYSdI4jNvTxJgtzQ59TbAj4luUE2OeEBErgMWUAH1CROwPXAu8oRY/FdgDWA7cC7yjn3WVJBm3JWky+n0VkTeNMWvXDmUTOGh6azR3ONZK0mQYtwfDmC0Nt5kwBluSJEmaNUywJUmSpAaZYEuSJEkNMsGWJEmSGmSCLUmSJDXIBFuSJElqkAm2JEmS1CATbEmSJKlBJtiSJElSg0ywJUmSpAaZYEuSJEkNMsGWJEmSGjRv0BVQd+KwGHQVJEk9MG5Lc5c92JIkSVKD7MEeMiOMTGt5SVKzeonDxmxpdjDBljQwkz2Enouz4ZpIkroxmbg9F2O2Q0QkSZKkBtmDLWlgHPIkScPFIU/dsQdbkiRJapAJtiRJktQgh4gMgNdGlaThYcyW1Ct7sCVJkqQG2YM9QJ4oIEnDw5NyJXXLHmxJkiSpQSbYkiRJUoMcIiJp6HgHSEkaHnMxZs/4BDsidge+CKwJfC0zDx9wlR7hmeWStLqZGreN2ZL6ZUYn2BGxJvCvwCuBFcD5EXFKZl7a6OcYdKWhMtmTzeZiL0q/9SNuG7Ol4TIXY/aMTrCBHYDlmXkVQEQcD+wFNJpgT5Vnlkuz2zAH+QGY8XHbmC3NbjMhZkfmzN0BRMQ+wO6Z+a76/m3ACzPz3WMts3Dhwly2bFlvn2NviKRp0GuwjogLMnPhNFWnL3qN28ZsSTPFZBLsseL2TO/B7kpEHAAcUN/eHRFXjFH0CcAt/alV39m24TWb2zeb2wYTtC9Gek4E/3xKtRkSPcTsmWRO/y0PudncNpjd7etr2yYRs2GMuD3TE+yVwJYt77eo0x4lM48CjppoZRGxbNh7h8Zi24bXbG7fbG4bzP72TdKEcbvbmD2TzPZtPZvbN5vbBrO7fcPctpl+Hezzga0j4skRsRawL3DKgOskSRqbcVvSnDeje7Az86GIeDfwX5TLPf1bZv56wNWSJI3BuC1JMzzBBsjMU4FTG1rdUB2S7JFtG16zuX2zuW0w+9s3KQ3H7Zlitm/r2dy+2dw2mN3tG9q2zeiriEiSJEnDZqaPwZYkSZKGyqxIsCNi94i4IiKWR8QhHeavHRHfrvN/HhEL2uZvFRF3R8QH+lXnXkylfRHxnIg4JyJ+HRG/ioh1+ln3iUy2bRHxmIg4trbpsog4tN9170YX7ds5Ii6MiIfq9YNb5y2KiCvrY1H/at2dybYtIrZr+Zu8OCLe2N+ad2cq267O3yAiVkTEl/tTY01VRGwcEUvq/9ySiNhojHLj/m9GxCkRccn017h7U2lbRDw2In4YEZfX/9vD+1v7sU1x/3honX5FROzWz3p3Ywr7x1dGxAV1/3hBRLy833XvxlS2XZ0/o3M3MnOoH5STaH4LPAVYC/glsG1bmQOBr9bX+wLfbpt/IvAd4AODbk+T7aOMsb8YeG59vwmw5qDb1FDb3gwcX18/FrgGWDDoNk2ifQuA5wDHAfu0TN8YuKo+b1RfbzToNjXUtv8BbF1f/xlwPbDhoNvUVPta5n8R+A/gy4Nuj4+ut/ungUPq60OAIzqUGfd/E3h93e6XDLo9TbWtxti/qGXWAn4GvGoGtGkq+5Bta/m1gSfX9cyW/ePzgD+rr58FrBx0e5psX8v8GZu7Zeas6MF+5La8mfkgMHpb3lZ7AcfW1ycCu0ZEAETE3sDVwEw9y30q7ftL4OLM/CVAZt6amQ/3qd7dmErbElgvIuYB6wIPAn/oT7W7NmH7MvOazLwY+FPbsrsBSzLztsy8HVgC7N6PSndp0m3LzN9k5pX19e+Bm4D5/al216ay7YiI5wObAj/uR2XVmNZ4cyywd4cyY/5vRsT6wD8An+hDXXs16bZl5r2ZeQZA/X+4kHJ980Gbyj5kL0onzQOZeTWwvK5vpph02zLzFzW2Qslt1o2ItftS6+7N9txtViTYmwPXtbxfUad1LJOZDwF3ApvUYHgwcFgf6jlZk24fpacwI+K/6qHsD/Whvr2YSttOBO6h9H7+DviXzLxtuivco27aNx3L9kMj9YuIHSi9F79tqF5NmXT7ImIN4LPAzDxsqfFsmpnX19c3UH4ktRvvb+PjlG1/77TVcPKm2jYAImJD4LXA0umoZI+msg+ZDTF2rLa1+ivgwsx8YJrqOVmzPXeb+Zfpm2YjwOcz8+76o2i2mQe8BHgBJeAvjYgLMnMmBMap2gF4mDLEYCPgZxHxk8y8arDVUrciYjPg34FFmblaL/AQOxA4NTNXzNK4MtQi4ifAkzrM+kjrm8zMiOj6MlsRsR3w1Mx8f/tY0X6Zrra1rH8e8C3gSGPtzBcRzwSOoBzNnk1GGILcbTYk2N3cTn20zIoaIB4P3Aq8ENgnIj4NbAj8KSLuz8yZdFLSVNq3AjgrM28BiIhTge2ZGT0PMLW2vRk4LTP/CNwUEf8PWEgZMzhTdNO+8ZbdpW3ZMxupVTOm0jYiYgPgh8BHMvPchuvWhKm0byfgpRFxILA+sFZE3J2Zq53Eo/7LzFeMNS8iboyIzTLz+voD8KYOxcb639wJWBgR11D2rU+MiDMzcxf6ZBrbNuoo4MrM/EID1W3CVPYhU4phfTCVthERWwAnAW/PzJl2hBBmf+42K05ynEdJqp7MqoHyz2wrcxCPHih/Qof1jDADB8pPpX2Unt0LKSeozAN+Arx60G1qqG0HA1+vr9cDLgWeM+g29dq+lrLHsPpJjlfXbbhRfb3xoNvUUNvWovzIe9+g2zEd7Wubtx+e5Dg0D+AzPPpEwE93KDPh/yblBNiZdpLjlNpGGVf+XWCNQbelpb5T2Yc8k0ef5HgVM+skx6m0bcNa/vWDbsd0tK+tzAgzMHfLzOFPsOsXvAfwG8o4zo/UaR8D9qyv16GcabocOA94ylBtpCm0D3gr5SSASzoF1EE/Jts2Ss/gd2rbLgU+OOi2TLJ9L6AcabiH8sv81y3LvrO2eznwjkG3pam21b/JPwIXtTy2G3R7mtx2LevYDxPsoXlQxq8uBa6kdEiMJpcLga+1lBv3f5OZmWBPum2U3sUELmv5n33XoNtU6zaV/eNH6nJXMAOuitJU24B/qnGpNcY+cdDtaXLbtaxjhBmau3knR0mSJKlBs+EqIpIkSdKMYYItSZIkNcgEW5IkSWqQCbYkSZLUIBNsSZIkqUEm2DNcROwXEdnyeDgiVkbECRGxzTR+7oYRMRIR209i2b0j4h+mo15NiIhtIuLY+j0+WJ//vdP3GRFrR8T7I+KXEXFXRPwhIi6vy2/dUm6kbTs9EBGXRsQH662zW9e5T0R8NyKujYj7IuKKiPhURDyui7ovaPuc9sd2PXwPIxHx8g7Tj6k3y+ir+rf+zn5/rjRZwxafI2LfiPhpRNwREfdGxK8i4sMRsW6Hsk+pseCqGs9uiohzIuLjbeWuafsO7oiIJRHxkrZyG0TERyPivyPi1lruvyNi78l9Cx3bt8sE8XH0cUwDn9XTfq7b77PLdS2o2/8pPSyzZkT8XUScV/dld0fE+RFxYESs2aH8dnU/9bta3+sj4oyI+Pu2cq3f658i4paIODnKXSRby21W93PL6ra/OSKWRsTOvbZ/WHiZvhkuIvYDvg78NeWau2sCTwX+F7Au5cLsd07D5y6g3GTgf2bm13pc9hjgFZm5RdP1mqqIeAVwMuVasJ+ntHEB8D5gG2CvzPxJS/nvUW4z+2ngXMr3/wzK9jg8M0+u5UaAxZRb0z9MuWHDfrXcBzLzsy3rPBf4Xa3HCuB5lGt5Xg68KMe5bXjLdvkUcEqHIhdn5r1dfhcJfDIz/6lt+lOBDTLzF92spykRcSYwLzNfMlFZaSYYpvgcEf8H+J+UGyN9F7gX2Bn4AOX61q/IzD/Usn9OuXbytcAXgGuATYEdgD0y8xkt672GErtGKJ12W1Ni4RMoN/+6ppZ7FuX6218HzgL+BLwJWAS8OzP/tdfvoUMbNwC2bZm0GfA9Vo+XN+cU727Yy36ul++zy8/eBTgDeGXr/mqc8o8B/hN4JfBl4DTKdc13B94NLAH2zsyHavkXAD8Dfg58BbiBci30lwDbZOZftKw7KX9T/4dy85hnU65lfT/w7My8o5Z7DXAkZfufS7m5zIHAqyjXvf5BL9/BUMI0X7kAAA8SSURBVBj0hbh9jP+gJGkJPK1t+ivq9Gm5OD4l6UwmcTMByj/bii7Lrt3H73IT4Bbgv4F12uatU6ffAmxSpz2lfgfvHWN9a7S8Hqll57XOp+x4Lm9bbn6Hdb29Lv/y6douHdaVwCf69f13UZ8zgbMHXQ8fPrp9DEt8bqnnarGMcsOkB6h3xq3TPka5GdQmHcqv0fb+GuAbbdNeXD/vkJZp6wGP7bC+pcDvZsL31OO6e9nPdf19drm+XWq7XtFl+dH9014d5u1V5y1umXYcJalebf/cYfuvth8B3lKn79sybcPW/WOdNo9yk5+zpmP7D/rhEJHh9Yf6/JjWiRGxez3sdF9E3BkR/9l+qDKK90cZmvBgPfTz5frrv7V3BOD/thz+2a/O360e2ruzHma6IiI+WucdQ+mR2LxluWvqvNHDd6+PiP8bETcDN9Z5T4syTOPqWverIuIrEbFRW92PiYgVEfGienjr/iiHKN/TxXf2LkqS/d7MvL91Rn3/vjr/XXXyxvX5hk4ry3F6mlvm/xLYqm36zR2Kn1+fNx9vnd2KiHkR8fGI+G39jm6JiLOjHratvQ4AH2nZTiN13qOGiMSqYSl/Ww/x3VAPMX4jIh5bt91/1b+F5RGxqK0uE27b2nv9MuDFLfU5s2X+kyPim/Ww4gMRcVFEvK6J70qaBgOLz2M4mHLX2yPbZ2Tm+cDRwNsi4s/q5I0pPZB3dCg/btyrLqzPj8S+zLwnOx9dWwb8WYfp0yYiXhZleMJdEXFPjV/Paiszqf3cGLr+PmvsPjTKUMQHIuL3EfHZiFinzt+F0nsNsKTl83cZo61rU/Ztp2Y94tr2+ScDPwLeV8uO1vf2zHxgovqOodP2vyNrD3nLtIcoPfuN7PdmGhPs4bFm/cdbOyKeAfwzcBOl1w8owRv4IXA38Ebg74BnAWdHROsf8CeBz1EOC72WMvxhP+CHUcYLXw+8vpb9FLBTffwwypivUygB/o3AnnVd69XyHwdOBW5uWa49EfoSEMDb6udCCbDXUQLBbpRf/LvWdbXbAPg2cCywd/0OjpxgB0Nd3w11h7KazDyPkvCPjku+nLKjPDwi3hoRm06w/k4WUG4DO5GX1efLulzvGvXvofXROo7uYOD9lB3qbsA7KD1Foz8adqrPx7BqO010qPlQynZaBHyUsv2/CpxE+bt7HXAx8PV49Pi7brbtgcAv6vKj9TkQICK2pByqfG5t056UAP7diNhzgjpL/TAj4nOnitWk+enA97N2G3ZwCmV4y2gcOg9YH/h2ROzcknh1a0F97ib27UyJtX0REa+mxMK7gbcCbwYeB/ysxhoa2s+16uX7/AblVuf/Abyaso33B75Z518IHFRf/33L519IZ88HHk/nIYWjTqH0MI+O6T8PeHpEfDUidoiIeeMs28mC+jzu9o+ItSh173a/N1wG3YXuY/wHqw7ttT9WAi9oK7uMMra4dZjCkymHpj5X329MORx4TNuyb63r3bO+X0CHQ2vAPnX6BuPU+Rg6HDpj1WGtk7po9zzKeK8Ente27kcdeqrTl1DGt8U467wMOGeCzz0XuLTl/WspQXT0e/8tZQzb09uWG6nz1651n09JSB+ijG0b7zM3p+yMl3TxvSyg899DAne3lPsB8L0J1tVxiEj9jq/p8Jmnt5X7Xp3+1pZpG9U2L57Etj2TDkNEKL1rN9N2eLVu84um8//Ph4/xHsyw+DxGHV9Yy/7NOGWeXst8qL4Pyo/nP9XpD1DG5P4jqw+vu4aS/M2jjKvdFvgp8BtgownqdkBd/1umafus9j0By4GlbeU2oAwP/EJ9P+n93Bhlu/o+gZfW+W9vW350yMV29f0udDlEhPIDIYHdximzey3zhvp+XUrHyejf873Ajylj+DsNEflk3f7rUIYc/Qo4B3jMBHX75/qdvHQ6tv+gH/ZgD4/XUf5wd6D02l4KnFp7S4iI9Si/Pr+dLYdhMvNq4P+xqmdiR0oQ/Ebb+o+nJEYvY3wXUXYIx0e5GsYTJ9GWk9onRMRaUc5mvzwi7quf8bM6u/1s/IcpJ+m0Op5yOKrRQ02Z+X1KkH49pef9Dmpva5QTJtvdT6n7TZTgcWhm/udY64+I9SknOz5E6WXu1icofw+tj5e2zD8f2CMiPhkRL6k9BVP1o7b3o71O/zU6ITNvp7R9y9FpPW7bTnan9Bbd2dpjXz/3uaOHzqUBminxuRFZ/C3lhM33UOLt04B/Ac6L1a868mbK//UDlKEozwJeW+NBR3VIw5HAcZn5zbHK1bIxztG6rkW58tNTgW+2xZJ7KQnh6BUtmtjPPaKH73N34EHgxLb6/bjO78sVNzLzvsx8HfBM4IOU2L8QOAr4UURE2yIfpnxf97Gqt37PzPzjWJ8REW8GDgE+npk/G6vcMDPBHh6XZOayzDw/y5ipPSm/ikfq/I3q++s7LHsDq4YGjD4/qlwN+re2zO8oM5dTDvOvAfw7cENEnBsRvQT+TnX8FKUt36AcFtuBVYdB12kre3uHf9wb6/N4CfYKVh26GssCynCGR2QZO3hSZv59Zj4feBElyT+8w/I71rq/jnLI7vBxxsatC3yfcjLlbpm5YoK6tbq2/j20Plqv+vHPlDP596Qks7dGxNcj4gk9fEa79p3lg+NMb91mvWzbTp5IOQn0j22Pz9T5m3SxDmk6zYj4PIbRuLJgnDKj89pj39WZ+eXMfDPlKhKfplwlYv+25X9E+YHxIspQsHWB742OG24X5SoVpwCns+qcl/Es4tH/+5O9Ashoonw0q8eT11BjSUP7udV08X0+kfID6562ut1U508m1k1l+1+amf+SmX9FGer3DcpVtV7dtvy/saqTZ4TS2XV8h0QcgIh4LeUIwNGZubjLdgydXsfVaIbIzPsi4irgOXXS7ZRDNU/qUPxJwG319W0t0349WqD+St6kZf54n30GcEYdR/ZiypjaH0bEgsy8pZvqd5i2L6Un4xMtdVp/jOU3iojHtCXZo+OjV47zuUuBV0TEC7LDOOyI2KGu5/RxK595bkT8mNLb0O6CujM8PyLOpvTyfikinpstJ4dEuWzSiZRegVdm5q/G+8xe1e/mCOCIiHgSZefxOeCxlEOG/dTLtu3kVsqPhCPGmP/7KdRNatwg43OHuqyMiCsow90OHaPYnpROg5+Os56HI+KTwId49KXwAG7LzGX19TkRcSflcmzvYdUP4dG2PJty9Oki4K/G6+Vs8X1KAjdqtZPvunRrfT6UcsnAdqOdBk3s58Y1xvd5K+Uo6EvHWGwysW4Z5VyiPSmX0utkT+BOxh7HTWbeHxGfoQxX2pYyDHHU9S3b/+yaWC+mDLX5Tut6ImLXOu0k4G96bs0QsQd7SEXEYymHm26G0ssKXAD8devhsyjX33wRq062OZcSRPZtW+UbKT+4RsuNBrDVbkAwKjMfyMzTKb/C16OMJxxddszlxvBYyi/1VmMNmVgT+Ku2aftSri09XoL9NcqO7ovtPSv1/RcoO7Cv1WmPq4d2aSu7JuVar516ox5Rg/DHKIdLH6lvPVHpm5STKffOzHPHW89UZeYNWa6V+5Nal1EP0vt2moxut+1YfzenURKVX3fotV+WHc50lwZpJsTnNp8BnhltNwmpdXgB9SS6zPx9nbbZGOt5en0eN/ZRTkC/EPhg/S5GP2tryrkTVwGvycz7uql8Zt7a9j8/2Q6JKyhjxp85Riy5uMNnT3k/18P3eRrlqN7jx6jfaILd9fav8fFIypDBvTrUbS/Ktai/OBpLG9j+R1B+DHy0tRc7InaiDIlcSjl3p5srkgwte7CHx3b18H5QLp7/bsrhwi+1lPlflDPJfxAR/5syDuowyi/TzwJk5m0R8Vng0Ii4hzK29RmUMb1ns+pM9Bspv6b3jYiLKYesrqbcUGHnutx1lJsJHEr5Z7qkLnspsHFE/B3l1/P9XQTE04BFEfErykkor6fseDq5C/h0/T6upNys4BXAfpk51lnyZOYtEfEmyi/ncyKi9UYz76cEj9dl5mgvxzbAaRHxLcqO7SbKd/8uSqJ64ARtgtJj8EHgnyLixFq/f6V8j58E7omIHVvKr+hyqMhT2pYb9Zu6jU+mXCLwQsqPiudRetxbezAuBV4dEafVMr9vCeBN6nbbXgocGBFvpBwCviszr6BcseQ84KyI+DJlB7kRZRs8JTO9+6MGbUbE55bY9SiZeXREvAj4QkQ8lzIG+D5KT+kHKLH7vS2LfKSWP55V45GfQ+ltvZXSOz2mzMwol7T7AeVqKZ+t45iXUIZALAa2bRtB8Ivp/rFc63UQcHKU81JOoJzcuCklJv0uMz8XEX9Ls/u5rr7PzDyz7m9OjIjPUeLenyj7qD2AgzPzN5QTSB8C3hkRt1ES7isy864xPv9jlKOlJ0TEv1KG9CRln/AeSoz+REv5o6Kc2/Ld2t41KUcQPkSJzaudR9WqHsH5Z8oFAV5PueLT0yl/v7dQfvA9v3X7T3dH00DkDDjT0sfYDzqfpX4TZRjDamcFU/5hzqEEzzspvxa3aSsTlITyCkpvyfWUpG+DtnKjJ+v8sX7ufpRL6pxMCToP1GW/0/oZlF/532LVYdFr6vRdGOPMZ0oAO74uczulh/cFo5/bUu4YypiyF1FO5LufcvWQv+/hO30GZVzd71va/01g27ZyG1KSu7NqmT/Wup0B7NNWdqTWdV6Hzxs9U/519f01Hbbp6GNkgrovGGfZHK0X5ez0cynB+766rUdoOaubctjzgvodPvLZjH0VkfYrynRsM203nuhh2z6JskO7q847s2XeFpQjCytbttkSWq5g4sNHvx/MsPjcRX3fXOPZH2odLqFcEu6xbeVeSLnqxSWUE7v/SDlCeAzw1Layj/p/b5v337X+67Iq/o/1WDAN22es2LUTJfm/vca/a2qM2qll/qT2c2PUo5fvcw3Kj51f1rrdWV9/mtKzPVrubyhHAh6qn7/LBN/FPMrl/c6n/CC7h/LD4N2sHsN3oxyFuIISjx9g1RW0Nm0rm3S+GtVa9Xv9Rf2b3m+87T/o/+XpeHirdA2VmMG3YZckSQLHYEuSJEmNMsGWJEmSGuQQEUmSJKlB9mBLkiRJDTLBliRJkhpkgi1JkiQ1yARbkiRJapAJtiRJktQgE2xJkiSpQf8f52lwQTyOA70AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yxlt1MoWwkR",
        "outputId": "73a5020a-f255-4e9f-e95c-2e6886dd6a01"
      },
      "source": [
        "# Mean of bootstrapped estimate\n",
        "print(\"Mean of Linear Regression estimates: %s\" % np.mean(sha_output)[0])\n",
        "# The 95% confidence interval\n",
        "CI_0 = np.quantile(sha_output.iloc[:,0]-sha_test_OSR2,np.array([0.025,0.975]))\n",
        "left = sha_test_OSR2 - CI_0[1]\n",
        "right = sha_test_OSR2 - CI_0[0]\n",
        "print(\"The 95-percent confidence interval of the test set OSR2 for the linear regression shared interests model is\",[left, right])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean of Linear Regression estimates: 0.09264698060821143\n",
            "The 95-percent confidence interval of the test set OSR2 for the linear regression shared interests model is [0.06655472412329201, 0.12026468300504232]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ5SDUa3vuEv"
      },
      "source": [
        "## Shared Interests (LDA - Train Model + Bootstrap Accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGViXNzWNJo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b5222b-76b7-4806-bbea-8c16f6585301"
      },
      "source": [
        "# redefine y_train/y_test type to int\n",
        "y_train = sha_y_train.astype(int)\n",
        "y_test = sha_y_test.astype(int)\n",
        "\n",
        "# train\n",
        "sha_lda = LinearDiscriminantAnalysis()\n",
        "sha_lda.fit(sm.add_constant(sha_X_train), y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearDiscriminantAnalysis()"
            ]
          },
          "metadata": {},
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahsvL1j-NJo1",
        "outputId": "2b2620dc-aa14-4a26-a6ba-5bac95e7dba8"
      },
      "source": [
        "# compute ambition accuracy of LDA\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = sha_lda.predict(sm.add_constant(sha_X_test))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix : \\n\", cm)\n",
        "sha_lda_test_acc = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nTest Set Accuracy:\", sha_lda_test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix : \n",
            " [[  2   1   0   0   0   5   1   3   1   0   0]\n",
            " [  1  10   3   0   1  21  12   6   0   0   1]\n",
            " [  7   1  12   1   4  55  22   8   4   2   0]\n",
            " [  2   2  11   0   4  62  24  11   9   2   1]\n",
            " [  0   1   3   4   9  78  40  23   1   0   2]\n",
            " [  5   5   9   6  12 155  71  28   6   1   5]\n",
            " [  1   2   2   2   1 109  77  33  14   1  14]\n",
            " [  1   2   6   1  10  98  87  37  14   1  17]\n",
            " [  3   2   4   1   1  53  39  20  12   1   9]\n",
            " [  1   0   1   0   2  17   9   9   9   2   6]\n",
            " [  0   0   1   0   0   8   7   6   8   0  12]]\n",
            "\n",
            "Test Set Accuracy: 0.2117495158166559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M7pJwelNJo2",
        "outputId": "193511d3-2352-412e-878e-736b1736f2b2"
      },
      "source": [
        "sha_lda_output = bootstrap_validation(sm.add_constant(sha_X_test),y_test,y_train,sha_lda,\n",
        "                                 metrics_list=[acc],\n",
        "                                 sample = 5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "YWEtvV1dbGsF",
        "outputId": "07d18072-18cd-45c7-9856-b8e97adb79cf"
      },
      "source": [
        "#bootstrap accuracy plots for sincerity\n",
        "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
        "axs[0].set_xlabel('Bootstrap Accuracy Estimate', fontsize=16)\n",
        "axs[1].set_xlabel('Boot Acc - Test Set Acc', fontsize=16)\n",
        "axs[0].set_ylabel('Count', fontsize=16)\n",
        "axs[0].hist(sha_lda_output.iloc[:,0], bins=20,edgecolor='green', linewidth=2,color = \"grey\")\n",
        "axs[1].hist(sha_lda_output.iloc[:,0]-sha_lda_test_acc, bins=20,edgecolor='green', linewidth=2,color = \"grey\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  4.,   6.,  20.,  76., 111., 189., 348., 538., 645., 647., 735.,\n",
              "        616., 475., 285., 161.,  68.,  48.,  16.,   8.,   4.]),\n",
              " array([-0.03679793, -0.03305358, -0.02930923, -0.02556488, -0.02182053,\n",
              "        -0.01807618, -0.01433183, -0.01058748, -0.00684312, -0.00309877,\n",
              "         0.00064558,  0.00438993,  0.00813428,  0.01187863,  0.01562298,\n",
              "         0.01936733,  0.02311168,  0.02685604,  0.03060039,  0.03434474,\n",
              "         0.03808909]),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 221
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbBG9bbmU-qE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8353a9-e54d-43db-8b6e-fc74155b089c"
      },
      "source": [
        "# Mean of bootstrapped estimate\n",
        "print(\"Bootstrapped Mean of LDA Accuracy estimates: %s\" % np.mean(sha_lda_output)[0])\n",
        "# The 95% confidence interval\n",
        "CI_0 = np.quantile(sha_lda_output.iloc[:,0]-sha_lda_test_acc,np.array([0.025,0.975]))\n",
        "left = sha_lda_test_acc - CI_0[1]\n",
        "right = sha_lda_test_acc - CI_0[0]\n",
        "print(\"The 95-percent confidence interval of the test set accuuracy for the LDA shared interests model is: \",[left, right])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped Mean of LDA Accuracy estimates: 0.21168353776630083\n",
            "The 95-percent confidence interval of the test set accuuracy for the LDA shared interests model is:  [0.19173660426081343, 0.23240800516462234]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shared Interests"
      ],
      "metadata": {
        "id": "e--dsAEvPO0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_sha = 1 - (np.sum((sha_y_test - sha_y_train.value_counts().index[0])**2)/np.sum((sha_y_test - np.mean(sha_y_train))**2))\n",
        "rm1_lr_sha = rescaled_OSR2_1(sha_lr, sha_y_train, sha_test_2.drop(columns= ['museums_diff','order','movies_diff', 'mn_sat_o', 'art_diff', 'tvsport_diff']), 'shar_o', 'lr')\n",
        "rm2_lr_sha = rescaled_OSR2_2(sha_lr, sha_y_train, sha_test_2.drop(columns= ['museums_diff','order','movies_diff', 'mn_sat_o', 'art_diff', 'tvsport_diff']), 'shar_o', 'lr')\n",
        "rm1_lda_sha = rescaled_OSR2_1(sha_lda, sha_y_train, sha_test, 'shar_o', 'lda')\n",
        "rm2_lda_sha = rescaled_OSR2_1(sha_lda, sha_y_train, sha_test, 'shar_o', 'lda')\n",
        "\n",
        "\n",
        "print('Models for SHARED INTERESTS:')\n",
        "print('No Rescale Bootstrap, Linear Regression OSR2 for SHARED INTERESTS:', np.mean(sha_output)[0])\n",
        "print('No Rescale Bootstrap, LDA Accuracy for SHARED INTERESTS:', np.mean(sha_lda_output)[0])\n",
        "print()\n",
        "print('Rescaling Linear Regression Model before OSR2 Calculation:')\n",
        "print('Rescale Method 1, Linear Regression OSR2 for SHARED INTERESTS:', rm1_lr_sha)\n",
        "print('Rescale Method 2, Linear Regression OSR2 for SHARED INTERESTS:', rm2_lr_sha)\n",
        "print()\n",
        "print('BASELINE:')\n",
        "print('Baseline OSR2:', base_sha)\n",
        "print()\n",
        "print('SHARED INTERESTS MODEL COMPARISONS:')\n",
        "print('OSR2 difference between LR model and baseline:', np.amax(np.array([rm1_lr_sha, rm2_lr_sha, rm1_lda_sha, rm2_lda_sha, np.mean(sha_output)[0]])) - base_sha)\n",
        "print('Accuracy difference for LDA and baseline:', np.mean(sha_lda_output)[0] - sha_y_train.value_counts().values[0]/np.sum(sha_y_train.value_counts()))"
      ],
      "metadata": {
        "id": "zEUvWDCQ3WpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a6c7db-7de1-4f58-c5f3-fe7dfe396000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models for SHARED INTERESTS:\n",
            "No Rescale Bootstrap, Linear Regression OSR2 for SHARED INTERESTS: 0.09264698060821143\n",
            "No Rescale Bootstrap, LDA Accuracy for SHARED INTERESTS: 0.21168353776630083\n",
            "\n",
            "Rescaling Linear Regression Model before OSR2 Calculation:\n",
            "Rescale Method 1, Linear Regression OSR2 for SHARED INTERESTS: -3.373976565540401\n",
            "Rescale Method 2, Linear Regression OSR2 for SHARED INTERESTS: -1.4849082546637846\n",
            "\n",
            "BASELINE:\n",
            "Baseline OSR2: -0.03262780751053174\n",
            "\n",
            "SHARED INTERESTS MODEL COMPARISONS:\n",
            "OSR2 difference between LR model and baseline: 0.12527478811874315\n",
            "Accuracy difference for LDA and baseline: 0.011877211103938023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wVCzf8ejDnMw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}